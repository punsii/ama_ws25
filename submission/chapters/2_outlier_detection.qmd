---
title: "Outlier Detection and Cleaning"
format:
  html:
    toc: true
---

# Outlier detection in the longitudinal Life Expectancy data

Outliers are observations or feature values that deviate markedly from the bulk of the data. In multivariate health and socio-economic datasets such as the WHO Life Expectancy panel, extreme values can arise from measurement error, inconsistent reporting, or genuine structural differences between countries (for example very large populations or exceptionally low immunization coverage). Because outliers can dominate covariance structure and bias parametric fits, we screen the longitudinal dataset before downstream analyses. We compare three complementary strategies: a univariate Interquartile Range (IQR) rule, a univariate Z-score rule, and a multivariate Isolation Forest. Each method emphasizes a different notion of atypicality, so agreement and disagreement across methods provides guidance on which approach is most defensible for the subsequent cross-sectional modelling.

## Setup and data loading

```{python}
# | label: setup-outlier-detection
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from ama_tlbx.data import LECol, LifeExpectancyDataset
from ama_tlbx.utils.plotting_config import DEFAULT_PLOT_CFG
np.random.seed(42)

DEFAULT_PLOT_CFG.apply_global()

```

```{python}
# | label: load-longitudinal-outliers
# | output: false
le_ds = LifeExpectancyDataset.from_csv(
    aggregate_by_country=False, resolve_nand_pred=False
)
df = le_ds.df.assign(year=le_ds.df[LECol.YEAR].dt.year).set_index(
    [LECol.COUNTRY, LECol.YEAR]
)
df
```

```{python}
# | label: tbl-descriptive-numeric
# | tbl-cap: "Descriptive statistics of numeric features (longitudinal data)."
df[le_ds.numeric_cols].describe().T
```

The descriptive summary already indicates that several predictors have heavy right tails, which motivates both transformation and robust outlier rules.

## Exploratory distributions

We first visualize the marginal distributions of a subset of key features to understand their scale and skewness.

```{python}
# | label: fig-key-feature-distributions
# | fig-cap: "Distributions of selected key features with mean indicators."
key_features = [
    LECol.LIFE_EXPECTANCY,
    LECol.ADULT_MORTALITY,
    LECol.INFANT_DEATHS,
    LECol.GDP,
    LECol.SCHOOLING,
    LECol.BMI,
]

# Create distribution plots
fig, axes = plt.subplots(3, 2, figsize=(14, 12))
axes = axes.flatten()

for idx, col in enumerate(key_features):
    data = df[col].dropna()

    sns.histplot(
        data=data,
        kde=True,
        ax=axes[idx],
        color="steelblue",
        edgecolor="black",
    )

    pretty_name = le_ds.get_pretty_name(col)
    axes[idx].set_title(f"{pretty_name}", fontsize=12, fontweight="bold")
    axes[idx].set_xlabel(pretty_name)
    axes[idx].set_ylabel("Frequency")

    axes[idx].axvline(
        data.mean(),
        color="red",
        linestyle="--",
        linewidth=2,
        label=f"Mean: {data.mean():.1f}",
    )
    axes[idx].legend(fontsize=9)
    axes[idx].grid(axis="y", alpha=0.3)

plt.tight_layout()
plt.suptitle(
    "Feature Distributions - Key Health & Economic Indicators",
    y=1.01,
    fontsize=14,
    fontweight="bold",
)
plt.show()
```

Life expectancy and schooling are close to symmetric, making them suitable for standard scaling without prior transformation. In contrast, adult mortality, infant deaths, and GDP exhibit pronounced right skewness: most countries cluster at low to moderate values, while a small number of observations occupy extremely high ranges. Such tails are typical for epidemiological counts and economic variables and imply that classical Z-score detection will be conservative. BMI shows bimodality, reflecting the coexistence of under-nutrition and over-nutrition regimes across countries. Overall, the exploratory view suggests that robust, distribution-agnostic procedures are preferable for feature-level screening, and that log-type transforms are likely required for skewed predictors.

To confirm this pattern across all features, we visualize every numeric predictor on its raw scale.

```{python}
# | label: fig-all-feature-distributions
# | fig-cap: "Raw-scale distributions for all numeric features."
# Get grid dimensions
n_features = len(le_ds.feature_columns(include_target=True))
n_cols = 4
n_rows = (n_features + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 3.5))
axes = axes.flatten()

for idx, col in enumerate(le_ds.feature_columns(include_target=True)):
    data = df[col].dropna()

    sns.histplot(
        data=data,
        kde=True,
        ax=axes[idx],
        color="steelblue",
        edgecolor="black",
    )

    pretty_name = le_ds.get_pretty_name(col)
    axes[idx].set_title(f"{pretty_name}", fontsize=10, fontweight="bold")
    axes[idx].set_xlabel(pretty_name, fontsize=9)
    axes[idx].set_ylabel("Frequency", fontsize=9)

    axes[idx].axvline(
        data.mean(),
        color="red",
        linestyle="--",
        linewidth=1.5,
        label=f"Mean: {data.mean():.1f}",
    )
    axes[idx].axvline(
        data.median(),
        color="green",
        linestyle="--",
        linewidth=1.5,
        label=f"Median: {data.median():.1f}",
    )
    axes[idx].legend(fontsize=7)
    axes[idx].grid(axis="y", alpha=0.3)

for idx in range(n_features, len(axes)):
    axes[idx].set_visible(False)

plt.tight_layout()
plt.suptitle(
    f"Complete Feature Distributions - All {n_features} Numeric Features",
    y=1.001,
    fontsize=14,
    fontweight="bold",
)
plt.show()
```

The full overview reinforces the heterogeneity of marginal shapes. Count-type variables (measles, HIV/AIDS, infant and under-five deaths, population) are highly skewed, while immunization coverage rates concentrate near high values with a left tail capturing low-coverage countries. This mixture of bounded proportions, counts, and economic magnitudes makes it unlikely that a single normality-based threshold will behave uniformly well.

Vaccination coverage variables are provided as percentages. To emphasize shortfall rather than saturation, we also inspect their distributions after converting to proportions of missing coverage.

```{python}
# | label: fig-coverage-shortfall
# | fig-cap: "Vaccination shortfall distributions for Polio, Hepatitis B, and Diphtheria."
cov_cols = [LECol.POLIO, LECol.HEPATITIS_B, LECol.DIPHTHERIA]

coverage = le_ds.df[cov_cols].copy() / 100
coverage = coverage.assign(**{col: 1 - coverage[col] for col in cov_cols})

fig, axes = plt.subplots(1, 3, figsize=(14, 4))
axes = axes.flatten()
for idx, col in enumerate(cov_cols):
    data = coverage[col].dropna()

    sns.histplot(
        data=data,
        kde=True,
        ax=axes[idx],
        color="steelblue",
        edgecolor="black",
    )

    pretty_name = le_ds.get_pretty_name(col)
    axes[idx].set_title(f"{pretty_name}", fontsize=10, fontweight="bold")
    axes[idx].set_xlabel(f"{pretty_name} - lack of coverage (proportion)", fontsize=9)
    axes[idx].set_ylabel("Frequency", fontsize=9)

    axes[idx].axvline(
        data.mean(),
        color="red",
        linestyle="--",
        linewidth=1.5,
        label=f"Mean: {data.mean():.2f}",
    )
    axes[idx].axvline(
        data.median(),
        color="green",
        linestyle="--",
        linewidth=1.5,
        label=f"Median: {data.median():.2f}",
    )
    axes[idx].legend(fontsize=7)
    axes[idx].grid(axis="y", alpha=0.3)

plt.tight_layout()
```

The transformed coverage variables show a pronounced mass near zero shortfall and a thin right tail, indicating that the most relevant outliers are countries with unusually low vaccination uptake.

Given these shapes, we also inspect all features under a log transform to reveal structure in heavy-tailed variables.

```{python}
# | label: fig-all-features-log
# | fig-cap: "Log-scaled distributions for all numeric features."
# Get grid dimensions
n_features = len(le_ds.feature_columns(include_target=True))
n_cols = 4
n_rows = (n_features + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 3.5))
axes = axes.flatten()

for idx, col in enumerate(le_ds.feature_columns(include_target=True)):
    data = np.log1p(df[col].dropna())

    sns.histplot(
        data=data,
        kde=True,
        ax=axes[idx],
        color="steelblue",
        edgecolor="black",
    )

    pretty_name = le_ds.get_pretty_name(col)
    axes[idx].set_title(f"{pretty_name}", fontsize=10, fontweight="bold")
    axes[idx].set_xlabel(f"{pretty_name.replace(')', '')} - log scale)", fontsize=9)
    axes[idx].set_ylabel("Frequency", fontsize=9)

    axes[idx].axvline(
        np.mean(data),
        color="red",
        linestyle="--",
        linewidth=1.5,
        label=f"Mean: {np.mean(data):.1f}",
    )
    axes[idx].axvline(
        np.median(data),
        color="green",
        linestyle="--",
        linewidth=1.5,
        label=f"Median: {np.median(data):.1f}",
    )
    axes[idx].legend(fontsize=7)
    axes[idx].grid(axis="y", alpha=0.3)

for idx in range(n_features, len(axes)):
    axes[idx].set_visible(False)

plt.tight_layout()
plt.suptitle(
    f"Complete Feature Distributions - All {n_features} Numeric Features",
    y=1.001,
    fontsize=14,
    fontweight="bold",
)
plt.show()

# TODO: group features by type (demographic, economic, vaccination coverage, index for infectious diseases, etc.)
# TODO: which features need log or power transformation to reduce skewness?
```

After log transformation, the extremely skewed predictors become substantially more regular, supporting the use of $\log(1+x)$-style transforms in later modelling. Importantly, however, transformation does not remove the possibility of structurally extreme countries, so we proceed with explicit outlier detection.

## Outlier detection methods

### IQR rule

The IQR method flags values beyond Tukey fences, defined as

$$
Q_1 - k\,IQR \leq x \leq Q_3 + k\,IQR,
$$

where $IQR = Q_3 - Q_1$ and $k=1.5$ by convention. Because this rule depends only on empirical quartiles, it is robust to skewness and heavy tails.

```{python}
# | label: iqr-outliers
# Use factory method to create IQR detector
iqr_detector = le_ds.make_iqr_outlier_detector(
    columns=None,  # None = all numeric features
    standardized=False,
    threshold=1.5,
)

iqr_result = iqr_detector.fit().result()

print("=== IQR Method Results ===")
print(
    f"Total data points: {iqr_result.outlier_mask.shape[0] * iqr_result.outlier_mask.shape[1]:,}",
)
print(
    f"\nRows with at least one outlier: {iqr_result.outlier_mask.any(axis=1).sum()} / {len(iqr_result.outlier_mask)}",
)

print("\n=== Features with Most Outliers ===")
feature_outliers_df = (
    iqr_result.n_outliers_per_column.sort_values(ascending=False)
    .head(10)
    .to_frame(name="Count")
)
feature_outliers_df["Percentage"] = (
    feature_outliers_df["Count"] / len(df) * 100
).round(1)
feature_outliers_df.index = [
    iqr_result.pretty_names.get(col, col) for col in feature_outliers_df.index
]
feature_outliers_df.index.name = "Feature"
display(feature_outliers_df)
```

On the longitudinal data, IQR flags outlying values in many skewed indicators, particularly measles cases, HIV/AIDS prevalence, population, and child mortality counts. This is expected given their long-tailed distributions. At the row level, about two thirds of country-year observations contain at least one IQR-flagged feature, emphasizing that IQR is sensitive to marginal extremes but still provides interpretable, feature-specific diagnostics.

### Z-score threshold

The Z-score method standardizes each feature and flags observations with $|z| > 3$, which corresponds to very rare events under normality. Its usefulness therefore depends on approximate Gaussian behavior.

```{python}
# | label: zscore-outliers
# Use factory method to create Z-Score detector
zscore_detector = le_ds.make_zscore_outlier_detector(
    columns=None,
    standardized=True,
    threshold=3.0,
)

zscore_result = zscore_detector.fit().result()

print("=== Z-Score Method Results ===")
print(
    f"Total data points: {zscore_result.outlier_mask.shape[0] * zscore_result.outlier_mask.shape[1]:,}",
)
print(
    f"\nRows with at least one outlier: {zscore_result.outlier_mask.any(axis=1).sum()} / {len(zscore_result.outlier_mask)}",
)

print("\n=== Features with Most Outliers ===")
feature_outliers_df = (
    zscore_result.n_outliers_per_column.sort_values(ascending=False)
    .head(10)
    .to_frame(name="Count")
)
feature_outliers_df["Percentage"] = (
    feature_outliers_df["Count"] / len(df) * 100
).round(1)
feature_outliers_df.index = [
    zscore_result.pretty_names.get(col, col) for col in feature_outliers_df.index
]
feature_outliers_df.index.name = "Feature"
display(feature_outliers_df)
```

As anticipated, the Z-score rule is far more conservative: only around one fifth of rows exhibit any Z-score outlier. The method concentrates on predictors that are closer to symmetric after standardization, notably immunization rates and expenditure ratios. For strongly skewed count variables, Z-scores rarely exceed the Gaussian tail threshold, so this detector risks overlooking substantively extreme countries in epidemiological indicators.

### Isolation Forest

Isolation Forest is a multivariate anomaly detector that isolates rare observations with short average path lengths in random trees. It produces a row-wise notion of atypicality, rather than feature-wise fences.

```{python}
# | label: iforest-outliers
# Use factory method to create Isolation Forest detector
iforest_detector = le_ds.make_isolation_forest_outlier_detector(
    columns=None,
    standardized=True,
    contamination="auto",
    n_estimators=100,
)

iforest_result = iforest_detector.fit().result()

print("=== Isolation Forest Method Results ===")
print(
    f"Total data points: {iforest_result.outlier_mask.shape[0] * iforest_result.outlier_mask.shape[1]:,}",
)
print(
    f"\nRows with at least one outlier: {iforest_result.outlier_mask.any(axis=1).sum()} / {len(iforest_result.outlier_mask)}",
)

print("\n=== Features with Most Outliers ===")
feature_outliers_df = (
    iforest_result.n_outliers_per_column.sort_values(ascending=False)
    .head(10)
    .to_frame(name="Count")
)
feature_outliers_df["Percentage"] = (
    feature_outliers_df["Count"] / len(df) * 100
).round(1)
feature_outliers_df.index = [
    iforest_result.pretty_names.get(col, col) for col in feature_outliers_df.index
]
feature_outliers_df.index.name = "Feature"
display(feature_outliers_df)
```

Isolation Forest flags a comparatively small subset of country-years as multivariate anomalies. Because the detector works on standardized features jointly, the resulting outlier mask is dense across columns for the same rows: if a row is anomalous, many of its feature values inherit an outlier label. This behavior is useful for screening globally inconsistent country profiles, but less informative for identifying which individual indicators are extreme.

## Method comparison

We next compare the percentage of outlying values per feature across methods.

```{python}
# | label: fig-method-comparison-features
# | fig-cap: "Percentage of outliers per feature by detection method."
comparison_data = pd.DataFrame(
    {
        "IQR": iqr_result.n_outliers_per_column,
        "Z-Score": zscore_result.n_outliers_per_column,
        "Isolation Forest": iforest_result.n_outliers_per_column,
    },
).fillna(0)

pretty_labels = [iqr_result.pretty_names.get(col, col) for col in comparison_data.index]

comparison_pct = comparison_data / len(df) * 100
comparison_pct_long = comparison_pct.reset_index()
comparison_pct_long.columns = ["Feature", "IQR", "Z-Score", "Isolation Forest"]
comparison_pct_long["Feature"] = pretty_labels
comparison_pct_long_melted = comparison_pct_long.melt(
    id_vars="Feature",
    var_name="Method",
    value_name="Percentage",
)

fig, ax = plt.subplots(figsize=(16, 8))

sns.barplot(
    data=comparison_pct_long_melted,
    x="Feature",
    y="Percentage",
    hue="Method",
    palette=["steelblue", "coral", "mediumseagreen"],
    ax=ax,
)
ax.set_title(
    "Outlier Detector Comparison (Percentage of Samples)",
    fontsize=14,
    fontweight="bold",
)
ax.set_xlabel("Feature", fontsize=12)
ax.set_ylabel("Percentage of Outliers (%)", fontsize=12)
ax.legend(title="Method", fontsize=11)
ax.tick_params(axis="x", rotation=45)
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha="right")
ax.grid(axis="y", alpha=0.3)

sns.despine(trim=True)
plt.tight_layout()
plt.show()
```

The comparison makes the trade-offs explicit. IQR yields moderate to high outlier rates for heavy-tailed variables but remains transparent about which features drive the labels. Z-scores detect outliers almost exclusively in features that are close to normal after standardization, and therefore under-represent extremes in skewed predictors. Isolation Forest behaves as an anomaly screen, assigning outlier labels to a small set of rows but spreading them across many columns. In terms of average feature-level rates, IQR and Isolation Forest are of similar magnitude, while Z-scores are an order of magnitude smaller.

A country-level comparison shows which observations are most frequently flagged.

```{python}
# | label: fig-method-comparison-countries
# | fig-cap: "Top 20 country-year observations by outlier count across methods."
country_comparison_data = pd.DataFrame(
    {
        "IQR": iqr_result.n_outliers_per_row,
        "Z-Score": zscore_result.n_outliers_per_row,
        "Isolation Forest": iforest_result.n_outliers_per_row,
    },
)

country_comparison_filtered = country_comparison_data[
    (country_comparison_data > 0).any(axis=1)
]

top_countries = (
    country_comparison_filtered.sum(axis=1).sort_values(ascending=False).head(20).index
)

country_comparison_top = country_comparison_filtered.loc[top_countries]
country_comparison_top["Country"] = country_comparison_top.index
country_comparison_melted = country_comparison_top.melt(
    id_vars="Country",
    var_name="Method",
    value_name="Outlier Count",
)

fig, ax = plt.subplots(figsize=(16, 10))

sns.barplot(
    data=country_comparison_melted,
    x="Country",
    y="Outlier Count",
    hue="Method",
    palette=["steelblue", "coral", "mediumseagreen"],
    ax=ax,
)
ax.set_title(
    "Outlier Detection by Country: Top 20 Countries\n(Number of Features Flagged as Outliers)",
    fontsize=14,
    fontweight="bold",
)
ax.set_xlabel("Country", fontsize=12)
ax.set_ylabel("Number of Outlier Features", fontsize=12)
ax.legend(title="Method", fontsize=11)
ax.tick_params(axis="x", rotation=45)
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha="right")
ax.grid(axis="y", alpha=0.3)

sns.despine(trim=True)
plt.tight_layout()
plt.show()
```

Countries with repeated extreme values across years and indicators dominate the top ranks under IQR. Z-scores tend to highlight distinct cases in coverage and expenditure, while Isolation Forest isolates a limited set of globally unusual profiles. This heterogeneity indicates that extreme values are not merely random noise but correspond to different structural dimensions of the data.

## Agreement across methods

```{python}
# | label: outlier-agreement
iqr_outlier_rows = iqr_result.outlier_mask.any(axis=1)
zscore_outlier_rows = zscore_result.outlier_mask.any(axis=1)
iforest_outlier_rows = iforest_result.outlier_mask.any(axis=1)

iqr_outlier_rows = iqr_outlier_rows.reindex(df.index, fill_value=False)
zscore_outlier_rows = zscore_outlier_rows.reindex(df.index, fill_value=False)
iforest_outlier_rows = iforest_outlier_rows.reindex(df.index, fill_value=False)

all_three = iqr_outlier_rows & zscore_outlier_rows & iforest_outlier_rows
iqr_zscore = iqr_outlier_rows & zscore_outlier_rows & ~iforest_outlier_rows
iqr_iforest = iqr_outlier_rows & ~zscore_outlier_rows & iforest_outlier_rows
zscore_iforest = ~iqr_outlier_rows & zscore_outlier_rows & iforest_outlier_rows
only_iqr = iqr_outlier_rows & ~zscore_outlier_rows & ~iforest_outlier_rows
only_zscore = ~iqr_outlier_rows & zscore_outlier_rows & ~iforest_outlier_rows
only_iforest = ~iqr_outlier_rows & ~zscore_outlier_rows & iforest_outlier_rows

print(
    f"- Countries flagged by all 3 methods: {all_three.sum()}:\n{df.index[all_three].tolist()}",
)
print(
    f"- Countries flagged by 2 methods: {(iqr_zscore.sum() + iqr_iforest.sum() + zscore_iforest.sum())}:\n{df.index[iqr_zscore | iqr_iforest | zscore_iforest].tolist()}",
)
print(
    f"- Countries flagged by only 1 method: {(only_iqr.sum() + only_zscore.sum() + only_iforest.sum())}",
)
print(f"\nUnique outliers per method:")
print(f"  - IQR only: {only_iqr.sum()}")
print(f"  - Z-Score only: {only_zscore.sum()}")
print(f"  - Isolation Forest only: {only_iforest.sum()}")
```

Agreement is partial rather than universal. A substantial subset of observations is flagged by all three methods, representing robust anomalies that are extreme both marginally and jointly. At the same time, each method identifies a large pool of unique cases, consistent with their distinct assumptions. This supports using IQR for feature-level screening while treating Isolation Forest as an auxiliary multivariate anomaly check.

### Feature-wise comparison with the target

To illustrate how the detectors behave on the same feature-target relationship, we plot life expectancy against each predictor and mark outliers per method.

```{python}
# | label: fig-selected-features-comparison
# | fig-cap: "Outlier labeling by method across all predictors versus life expectancy."
selected_comparison_features = iqr_result.outlier_mask.columns.tolist()


fig, axes = plt.subplots(
    len(selected_comparison_features),
    3,
    figsize=(18, len(selected_comparison_features) * 4),
)

results = [
    ("IQR", iqr_result, "steelblue", "red"),
    ("Z-Score", zscore_result, "coral", "darkred"),
    ("Isolation Forest", iforest_result, "mediumseagreen", "purple"),
]

all_three_mask = (
    iqr_result.outlier_mask.any(axis=1).reindex(df.index, fill_value=False)
    & zscore_result.outlier_mask.any(axis=1).reindex(df.index, fill_value=False)
    & iforest_result.outlier_mask.any(axis=1).reindex(df.index, fill_value=False)
)

for row_idx, feature in enumerate(selected_comparison_features):
    for col_idx, (method_name, result, color_normal, color_outlier) in enumerate(
        results,
    ):
        ax = axes[row_idx, col_idx]

        feature_outliers = result.outlier_mask[feature].reindex(
            df.index, fill_value=False
        )

        normal_mask = ~feature_outliers
        ax.scatter(
            df.loc[normal_mask, feature],
            df.loc[normal_mask, LECol.LIFE_EXPECTANCY],
            alpha=0.5,
            s=40,
            color=color_normal,
            label="Normal",
            edgecolors="none",
        )

        single_method_outliers = feature_outliers & ~all_three_mask
        if single_method_outliers.any():
            ax.scatter(
                df.loc[single_method_outliers, feature],
                df.loc[single_method_outliers, LECol.LIFE_EXPECTANCY],
                alpha=0.9,
                s=80,
                color=color_outlier,
                marker="^",
                label=f"Outlier (n={single_method_outliers.sum()})",
                linewidth=1,
            )

            if single_method_outliers.sum() < 15:
                for country in df.index[single_method_outliers]:
                    ax.annotate(
                        country,
                        xy=(
                            df.loc[country, feature],
                            df.loc[country, LECol.LIFE_EXPECTANCY],
                        ),
                        xytext=(3, 3),
                        textcoords="offset points",
                        fontsize=6,
                        alpha=0.7,
                        color=color_outlier,
                    )

        all_three_outliers = feature_outliers & all_three_mask
        if all_three_outliers.any():
            ax.scatter(
                df.loc[all_three_outliers, feature],
                df.loc[all_three_outliers, LECol.LIFE_EXPECTANCY],
                alpha=1.0,
                s=120,
                color="black",
                marker="*",
                label=f"All Methods (n={all_three_outliers.sum()})",
                linewidth=2,
                edgecolors="yellow",
            )

            for country in df.index[all_three_outliers]:
                ax.annotate(
                    country,
                    xy=(
                        df.loc[country, feature],
                        df.loc[country, LECol.LIFE_EXPECTANCY],
                    ),
                    xytext=(5, 5),
                    textcoords="offset points",
                    fontsize=6,
                    fontweight="bold",
                    alpha=0.9,
                    color="black",
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.3),
                )

        feature_pretty = le_ds.get_pretty_name(feature)
        target_pretty = le_ds.get_pretty_name(LECol.LIFE_EXPECTANCY)

        if row_idx == len(selected_comparison_features) - 1:
            ax.set_xlabel(feature_pretty, fontsize=10)

        if col_idx == 0:
            ax.set_ylabel(target_pretty, fontsize=10)

        if row_idx == 0:
            ax.set_title(
                f"{method_name}\n{feature_pretty}",
                fontsize=11,
                fontweight="bold",
            )
        else:
            ax.set_title(f"{feature_pretty}", fontsize=10)

        ax.legend(fontsize=8, loc="best")
        ax.grid(alpha=0.2)

plt.tight_layout()
plt.suptitle(
    "Outlier Detection Across Different Algorithms",
    y=1.001,
    fontsize=14,
    fontweight="bold",
)
plt.show()
```

Across predictors, IQR isolates feature-specific extremes along the horizontal axis, while Z-scores highlight only the most isolated points after standardization. Isolation Forest tends to label entire vertical bands corresponding to multivariate anomalies, which is informative for holistic screening but not for pinpointing which predictors drive atypicality.

## Method selection and cleaning decision

The distributional evidence and method comparison indicate that the IQR rule is the most appropriate default for this dataset. It does not rely on normality assumptions, is easily interpretable, and retains feature-level granularity, which is crucial when later assessing how specific health or socio-economic indicators relate to life expectancy. Z-scores remain useful as a conservative cross-check on near-Gaussian features, and Isolation Forest provides a multivariate anomaly audit, particularly relevant for longitudinal patterns, but neither is as suitable as a primary feature-wise detector in the present setting.

We therefore create a cleaned longitudinal dataset by removing observations that display more than four IQR outlier features. This threshold is intentionally moderate: it avoids dropping countries based on a single extreme indicator, while still excluding globally inconsistent rows.

```{python}
# | label: cleaning-iqr
# Create cleaned dataset by removing rows with outliers (using IQR method)
rows_with_outliers = (iqr_result.outlier_mask.sum(axis=1) > 4).reindex(df.index, fill_value=False)
cleaned_data = df[~rows_with_outliers].copy()

print("=== Data Cleaning Summary (IQR Method) ===")
print(f"Original dataset: {len(df)} countries")
print(f"Countries with outliers: {rows_with_outliers.sum()}")
print(f"Cleaned dataset: {len(cleaned_data)} countries")
print(
    f"Removed: {len(df) - len(cleaned_data)} countries ({(len(df) - len(cleaned_data)) / len(df) * 100:.2f}%)",
)

print("\n=== Removed Countries ===")
removed_countries = df.index[rows_with_outliers].sort_values()
print(f"Total: {len(removed_countries)}")
print(f"Countries: {', '.join(map(str, removed_countries.tolist()))}")
```

The applied rule removes 110 of 1,790 country-year observations ($\approx 6.1\%$). The effect is a modest reduction in marginal variance for heavy-tailed predictors while preserving the overall global structure of the data.

```{python}
# | label: tbl-before-after-stats
print("\n=== Before vs After Cleaning Statistics ===")

for col in le_ds.feature_columns(include_target=True):
    print(f"\n{le_ds.get_pretty_name(col)}:")
    print(f"  Before - Mean: {df[col].mean():.2f}, Std: {df[col].std():.2f}")
    print(
        f"  After  - Mean: {cleaned_data[col].mean():.2f}, Std: {cleaned_data[col].std():.2f}",
    )
    print(
        f"  Change - Mean: {((cleaned_data[col].mean() - df[col].mean()) / df[col].mean() * 100):+.2f}%, "
        f"Std: {((cleaned_data[col].std() - df[col].std()) / df[col].std() * 100):+.2f}%",
    )
```

Finally, we compare the full set of feature distributions before and after cleaning.

```{python}
# | label: fig-before-after-kde
# | fig-cap: "Kernel density estimates of each feature before and after IQR-based cleaning."
all_features = le_ds.feature_columns(include_target=True)
n_features = len(all_features)
n_cols = 3
n_rows = int(np.ceil(n_features / n_cols))

fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))
axes = axes.flatten()

for idx, col in enumerate(all_features):
    sns.kdeplot(
        data=df[col].dropna(),
        ax=axes[idx],
        color="red",
        label="Before Cleaning",
        fill=True,
        alpha=0.3,
        linewidth=2,
    )

    sns.kdeplot(
        data=cleaned_data[col].dropna(),
        ax=axes[idx],
        color="green",
        label="After Cleaning",
        fill=True,
        alpha=0.3,
        linewidth=2,
    )

    pretty_name = le_ds.get_pretty_name(col)
    axes[idx].set_title(f"{pretty_name}")
    axes[idx].set_xlabel(pretty_name)
    axes[idx].set_ylabel("Density")
    axes[idx].legend(fontsize=8, loc="best")
    axes[idx].grid(alpha=0.3)

for idx in range(n_features, len(axes)):
    axes[idx].set_visible(False)

plt.tight_layout()
plt.suptitle(
    "Distribution Changes After Outlier Removal",
    y=1.001,
    fontsize=12,
)
plt.show()
```

The cleaned distributions are slightly more concentrated around their central mass, especially for strongly skewed predictors, but their qualitative shapes remain intact. This supports the view that the cleaning step removes only the most extreme inconsistencies rather than altering the substantive signal. For sensitivity analyses, we retain the original data and consider alternative treatments such as winsorization or additional transformations where domain knowledge suggests that extreme values may be genuine rather than erroneous.

## Outlier screening in the 2014 cross-section

The method comparison above is performed on the longitudinal panel to understand how detector assumptions interact with the distributional shapes of the predictors. Since the remainder of this report is based on the single-year 2014 cross-section, we also provide a compact outlier screen for that snapshot. This connects the methodological discussion to the dataset that enters correlation analysis, PCA, clustering, and regression.

```{python}
# | label: tbl-outliers-2014-summary
# | tbl-cap: "Outlier detectors on the standardized 2014 cross-section."
le_2014 = LifeExpectancyDataset.from_csv(aggregate_by_country=2014)

cols_2014 = le_2014.feature_columns(include_target=False)

iqr_2014 = le_2014.make_iqr_outlier_detector(
    columns=cols_2014,
    standardized=True,
    threshold=1.5,
).fit().result()

zscore_2014 = le_2014.make_zscore_outlier_detector(
    columns=cols_2014,
    standardized=True,
    threshold=3.0,
).fit().result()

iforest_2014 = le_2014.make_isolation_forest_outlier_detector(
    columns=cols_2014,
    standardized=True,
    contamination=0.07,
    n_estimators=200,
).fit().result()

pd.DataFrame(
    {
        "rows_flagged": {
            "iqr": int(iqr_2014.n_outliers_per_row.gt(0).sum()),
            "zscore": int(zscore_2014.n_outliers_per_row.gt(0).sum()),
            "isolation_forest": int(iforest_2014.n_outliers_per_row.gt(0).sum()),
        },
        "max_feature_outliers": {
            "iqr": int(iqr_2014.n_outliers_per_column.max()),
            "zscore": int(zscore_2014.n_outliers_per_column.max()),
            "isolation_forest": int(iforest_2014.n_outliers_per_column.max()),
        },
        "n_features": len(cols_2014),
        "n_rows": len(le_2014.df),
    }
)
```

```{python}
# | label: tbl-outliers-2014-iqr-top
# | tbl-cap: "2014 cross-section: IQR features with most flagged values."
iqr_2014.n_outliers_per_column.sort_values(ascending=False).head(10)
```

```{python}
# | label: tbl-outliers-2014-zscore-top
# | tbl-cap: "2014 cross-section: Z-score features with most flagged values."
zscore_2014.n_outliers_per_column.sort_values(ascending=False).head(10)
```

In the 2014 snapshot, IQR remains the most informative feature-level screen because it tolerates skewed epidemiological counts while still surfacing extreme shortfalls in immunization coverage. Z-scores are intentionally stringent and therefore primarily flag unusually low vaccination coverage and atypical expenditure ratios after standardization. Isolation Forest operates at the row level and is best interpreted as an anomaly audit rather than a feature-wise trimming rule.

For the baseline analyses in later chapters, we retain the full 2014 cross-section. Outlier flags are instead used for sensitivity checks, for example by winsorizing extreme predictor values or by refitting models after excluding a small set of flagged observations. This avoids conflating statistical atypicality with substantive invalidity, which is particularly important in global health data where extreme values can reflect genuine country-level realities.
