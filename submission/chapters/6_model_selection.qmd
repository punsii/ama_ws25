---
title: "Model Selection"
format:
  html:
    toc: true
    code-fold: true
execute:
  warning: false
---

<style>
  figcaption {
    text-align: center;
  }
</style>

```{python}
# | label: init
# | include: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm
from IPython.display import Markdown, display

from ama_tlbx.analysis import FeatureGroup, ColumnConcatenator
from ama_tlbx.analysis.model_registry import ModelRegistry
from ama_tlbx.analysis.model_selection import collect_selection_paths
from ama_tlbx.data import LECol, LifeExpectancyDataset
from ama_tlbx.utils.plotting_config import DEFAULT_PLOT_CFG

np.random.seed(0)
DEFAULT_PLOT_CFG.apply_global()

TRAIN_YEAR = 2014

def get_pretty_name(col_name):
    """Resolve pretty name from metadata or fallback to title case."""
    try:
        return LECol(col_name).metadata().pretty_name
    except ValueError:
        return col_name.replace("_", " ").title()
```

# Motivation

Model selection balances fit and complexity.
We want models that explain life expectancy well, but avoid overfitting by adding unnecessary predictors.
We perform selection on the transformed and standardized 2014 cross-section.
HDI is excluded from the candidate set to prevent target leakage, as established in previous chapters.

```{python}
# | label: data-loading
# Define Feature Groups
feature_groups = [
    FeatureGroup(
        name="child_mortality",
        features=[LECol.INFANT_DEATHS, LECol.UNDER_FIVE_DEATHS],
    ),
    FeatureGroup(
        name="child_nutrition",
        features=[LECol.THINNESS_5_9_YEARS, LECol.THINNESS_1_19_YEARS],
    ),
    FeatureGroup(
        name="economic_development",
        features=[LECol.GDP, LECol.PERCENTAGE_EXPENDITURE],
    ),
    FeatureGroup(
        name="immunization",
        features=[
            LECol.DIPHTHERIA,
            LECol.HEPATITIS_B,
            LECol.MEASLES,
            LECol.POLIO,
        ],
    ),
]

# Load and process data
train_ds_raw = LifeExpectancyDataset.from_csv_updated(
    aggregate_by_country=TRAIN_YEAR,
    resolve_nand_pred="carry_forward",
    drop_remaining_nan=True,
)

# Apply Feature Groups
cols_to_drop = []
for group in feature_groups:
    concatenator = ColumnConcatenator(train_ds_raw)
    train_ds_raw = concatenator.concatenate(
        columns=group.features,
        new_column_name=group.name
    )
    cols_to_drop.extend(group.features)

# Transform, Normalize, and drop Year
df_train = train_ds_raw.tf_and_norm().drop(columns=[LECol.YEAR])

# Drop HDI and columns of the feature groups
df_train = df_train.drop(
    columns=["human_development_index"] + cols_to_drop,
    errors="ignore"
)
df_train = df_train.select_dtypes(include=["number"])

# Define candidate predictors
target_col = LECol.TARGET
candidate_cols = [c for c in df_train.columns if c != target_col]
```

# Information Criteria

We calculate AIC and BIC using both forward selection and backward selection.

```{python}
# | label: run-ic-selection
registry = ModelRegistry()

strategies = ["AIC_Forward", "AIC_Backward", "BIC_Forward", "BIC_Backward"]

# 1. AIC Forward
registry.fit_stepwise(
    df_train,
    base_terms=[],
    candidates=candidate_cols,
    direction="forward",
    criterion="aic",
    threshold=-np.inf,
    name="AIC_Forward",
)

# 2. AIC Backward
registry.fit_stepwise(
    df_train,
    base_terms=[],
    candidates=candidate_cols,
    direction="backward",
    criterion="aic",
    threshold=-np.inf,
    name="AIC_Backward",
)

# 3. BIC Forward
registry.fit_stepwise(
    df_train,
    base_terms=[],
    candidates=candidate_cols,
    direction="forward",
    criterion="bic",
    threshold=-np.inf,
    name="BIC_Forward",
)

# 4. BIC Backward
registry.fit_stepwise(
    df_train,
    base_terms=[],
    candidates=candidate_cols,
    direction="backward",
    criterion="bic",
    threshold=-np.inf,
    name="BIC_Backward",
)

path_dict = {name: registry.get(name).selection_path for name in strategies}
df_paths = collect_selection_paths(path_dict)
```

The plot below shows the evolution of the information criteria.
The vertical alignment of markers shows that for AIC and BIC, the forward and backward selection each produce very similar paths and lead to the same optimal model.

```{python}
# | label: plot-ic-paths
# | fig-cap: "AIC and BIC scores vs Number of Features for Forward and Backward selection."

fig, ax = plt.subplots()

colors = ["tab:blue", "tab:cyan", "tab:red", "tab:orange"]
markers = ["o", "x", "o", "x"]

for name, color, marker in zip(strategies, colors, markers):
    subset = df_paths[df_paths["label"] == name].sort_values("n_terms")
    metric_name = subset["criterion"].iloc[0]

    ax.plot(
        subset["n_terms"],
        subset[metric_name],
        label=name,
        color=color,
        marker=marker,
        alpha=0.7,
    )

min_features = int(df_paths["n_terms"].min())
max_features = int(df_paths["n_terms"].max())

ax.set_xticks(np.arange(min_features, max_features + 1))
ax.set_xlabel("Number of Features (n_terms)")
ax.set_ylabel("Score (AIC / BIC)")
ax.set_title("Information Criteria vs Model Complexity")
ax.legend()
ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)

plt.show()
```
## Results Summary

In the analysis above, AIC Forward and Backward selection converged to the identical model.
Similarly, BIC Forward and Backward converged to the same, sparser model.
Because the strategies converged, we report the results in a combined table below.
The values represent the standardized coefficients from the final models.
Features are sorted by their impact in the AIC model.

```{python}
# | label: feature-comparison-table

coefficients = {}
representative_strategies = {"AIC": "AIC_Forward", "BIC": "BIC_Forward"}

for label, strategy_key in representative_strategies.items():
    path_res = registry.get(strategy_key).selection_path
    best_step = path_res.best_step()
    # Normalize intercept name to "Intercept" to handle both 'const' and 'Intercept'
    params = best_step.model.params.rename(index={"const": "Intercept"})
    coefficients[label] = params

df_compare = pd.DataFrame(coefficients)

# Reindex to include ALL candidate columns + Intercept
# ensuring we capture the intercept value correctly
all_candidates = ["Intercept"] + candidate_cols
df_compare = df_compare.reindex(all_candidates)

df_compare["sort_key"] = df_compare["AIC"].abs().fillna(0)
df_compare = df_compare.sort_values("sort_key", ascending=False).drop(columns="sort_key")

# Format names
def format_index(idx):
    if idx == "Intercept":
        return "Intercept"
    if idx == "hiv_aids":
        return "HIV/AIDS prevalence"
    return get_pretty_name(idx)

df_compare.index = [format_index(idx) for idx in df_compare.index]

styled_table = (
    df_compare.style
    .format("{:.3f}", na_rep="")
    .set_caption("Selected Features and Standardized Weights")
)

display(styled_table)
```

BIC penalizes model complexity more heavily than AIC, leading to a simpler model with only 5 features compared to the 7 features selected using AIC.

## Removing Mortality Indicators

'Adult Mortality' and 'Child Mortality' are by definition highly correlated with life expectancy and are the largest contributors to the previous models.
To understand what other factors drive life expectancy, we replicate the AIC and BIC selection process after removing these features from the candidate set.

```{python}
# | label: run-ic-selection-alt
exclude_vars = ["adult_mortality", "child_mortality"]
alt_candidate_cols = [c for c in candidate_cols if c not in exclude_vars]
alt_registry = ModelRegistry()
alt_strategies = ["AIC_Forward", "AIC_Backward", "BIC_Forward", "BIC_Backward"]

# 1. AIC Forward
alt_registry.fit_stepwise(
    df_train,
    base_terms=[],
    candidates=alt_candidate_cols,
    direction="forward",
    criterion="aic",
    threshold=-np.inf,
    name="AIC_Forward",
)

# 2. AIC Backward
alt_registry.fit_stepwise(
    df_train,
    base_terms=[],
    candidates=alt_candidate_cols,
    direction="backward",
    criterion="aic",
    threshold=-np.inf,
    name="AIC_Backward",
)

# 3. BIC Forward
alt_registry.fit_stepwise(
    df_train,
    base_terms=[],
    candidates=alt_candidate_cols,
    direction="forward",
    criterion="bic",
    threshold=-np.inf,
    name="BIC_Forward",
)

# 4. BIC Backward
alt_registry.fit_stepwise(
    df_train,
    base_terms=[],
    candidates=alt_candidate_cols,
    direction="backward",
    criterion="bic",
    threshold=-np.inf,
    name="BIC_Backward",
)

alt_path_dict = {name: alt_registry.get(name).selection_path for name in alt_strategies}
df_alt_paths = collect_selection_paths(alt_path_dict)
```

```{python}
# | label: plot-ic-paths-alt
# | fig-cap: "Alternative Model: AIC/BIC vs Number of Features (No Mortality Indicators)."

fig, ax = plt.subplots()

for name, color, marker in zip(alt_strategies, colors, markers):
    subset = df_alt_paths[df_alt_paths["label"] == name].sort_values("n_terms")
    metric_name = subset["criterion"].iloc[0]

    ax.plot(
        subset["n_terms"],
        subset[metric_name],
        label=name,
        color=color,
        marker=marker,
        alpha=0.7,
    )

min_feat = int(df_alt_paths["n_terms"].min())
max_feat = int(df_alt_paths["n_terms"].max())

ax.set_xticks(np.arange(min_feat, max_feat + 1))
ax.set_xlabel("Number of Features")
ax.set_ylabel("Score (AIC / BIC)")
ax.set_title("Alternative Model: Information Criteria vs Complexity")
ax.legend()
ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)

plt.show()
```

## Results without Mortality Indicators

```{python}
# | label: feature-comparison-table-alt

alt_coefficients = {}

for label, strategy_key in representative_strategies.items():
    if strategy_key in alt_registry.models:
        path_res = alt_registry.get(strategy_key).selection_path
        best_step = path_res.best_step()
        params = best_step.model.params.rename(index={"const": "Intercept"})
        alt_coefficients[label] = params

df_alt_compare = pd.DataFrame(alt_coefficients)

all_alt_candidates = ["Intercept"] + alt_candidate_cols
df_alt_compare = df_alt_compare.reindex(all_alt_candidates)

df_alt_compare["sort_key"] = df_alt_compare["AIC"].abs().fillna(0)
df_alt_compare = df_alt_compare.sort_values("sort_key", ascending=False).drop(columns="sort_key")

df_alt_compare.index = [format_index(idx) for idx in df_alt_compare.index]

styled_alt_table = (
    df_alt_compare.style
    .format("{:.3f}", na_rep="")
    .set_caption("Alternative Model: Selected Features and Weights")
)

display(styled_alt_table)
```

This time the forward and backward selection take the exact same paths.
BIC still results in 5 selected features and the AIC model uses 6.
The columns 'HIV/AIDS prevalence' as well as 'Schooling' are the most significant factors, despite not being part of the previous models.
This indicates that they are the best replacements for the now missing 'Adult Mortality' and 'Child Mortality' respectively.

# Forward selection using cross validation

To evaluate the generalization performance of our models, we compare two different error estimates:

1.  **In-Sample MSE**: The error on the same data used to fit the model. This naturally decreases as more variables are added.
2.  **Cross-Validation MSE**: The average error across 10 folds. This provides a robust estimate of the test error by using all available data for both training and testing.

```{python}
# | label: cv-comparison
# | fig-cap: "Comparison of In-Sample and 10-Fold Cross-Validation Error."

# Helper function to evaluate a selection path
def evaluate_path(train_df, test_df, candidate_cols, name):
    reg = ModelRegistry()
    reg.fit_stepwise(
        train_df,
        base_terms=[],
        candidates=candidate_cols,
        direction="forward",
        criterion="aic",
        threshold=-np.inf,
        name=name,
    )
    path = reg.get(name).selection_path

    results = []
    target = LECol.TARGET

    for step in path.steps:
        features = step.terms
        n_feats = len(features)

        # Prepare Design Matrices
        if n_feats == 0:
            X_train = sm.add_constant(pd.DataFrame(index=train_df.index))
            X_test = sm.add_constant(pd.DataFrame(index=test_df.index))
        else:
            X_train = sm.add_constant(train_df[features])
            X_test = sm.add_constant(test_df[features])

        y_train = train_df[target]
        y_test = test_df[target]

        # Refit OLS on training data
        model = sm.OLS(y_train, X_train).fit()

        # Predict on test (or train for in-sample)
        preds = model.predict(X_test)
        mse = mean_squared_error(y_test, preds)

        results.append({"n_terms": n_feats, "mse": mse})

    return pd.DataFrame(results)

df_insample = evaluate_path(df_train, df_train, candidate_cols, "insample")

n_folds = 10
kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)
cv_results_list = []

fold_idx = 1
for train_idx, val_idx in kf.split(df_train):
    cv_train = df_train.iloc[train_idx].copy()
    cv_val = df_train.iloc[val_idx].copy()

    fold_res = evaluate_path(cv_train, cv_val, candidate_cols, f"fold_{fold_idx}")
    fold_res["fold"] = fold_idx
    cv_results_list.append(fold_res)
    fold_idx += 1

df_cv_all = pd.concat(cv_results_list)
df_cv_agg = df_cv_all.groupby("n_terms")["mse"].mean().reset_index()

df_insample_plot = df_insample[df_insample["n_terms"] >= 1]
df_cv_plot = df_cv_agg[df_cv_agg["n_terms"] >= 1]

fig, ax = plt.subplots()

ax.plot(
    df_insample_plot["n_terms"],
    df_insample_plot["mse"],
    color="black",
    label="In-sample",
    linewidth=1.5
)
ax.plot(
    df_cv_plot["n_terms"],
    df_cv_plot["mse"],
    color="purple",
    marker="o",
    label="Cross-validated",
    markersize=4
)

# Find minimum for CV
best_row = df_cv_plot.loc[df_cv_plot["mse"].idxmin()]
min_n_features = int(best_row['n_terms'])

ax.axvline(
    min_n_features,
    color="gray",
    linestyle=":",
    label=f"Min CV Error (p={min_n_features})",
)

# Formatting
x_max = int(df_cv_plot["n_terms"].max())
ax.set_xticks(np.arange(1, x_max + 1))
ax.set_xlabel("Number of Features (n_terms)")
ax.set_ylabel("Mean Squared Error (MSE)")
ax.set_title("Model Selection: In-Sample vs CV (k >= 1)")
ax.legend()
ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)

plt.show()
```

```{python}
# | label: cv-feature-table
full_forward_path = registry.get("AIC_Forward").selection_path

models_by_k = {}
target_k_range = range(3, 13)

for step in full_forward_path.steps:
    k = len(step.terms)
    if k in target_k_range:
        params = step.model.params.rename(index={"const": "Intercept"})
        models_by_k[f"k={k}"] = params

df_k_comparison = pd.DataFrame(models_by_k)
all_candidates = ["Intercept"] + candidate_cols
df_k_comparison = df_k_comparison.reindex(all_candidates)

sort_col = "k=12"
df_k_comparison["sort_key"] = df_k_comparison[sort_col].abs().fillna(0)
df_k_comparison = df_k_comparison.sort_values("sort_key", ascending=False).drop(columns="sort_key")

df_k_comparison.index = [format_index(idx) for idx in df_k_comparison.index]

styled_k_table = (
    df_k_comparison.style
    .format("{:.3f}", na_rep="")
    .set_caption("Standardized Coefficients by Model Size (k)")
)

display(styled_k_table)
```

## Results and Discussion

## Results and Discussion

The model selection process yields consistent results across different methodologies.
This convergence indicates that the findings are robust and do not fluctuate drastically with different metrics.

To assess generalization, we compared the In-Sample Mean Squared Error against the 10-fold Cross-Validation statistics.
As expected the in-sample error decreased monotonically as more variables were introduced, while the cross-validation curve showed little sign of overfitting, even when the full set of predictors was included.

The analysis highlights a clear trade-off: while the complex model minimizes the absolute error, the simpler model with 5 features is sufficient to capture the vast majority of the predictive signal.

The cross-validation error curve reveals distinct minima at $k=5$ and $k=7$.
The global optimum at $k=5$ aligns with the BIC recommendation, while the local optimum at $k=7$ corresponds to the AIC selection.
This alignment between different methods and information criteria strengthens the validity of these candidate models.

Ultimately, the choice of the optimal model depends on the specific use case.
For maximum predictive accuracy, it is logical to retain highly correlated variables such as adult mortality and employ the larger AIC-favored model.
However, to understand the underlying drivers of life expectancy, the smaller BIC-favored model is preferable for two key reasons.
First, its simplicity isolates the most critical factors, making the relationships easier to interpret.
Second, excluding confounding mortality statistics prevents circular reasoning, allowing us to focus on the socio-economic factors.
