---
title: "Regression: Supervised PCA"
format:
  html:
    toc: true
    toc-depth: 3
execute:
  warning: false
  message: false
  cache: false
---

# Regression: Supervised PCA

This chapter evaluates a **supervised PCA regression (SPCA)** workflow for explaining and predicting life expectancy. The motivation is pragmatic: many candidate predictors are strongly correlated (e.g., mortality indicators, immunization rates, and development proxies), so standard OLS on raw features can be unstable and hard to interpret. SPCA compresses the predictor space into **orthogonal components that are pre-screened for outcome relevance**: we (i) screen predictors by their marginal association with life expectancy, (ii) run PCA on the screened subset, and (iii) regress life expectancy on the resulting principal components (PCs). This follows the supervised principal components idea of @Bair01032006.

We use SPCA to answer four focused questions:

1. **Model specification:** Do we need additional structure beyond additive PCs (e.g., a small number of PC–PC interactions) to improve fit?
2. **Diagnostics:** Do the OLS assumptions for the final PC model look acceptable (residual structure, heteroscedasticity, influence, autocorrelation)?
3. **Generalization:** How well does the fitted PC model transfer to other years, in particular to a holdout year (2011) and across the 2000–2014 range?

The workflow includes screening, AIC-based selection, interaction screening, and Cook’s-distance sensitivity trimming. Therefore, coefficient p-values and confidence intervals are best treated as **post-selection / exploratory inference**, and we emphasize stability, diagnostics, and out-of-sample performance.

```{python}
# | label: setup
# | include: false
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import display
import random

from ama_tlbx.analysis import FeatureGroup
from ama_tlbx.analysis.model_registry import ModelRegistry
from ama_tlbx.data import LECol, LifeExpectancyDataset
from ama_tlbx.plotting import plot_biplot_plotly, plot_loadings_heatmap
from ama_tlbx.utils.plotting_config import DEFAULT_PLOT_CFG

DEFAULT_PLOT_CFG.apply_global()
np.random.seed(42)
random.seed(42)

TRAIN_YEAR = 2014
HOLDOUT_YEAR = 2011
STATUS_DUMMY = "status_developed"
N_BOOTSTAP = 3000
```

## Data preparation

```{python}
# | label: data-prep
# | code-fold: true
train_ds = LifeExpectancyDataset.from_csv_updated(
    aggregate_by_country=TRAIN_YEAR,
    resolve_nand_pred="carry_forward",
)
holdout_ds = LifeExpectancyDataset.from_csv_updated(
    aggregate_by_country=HOLDOUT_YEAR,
    resolve_nand_pred="carry_forward",
)

train_full = train_ds.tf_and_norm().drop(columns=[LECol.YEAR])
holdout_full = holdout_ds.tf_and_norm().drop(columns=[LECol.YEAR])

if STATUS_DUMMY not in train_full.columns and LECol.STATUS in train_full.columns:
    train_full = train_full.assign(
        **{STATUS_DUMMY: train_full[LECol.STATUS].astype(int)}
    )
    holdout_full = holdout_full.assign(
        **{STATUS_DUMMY: holdout_full[LECol.STATUS].astype(int)}
    )

registry = ModelRegistry(eval_year=HOLDOUT_YEAR)
```

## Supervised PCA regression

```{python}
# | label: supervised-pca
# | code-fold: false
corr_result = (
    LifeExpectancyDataset(df=train_full)
    .make_correlation_analyzer(standardized=False, include_target=True)
    .fit()
    .result()
)

strong_features = (
    corr_result.target_correlations.assign(abs_cor=lambda d: d.correlation.abs())
    .query("abs_cor > 0.25")
    .feature.tolist()
)
strong_features = [
    f
    for f in strong_features
    if f
    not in [
        LECol.HDI,
    ]
]
print(strong_features)
```

:::{.callout-note collapse="true"}
### Supervised PCA fit
```{python}
# | label: tbl-supervised-pca-fit
# | code-fold: true
# | tbl-cap: "Reduced supervised-PCA regression (significant PCs)."
feature_groups = [FeatureGroup(name="supervised", features=strong_features)]

pca_result = (
    LifeExpectancyDataset(df=train_full)
    .make_pca_dim_reduction_analyzer(
        feature_groups=feature_groups,
        standardized=True,
        min_var_explained=1.0,
    )
    .fit()
    .result()
)

pc_train = pca_result.reduced_df
pc_holdout = pca_result.transform(holdout_full)

train_pca_df = pc_train.assign(**train_full[[LECol.TARGET]])
holdout_pca_df = pc_holdout.assign(**holdout_full[[LECol.TARGET]])
print(f"#PCs: {len(pc_train.columns.tolist())}")
```

```{python}
# | label: supervised-pca-initial
# | code-fold: true
rhs_pca = " + ".join(pc_train.columns.astype(str))
spca_diag = registry.fit(
    train_pca_df,
    name="m_spca",
    rhs=rhs_pca,
)

sig_pcs = [
    term
    for term, pval in spca_diag.model.pvalues.items()
    if term.startswith("supervised_PC") and float(pval) < 0.05
]
if not sig_pcs:
    sig_pcs = pc_train.columns.tolist()

rhs_pca_reduced = " + ".join(sig_pcs)
spca_reduced = registry.fit(
    train_pca_df,
    name="m_spca_reduced",
    rhs=rhs_pca_reduced,
    refit=True,
)
display(spca_reduced)
```
:::

The reduced supervised-PCA model explains most variance in life expectancy (R^2 ~ 0.97) with low in-sample error (RMSE ~ 1.36). All retained PCs are statistically significant at 5%, indicating that the supervised PCA basis concentrates signal across multiple orthogonal axes rather than a single dominant factor.

:::{.callout-note collapse="true"}

### Stepwise AIC model selection

```{python}
# | label: supervised-pca-stepwise
# | code-fold: false
# | tbl-cap: "Stepwise AIC regression (PCs only)."
stepwise_paths, stepwise_best_map, stepwise_compare = registry.run_selection_grid(
    train_pca_df,
    base_terms=[],
    candidates=pc_train.columns.astype(str).tolist(),
    criteria=["aic"],
    thresholds={"aic": 1.0},
    name_prefix="m_stepwise",
    refit=True,
)
spca_stepwise_name = stepwise_best_map.loc[0, "model"]
spca_stepwise = registry.get(spca_stepwise_name).diag
display(spca_stepwise)
```
:::

The stepwise AIC model is only marginally different from the reduced model (AIC ~ 645 vs 647), adding PC2 (borderline p ~ 0.052). This suggests the reduced model is already near the AIC optimum while keeping the specification simpler.

:::{.callout-note collapse="true"}
### Interaction-augmented fit
```{python}
# | label: supervised-pca-interactions
# | code-fold: true
# | tbl-cap: "Interaction-augmented regression (screened PC interactions)."
import statsmodels.formula.api as smf
from itertools import combinations

top_k = 6
tvals = spca_reduced.model.tvalues.drop("Intercept").abs().sort_values(ascending=False)
top_pcs = tvals.head(top_k).index.tolist()

base_rhs = rhs_pca_reduced
base_aic = spca_reduced.metrics.aic
screen_rows = []
for pc_a, pc_b in combinations(top_pcs, 2):
    term = f"{pc_a}:{pc_b}"
    rhs = f"{base_rhs} + {term}"
    model = smf.ols(f"{LECol.TARGET} ~ {rhs}", data=train_pca_df).fit()
    screen_rows.append(
        {
            "term": term,
            "pval": float(model.pvalues.get(term, np.nan)),
            "delta_aic": float(model.aic - base_aic),
        }
    )

screen_df = pd.DataFrame(screen_rows)
if screen_df.empty:
    interaction_candidates = []
else:
    screen_df = screen_df.sort_values("delta_aic")
    interaction_candidates = (
        screen_df.query("pval < 0.05 and delta_aic < 0").head(5)["term"].tolist()
    )
    if not interaction_candidates:
        interaction_candidates = screen_df.head(3)["term"].tolist()

interaction_paths, interaction_best_map, interaction_compare = (
    registry.run_selection_grid(
        train_pca_df,
        base_terms=sig_pcs,
        candidates=interaction_candidates,
        directions=["stepwise"],
        criteria=["aic"],
        thresholds={"aic": 1.0},
        name_prefix="m_inter",
        refit=True,
    )
)
spca_interactions_name = interaction_best_map.loc[0, "model"]
spca_interactions = registry.get(spca_interactions_name).diag
display(spca_interactions)
```
:::

Interaction-augmented stepwise fit (top PCs only) offers a targeted check for nonlinear combinations of disease-burden and development axes. Treat the result as exploratory and validate across years before drawing causal conclusions.

Cook's-distance trimming is treated as a sensitivity analysis rather than data cleaning; exclusions require substantive justification.

:::{.callout-note collapse="true"}
### Cook's distance trim (interaction model)
```{python}
# | label: supervised-pca-cooks
# | code-fold: true
cooks_thresh = 4 / len(train_pca_df)
spca_cooks_initial, spca_cooks = registry.fit(
    train_pca_df,
    name="m_inter_trim",
    rhs=registry.get(spca_interactions_name).rhs,
    refit=True,
    cook_distance_threshold=cooks_thresh,
)
registry.remove("m_inter_trim_initial")  # keep only final trimmed model in registry
removed = len(spca_cooks_initial.design_matrix) - len(spca_cooks.design_matrix)
print(f"Cook's threshold: {cooks_thresh:.4f} | removed: {removed}")
display(spca_cooks_initial)
```
:::

:::{.callout-note collapse="true"}
### Final model specification (trimmed)

```{python}
final_formula = (
    registry.get(spca_interactions_name)
    .rhs.replace(" + supervised_PC12", "")
    .replace(" + supervised_PC3:supervised_PC10", "")
)
cooks_thresh = 4 / len(train_pca_df)
spca_cooks_initial, spca_final = registry.fit(
    train_pca_df,
    name="m_final",
    rhs=final_formula,
    refit=True,
    cook_distance_threshold=cooks_thresh,
)
registry.remove("m_final_initial")  # keep only final trimmed model in registry
removed = len(spca_cooks_initial.design_matrix) - len(spca_final.design_matrix)
```

:::

This Cook's-distance trimming step (threshold 4/n, removing 4 points) is a sensitivity analysis rather than a data-cleaning rule. It can stabilize inference but should not be treated as a principled exclusion without substantive justification for the removed countries.

### Model comparison

```{python}
# | label: supervised-pca-compare
# | tbl-cap: "Model comparison across supervised PCA variants."
comp_tbl = registry.compare(sort_by="aic").drop(columns=["rhs"])[
    ["aic", "rmse", "r2", "adj_r2", "n_obs"]
]
comp_tbl
```

**Model evolution (concise):** full supervised-PCA (all PCs) -> reduced to significant PCs @tbl-supervised-pca-fit -> stepwise AIC among PCs -> add screened interactions -> Cook's-distance trim -> final model.

**AIC comparability:** interpret AIC only within the same sample size (trimmed vs untrimmed models are not directly comparable).

**Best-model criterion:** choose by mean cross-year RMSE/MAE for forecasting; use AIC for 2014 cross-sectional fit.

### Best model diagnostics

```{python}
# | label: supervised-pca-cooks-final
# | tbl-cap: "Final trimmed interaction model (best in-sample by AIC)."
best_model = registry.get(comp_tbl.index[0])
best_model_name = best_model.name
best_model = best_model.diag

display(best_model)
```

The AIC-ranked model is the best **in-sample** fit. Cross-year generalization favors the interaction model (mean RMSE 3.311; @tbl-supervised-pca-eval-all-models). Use AIC for 2014 cross-sectional fit and mean RMSE/MAE for forecasting across years.

```{python}
# | label: supervised-pca-diagnostics
# | fig-cap: "Supervised PCA regression diagnostics."
best_model.plot_residual_diags()
plt.show()
```

Diagnostics for the final model are largely favorable (see diagnostics plots below): JB p=0.560 and Shapiro p=0.774 do not reject normality; residual plots show only mild curvature. Heteroscedasticity is weak: BP p=0.048 is borderline while White p=0.529 passes, suggesting mild, form-dependent variance changes. Durbin-Watson ~ 1.99 indicates no autocorrelation. The condition number (~84) and max VIF (~1.7) are moderate, so multicollinearity is not a concern. For inference, non-robust SEs/CIs are slightly optimistic; prefer HC3/robust or bootstrap CIs. Prediction impacts are minor.

Because the model was selected via significance screening, stepwise AIC, interaction screening, and trimming, coefficient p-values/CIs are post-selection and should be interpreted as exploratory.

BP borderline failure with White passing implies mild heteroscedasticity: coefficients remain unbiased under exogeneity, but default SEs/CIs can be miscalibrated (often too narrow), and prediction intervals can be optimistic at the extremes.

## Holdout evaluation and bootstrap CIs

```{python}
# | label: supervised-pca-eval
# | tbl-cap: "Holdout-year performance (2011) for supervised PCA model."
eval_holdout = registry.evaluate_on(
    df=holdout_pca_df,
    name=best_model_name,
    label=f"year{HOLDOUT_YEAR}",
)
eval_holdout
```

Holdout performance (2011) is strong (RMSE 1.924, MAE 1.455, R^2 0.950; see table above). Bootstrap CIs quantify evaluation-sample uncertainty for a fixed model, not refit uncertainty.

```{python}
# | label: supervised-pca-eval-ci
# | tbl-cap: "Bootstrap CIs for holdout metrics (B=3000)."
eval_holdout.bootstrap_ci(n_bootstrap=N_BOOTSTAP, random_state=42).set_index("metric")
```

```{python}
# | label: supervised-pca-calibration
# | fig-cap: "Holdout calibration (2011) with bootstrap 95% CIs for binned means."
plt.figure(figsize=(7, 5))
eval_holdout.plot_calibration(bins=10, bootstrap=N_BOOTSTAP, random_state=42)
plt.show()
```

The calibration curve tracks the 45-degree line closely across most of the range, with slight over-prediction at the lowest life-expectancy levels (points below the diagonal) and good calibration in the mid-to-high range. This indicates that systematic bias is small and largely confined to the most disadvantaged countries.

```{python}
# | label: supervised-pca-eval-years
# | tbl-cap: "Supervised PCA evaluation across multiple years (same PCA mapping)."
# years_eval = [2010, 2011, 2012, 2013, 2014]
years_eval = range(2000, 2015)
rows = []
ci_rows = []
for year in years_eval:
    yr_ds = LifeExpectancyDataset.from_csv_updated(
        aggregate_by_country=year,
        resolve_nand_pred="carry_forward",
    )
    yr_full = yr_ds.tf_and_norm().drop(columns=[LECol.YEAR])
    if STATUS_DUMMY not in yr_full.columns and LECol.STATUS in yr_full.columns:
        yr_full = yr_full.assign(**{STATUS_DUMMY: yr_full[LECol.STATUS].astype(int)})
    yr_pca = pca_result.transform(yr_full)
    yr_df = yr_pca.assign(**yr_full[[LECol.TARGET]])
    metrics = registry.evaluate_on(df=yr_df, name=best_model_name, label=f"year{year}")
    rows.append(
        {
            "year": year,
            "rmse": metrics.rmse,
            "mae": metrics.mae,
            "r2": metrics.r2,
            "n_obs": metrics.n_obs,
        }
    )
    ci = metrics.bootstrap_ci(n_bootstrap=N_BOOTSTAP, random_state=42)
    ci_rows.append(ci.assign(year=year))

metrics_df = pd.DataFrame(rows).sort_values(by="year")
ci_df = pd.concat(ci_rows, ignore_index=True)

plot_df = ci_df.query("metric in ['mae', 'r2']").sort_values(["metric", "year"])
fig, axes = plt.subplots(1, 2, figsize=(11, 4), sharex=True)
for metric, ax in zip(["mae", "r2"], axes):
    sub = plot_df[plot_df["metric"] == metric]
    ax.plot(sub["year"], sub["estimate"], marker="o")
    ax.fill_between(sub["year"], sub["ci_low"], sub["ci_high"], alpha=0.2)
    ax.axvline(TRAIN_YEAR, color="grey", linestyle="--", linewidth=1, alpha=0.7)
    ax.set_title(f"{metric.upper()} with 95% CI")
    ax.set_xlabel("Year")
    ax.set_ylabel(metric.upper())
plt.tight_layout()
plt.show()

metrics_df

```

```{python}
# | label: supervised-pca-eval-all-models
# | fig-cap: "RMSE across years for supervised PCA variants."
all_years = (
    LifeExpectancyDataset.from_csv_updated(
        aggregate_by_country=False,
        resolve_nand_pred=False,
    )
    .df[LECol.YEAR]
    .dt.year.sort_values()
    .unique()
    .tolist()
)
years_eval_all = [year for year in all_years if year != TRAIN_YEAR]

model_labels = {
    "m_spca": "Full PCA",
    "m_spca_reduced": "Reduced PCA",
    spca_stepwise_name: "Stepwise AIC",
    spca_interactions_name: "Interactions",
    "m_inter_trim": "Cooks-trimmed",
    "m_final": "Final model",
}

rows = []
for year in years_eval_all:
    yr_ds = LifeExpectancyDataset.from_csv_updated(
        aggregate_by_country=year,
        resolve_nand_pred="carry_forward",
    )
    yr_full = yr_ds.tf_and_norm().drop(columns=[LECol.YEAR])
    if STATUS_DUMMY not in yr_full.columns and LECol.STATUS in yr_full.columns:
        yr_full = yr_full.assign(**{STATUS_DUMMY: yr_full[LECol.STATUS].astype(int)})
    yr_pca = pca_result.transform(yr_full)
    yr_df = yr_pca.assign(**yr_full[[LECol.TARGET]])
    for model in model_labels.keys():
        metrics = registry.evaluate_on(df=yr_df, name=model, label=f"year{year}")
        rows.append(
            {
                "year": year,
                "model": model_labels.get(model, model),
                "rmse": metrics.rmse,
                "mae": metrics.mae,
                "r2": metrics.r2,
                "n_obs": metrics.n_obs,
            }
        )

rmse_by_year = pd.DataFrame(rows)
plt.figure(figsize=(9, 5))
sns.lineplot(data=rmse_by_year, x="year", y="rmse", hue="model", marker="o")
plt.axvline(TRAIN_YEAR, color="grey", linestyle="--", linewidth=1, alpha=0.7)
plt.ylabel("RMSE")
plt.yscale("log")
plt.xlabel("Year")
plt.legend(title="Model")
plt.tight_layout()
plt.show()
```


```{python}
# | label: tbl-supervised-pca-eval-all-models
# | tbl-cap: "Supervised PCA model evaluation across years accumulated over years."

rmse_by_year.groupby("model")[["rmse", "mae", "r2"]].mean().sort_values("rmse")
```

## Conclusions on model choice

Two candidates emerge: the **Interactions model** (untrimmed) and the **Final model** (trimmed + simplified). The Final model is best **in-sample for 2014** ($R^2=0.981$, RMSE=1.066, AIC=533.2) and has the **most stable diagnostics** (JB p=0.560, Shapiro p=0.774, White p=0.529; BP borderline p=0.048; low VIF~1.7; max Cook's=0.056). Its drawback is **poorer temporal generalization**: mean RMSE across years is 3.403, the worst among candidates. The Interactions model generalizes **best across years** (mean RMSE=3.311; mean MAE=2.791; mean $R^2=0.865$), with the Final model worse by +0.092 RMSE (+2.79%) and +0.003 MAE (+0.10%). It also shows **less stable inference** (BP p=0.036; max Cook's=0.417; 12 points exceed 4/n), indicating higher influence sensitivity and more heteroscedasticity.

**Conclusion:** If the goal is **explain 2014 cross-sectional structure with stable diagnostics**, report the **Final model** (with a robust-SE/boot CI caveat). If the goal is **prediction across years**, select the **Interactions model** as the best generalizer despite less stable inference.


## Interpretation of principal components in the final SPCA model

```{python}
# | label: supervised-pca-loadings
# | fig-cap: "Loadings heatmap for retained supervised PCs."
# | code-fold: true
group_res = pca_result.group_results[0]
kept_pcs_prefixed = sig_pcs if sig_pcs else group_res.pc_scores.columns.tolist()
kept_pc_names = [
    pc.split(f"{group_res.group.name}_", 1)[1] for pc in kept_pcs_prefixed
]

plot_loadings_heatmap(
    group_res,
    pc_subset=kept_pc_names,
    top_n_features=15,
    figsize=(14, 8),
).show()
```



The final supervised PCA (SPCA) regression model (`m_final`) uses the additive PCs
PC1, PC3, PC4, PC5, PC6, PC7, PC8, PC9, PC10, PC11, PC13, PC14, and PC17,
as well as two interaction terms: PC3:PC4 and PC10:PC4.
All PCs are computed from standardized predictors after outcome-based screening, so each PC represents
an **orthogonal axis** in the screened feature space.

### How PCs are interpreted (rule)

A PC is not a “variable”; it is a weighted contrast of original predictors. Interpretation therefore proceeds in two steps:

1. **Loadings → meaning:** identify the dominant positive and negative loadings (top $|loading|$) from the loadings heatmap.
   These define what a “high” vs “low” score on a PC means in terms of original variables.
2. **Coefficient sign → association:** given the regression coefficient $\beta_k$, increasing the PC score by one unit changes the prediction by $\beta_k$ years.
   Hence, the side of the PC axis that increases life expectancy is determined by the joint sign structure of loadings and $\beta_k$.

Because PC signs are arbitrary, we do **not** interpret “positive PC values” per se; we interpret the **direction in original-variable space**
that corresponds to higher predicted life expectancy.

### Which PCs matter most (relative importance)

The t-statistics indicate that the model is dominated by a small number of axes.
PC1 is by far the strongest contributor (|t| $\approx 78.5$), followed by PC3, PC4, PC10, PC11, PC14, PC5, and PC8 (all |t| $\gtrsim 8$).
The remaining retained PCs contribute smaller but still statistically detectable adjustments in orthogonal directions.

### Interpretation of the dominant PCs

**PC1 — “Mortality burden vs socio-economic development” (dominant gradient).**
Loadings indicate that PC1 increases with mortality indicators (under-five deaths, infant deaths, adult mortality)
and decreases with development proxies (schooling, GDP). The coefficient is negative ($\beta_{PC1} < 0$),
so moving toward the mortality-burden side of this axis is associated with lower life expectancy.
Equivalently, conditional on the other PCs, countries characterized by higher schooling/GDP and lower mortality burden
lie on the life-expectancy–increasing direction of PC1.

**PC3 — “HIV/AIDS and residual mortality burden beyond the main gradient.”**
PC3 is dominated by HIV/AIDS and mortality-related loadings (as seen in the loadings heatmap).
Its coefficient is negative ($\beta_{PC3} < 0$), implying that, *after controlling for the broad PC1 gradient*,
the HIV/AIDS–related burden captured by PC3 remains strongly associated with reduced life expectancy.
PC3 also enters an interaction with PC4 (see below), indicating that the strength of this association depends on the level of PC4.

```{python}
# | label: supervised-pca-biplot-pc1-pc3
# | fig-cap: "Biplot for the first two retained supervised PCs."
# | code-fold: true
pc_axes = kept_pc_names[:2] if len(kept_pc_names) >= 2 else []
if len(pc_axes) == 2:
    biplot_fig = plot_biplot_plotly(
        group_res,
        dims=2,
        pc_axes=pc_axes,
        top_features=12,
        color=train_full.loc[group_res.pc_scores.index, LECol.TARGET],
    )
    biplot_fig.show()
```

**PC4 — “Health spending / nutrition / development-composition contrast” (moderator axis).**
PC4 combines expenditure and anthropometric structure (e.g., BMI and expenditure-related variables) against development-status/expenditure composition
in a way that is orthogonal to PC1 and PC3. Its main-effect coefficient is negative ($\beta_{PC4} < 0$),
so higher scores along the PC4 direction are associated with lower life expectancy *at the reference levels of other PCs*.
Crucially, PC4 also appears in both retained interaction terms, so it acts as an effect modifier:
PC4 changes how strongly PC3 and PC10 relate to life expectancy.

**PC10 — “Development–disease contrast distinct from PC1” (interaction with PC4).**
PC10 forms another high-impact axis (large |t|), separating a residual development/disease pattern not already captured by PC1.
Its main coefficient is negative ($\beta_{PC10} < 0$), but the interpretation must be tied to its loadings:
the direction associated with higher life expectancy is the side of the PC10 axis whose movement yields an increase in $\hat{y}$
given $\beta_{PC10}$ (i.e., the side opposite the sign of $\beta_{PC10}$).
PC10 additionally interacts with PC4, so the association of PC10 with life expectancy changes depending on PC4.

```{python}
# | label: supervised-pca-biplot-pc4-pc10
# | fig-cap: "Biplot for the supervised PCs 4 and 10."
# | code-fold: true
pc_axes = ["PC4", "PC10"]
if len(pc_axes) == 2:
    biplot_fig = plot_biplot_plotly(
        group_res,
        dims=2,
        pc_axes=pc_axes,
        top_features=12,
        color=train_full.loc[group_res.pc_scores.index, LECol.TARGET],
    )
    biplot_fig.show()
```

**PC5 and PC8 — “Secondary development/health structure axes.”**
PC5 and PC8 have positive coefficients in the final model, indicating that moving along their life-expectancy–increasing directions
is associated with higher life expectancy, conditional on the dominant mortality/development gradients.
Based on the loadings heatmap, these PCs correspond to contrasts mixing development proxies, expenditure structure, nutrition, and infectious disease burden.
In practice, these axes capture predictable second-order structure: once the main mortality–development gradient (PC1) is accounted for,
there remains residual variation that separates countries by spending composition, nutrition, and disease patterns.

```{python}
# | label: supervised-pca-biplot-pc5-pc8
# | fig-cap: "Biplot for the supervised PCs 5 and 8."
# | code-fold: true
pc_axes = ["PC5", "PC8"]
if len(pc_axes) == 2:
    biplot_fig = plot_biplot_plotly(
        group_res,
        dims=2,
        pc_axes=pc_axes,
        top_features=12,
        color=train_full.loc[group_res.pc_scores.index, LECol.TARGET],
    )
    biplot_fig.show()
```


### Interactions in the final model (PC3:PC4 and PC10:PC4)

The final model includes two interactions:

- PC3:PC4 with a **positive** coefficient ($\beta_{34} > 0$)
- PC10:PC4 with a **positive** coefficient ($\beta_{10,4} > 0$)

This implies effect modification. The marginal effects are:

$$
\frac{\partial \hat{y}}{\partial PC3} = \beta_{PC3} + \beta_{34}\,PC4,\qquad
\frac{\partial \hat{y}}{\partial PC10} = \beta_{PC10} + \beta_{10,4}\,PC4.
$$

Since $\beta_{PC3} < 0$ and $\beta_{34} > 0$, the negative association of PC3 with life expectancy is **attenuated** at higher PC4 values
and **stronger (more negative)** at lower PC4 values. Numerically, a one-unit increase in PC4 changes the marginal effect of PC3 by $\beta_{34}$,
so PC4 acts as a buffer/moderator of the PC3-related burden axis.

Similarly, since $\beta_{PC10} < 0$ and $\beta_{10,4} > 0$, the negative association of PC10 is also **attenuated** at higher PC4 values.
Substantively, these interactions mean: the life-expectancy penalty associated with the PC3/PC10 “burden/development contrasts” depends on the structural axis represented by PC4.
The concrete interpretation should be stated using the top loadings of PC3, PC4, and PC10 (e.g., “HIV/mortality burden is less harmful at higher values of the spending/nutrition axis”, if that is what PC4 encodes).

### Remaining retained PCs (PC6, PC7, PC9, PC11, PC13, PC14, PC17)

The remaining retained PCs represent additional orthogonal directions that are statistically detectable but smaller than the dominant axes above.
To interpret them without overclaiming, we recommend documenting, for each PCk:

- the **top positive and negative loadings** (top 5–10 by $|loading|$),
- a short axis label (“immunization coverage contrast”, “nutrition deficiency contrast”, “spending composition contrast”, etc.),
- and a one-sentence association statement consistent with the coefficient sign:
  “moving toward the [loading-defined] side of PCk is associated with higher/lower life expectancy (conditional on the other PCs).”

Because these PCs are orthogonal and were selected after screening/selection, they should be treated as refining axes
rather than standalone causal drivers. In the main text, it is usually sufficient to interpret 3–5 of the most influential PCs
(PC1, PC3, PC4, PC10, plus one of PC5/PC8), and place the full loading summaries for all PCs into an appendix/callout.

```{python}
# | label: supervised-pca-biplot-3d
# | fig-cap: "3D biplot for supervised PCs 6, 7, 9 (if available)."
# | code-fold: true
pc_axes_3d = [pc for pc in ["PC6", "PC7", "PC9"] if pc in kept_pc_names]
if len(pc_axes_3d) == 3:
    biplot_fig_3d = plot_biplot_plotly(
        group_res,
        dims=3,
        pc_axes=pc_axes_3d,
        top_features=10,
        color=train_full.loc[group_res.pc_scores.index, LECol.TARGET],
    )
    biplot_fig_3d.show()
```

