---
title: "Regression: Supervised PCA"
format:
  html:
    toc: true
    toc-depth: 3
execute:
  warning: false
  message: false
  cache: false
---

# Regression: Supervised PCA

This chapter isolates the supervised PCA regression workflow and its interpretation.

We follow the supervised principal components (SPCA) idea: screen predictors by their association with the outcome, run PCA on the screened subset, and regress on the resulting PCs. This aligns with the canonical SPCA framework of Bair et al. @Bair01032006.

<!-- Motivation: even when n > p (as here), unsupervised PCA can emphasize variance unrelated to the outcome, which can weaken predictive focus and interpretability. SPCA filters predictors by outcome association before PCA, so the resulting components remain low-dimensional but outcome-relevant. The approach highlights important variables and keeps the regression model interpretable while allowing covariates and interaction terms. @Bair01032006 -->

```{python}
# | label: setup
# | include: false
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import display

from ama_tlbx.analysis import FeatureGroup
from ama_tlbx.analysis.model_registry import ModelRegistry
from ama_tlbx.data import LECol, LifeExpectancyDataset
from ama_tlbx.plotting import plot_biplot_plotly, plot_loadings_heatmap
from ama_tlbx.utils.plotting_config import DEFAULT_PLOT_CFG

DEFAULT_PLOT_CFG.apply_global()
np.random.seed(42)

TRAIN_YEAR = 2014
HOLDOUT_YEAR = 2011
STATUS_DUMMY = "status_developed"
N_BOOTSTAP = 3000
```

## Data preparation

```{python}
# | label: data-prep
# | code-fold: true
train_ds = LifeExpectancyDataset.from_csv_updated(
    aggregate_by_country=TRAIN_YEAR,
    resolve_nand_pred="carry_forward",
)
holdout_ds = LifeExpectancyDataset.from_csv_updated(
    aggregate_by_country=HOLDOUT_YEAR,
    resolve_nand_pred="carry_forward",
)

train_full = train_ds.tf_and_norm().drop(columns=[LECol.YEAR])
holdout_full = holdout_ds.tf_and_norm().drop(columns=[LECol.YEAR])

if STATUS_DUMMY not in train_full.columns and LECol.STATUS in train_full.columns:
    train_full = train_full.assign(
        **{STATUS_DUMMY: train_full[LECol.STATUS].astype(int)}
    )
    holdout_full = holdout_full.assign(
        **{STATUS_DUMMY: holdout_full[LECol.STATUS].astype(int)}
    )

registry = ModelRegistry(eval_year=HOLDOUT_YEAR)
```

## Supervised PCA regression



```{python}
# | label: supervised-pca
# | code-fold: false
corr_result = (
    LifeExpectancyDataset(df=train_full)
    .make_correlation_analyzer(standardized=False, include_target=True)
    .fit()
    .result()
)

strong_features = (
    corr_result.target_correlations.assign(abs_cor=lambda d: d.correlation.abs())
    .query("abs_cor > 0.25")
    .feature.tolist()
)
strong_features = [
    f
    for f in strong_features
    if f
    not in [
        LECol.HDI,
    ]
]
print(strong_features)
```

:::{.callout-note collapse="true"}
### Supervised PCA fit
```{python}
# | label: tbl-supervised-pca-fit
# | code-fold: true
# | tbl-cap: "Reduced supervised-PCA regression (significant PCs)."
feature_groups = [FeatureGroup(name="supervised", features=strong_features)]

pca_result = (
    LifeExpectancyDataset(df=train_full)
    .make_pca_dim_reduction_analyzer(
        feature_groups=feature_groups,
        standardized=True,
        min_var_explained=1.0,
    )
    .fit()
    .result()
)

pc_train = pca_result.reduced_df
pc_holdout = pca_result.transform(holdout_full)

train_pca_df = pc_train.assign(**train_full[[LECol.TARGET]])
holdout_pca_df = pc_holdout.assign(**holdout_full[[LECol.TARGET]])

rhs_pca = " + ".join(pc_train.columns.astype(str))
spca_diag = registry.fit(
    train_pca_df,
    name="m_spca",
    rhs=rhs_pca,
)

sig_pcs = [
    term
    for term, pval in spca_diag.model.pvalues.items()
    if term.startswith("supervised_PC") and float(pval) < 0.05
]
if not sig_pcs:
    sig_pcs = pc_train.columns.tolist()

rhs_pca_reduced = " + ".join(sig_pcs)
spca_reduced = registry.fit(
    train_pca_df,
    name="m_spca_reduced",
    rhs=rhs_pca_reduced,
    refit=True,
)
display(spca_reduced)
```
:::

The reduced supervised-PCA model explains most variance in life expectancy (R^2 ~ 0.97) with low in-sample error (RMSE ~ 1.36). All retained PCs are statistically significant at 5%, indicating that the supervised PCA basis concentrates signal across multiple orthogonal axes rather than a single dominant factor.

:::{.callout-note collapse="true"}

### Stepwise AIC model selection

```{python}
# | label: supervised-pca-stepwise
# | code-fold: false
# | tbl-cap: "Stepwise AIC regression (PCs only)."
stepwise_paths, stepwise_best_map, stepwise_compare = registry.run_selection_grid(
    train_pca_df,
    base_terms=[],
    candidates=pc_train.columns.astype(str).tolist(),
    criteria=["aic"],
    thresholds={"aic": 1.0},
    name_prefix="m_stepwise",
    refit=True,
)
spca_stepwise_name = stepwise_best_map.loc[0, "model"]
spca_stepwise = registry.get(spca_stepwise_name).diag
display(spca_stepwise)
```
:::

The stepwise AIC model is only marginally different from the reduced model (AIC ~ 645 vs 647), adding PC2 (borderline p ~ 0.052). This suggests the reduced model is already near the AIC optimum while keeping the specification simpler.

:::{.callout-note collapse="true"}
### Interaction-augmented fit
```{python}
# | label: supervised-pca-interactions
# | code-fold: true
# | tbl-cap: "Interaction-augmented regression (screened PC interactions)."
import statsmodels.formula.api as smf
from itertools import combinations

top_k = 6
tvals = spca_reduced.model.tvalues.drop("Intercept").abs().sort_values(ascending=False)
top_pcs = tvals.head(top_k).index.tolist()

base_rhs = rhs_pca_reduced
base_aic = spca_reduced.metrics.aic
screen_rows = []
for pc_a, pc_b in combinations(top_pcs, 2):
    term = f"{pc_a}:{pc_b}"
    rhs = f"{base_rhs} + {term}"
    model = smf.ols(f"{LECol.TARGET} ~ {rhs}", data=train_pca_df).fit()
    screen_rows.append(
        {
            "term": term,
            "pval": float(model.pvalues.get(term, np.nan)),
            "delta_aic": float(model.aic - base_aic),
        }
    )

screen_df = pd.DataFrame(screen_rows).sort_values("delta_aic")
interaction_candidates = (
    screen_df.query("pval < 0.05 and delta_aic < 0").head(5)["term"].tolist()
)
if not interaction_candidates:
    interaction_candidates = screen_df.head(3)["term"].tolist()

interaction_paths, interaction_best_map, interaction_compare = (
    registry.run_selection_grid(
        train_pca_df,
        base_terms=sig_pcs,
        candidates=interaction_candidates,
        directions=["stepwise"],
        criteria=["aic"],
        thresholds={"aic": 1.0},
        name_prefix="m_inter",
        refit=True,
    )
)
spca_interactions_name = interaction_best_map.loc[0, "model"]
spca_interactions = registry.get(spca_interactions_name).diag
display(spca_interactions)
```
:::

Interaction-augmented stepwise fit (top PCs only) offers a targeted check for nonlinear combinations of disease-burden and development axes. Treat the result as exploratory and validate across years before drawing causal conclusions.

Cook's-distance trimming is treated as a sensitivity analysis rather than data cleaning; exclusions require substantive justification.

:::{.callout-note collapse="true"}
### Cook's distance trim (interaction model)
```{python}
# | label: supervised-pca-cooks
# | code-fold: true
cooks_thresh = 4 / len(train_pca_df)
spca_cooks_initial, spca_cooks = registry.fit(
    train_pca_df,
    name="m_inter_trim",
    rhs=registry.get(spca_interactions_name).rhs,
    refit=True,
    cook_distance_threshold=cooks_thresh,
)
registry.remove("m_inter_trim_initial")  # keep only final trimmed model in registry
removed = len(spca_cooks_initial.design_matrix) - len(spca_cooks.design_matrix)
print(f"Cook's threshold: {cooks_thresh:.4f} | removed: {removed}")
display(spca_cooks_initial)
```
:::

:::{.callout-note collapse="true"}
### Final model specification (trimmed)

```{python}
final_formula = (
    registry.get(spca_interactions_name)
    .rhs.replace(" + supervised_PC12", "")
    .replace(" + supervised_PC3:supervised_PC10", "")
)
cooks_thresh = 4 / len(train_pca_df)
spca_cooks_initial, spca_final = registry.fit(
    train_pca_df,
    name="m_final",
    rhs=final_formula,
    refit=True,
    cook_distance_threshold=cooks_thresh,
)
registry.remove("m_final_initial")  # keep only final trimmed model in registry
removed = len(spca_cooks_initial.design_matrix) - len(spca_final.design_matrix)
```

:::

This Cook's-distance trimming step (threshold 4/n, removing 4 points) is a sensitivity analysis rather than a data-cleaning rule. It can stabilize inference but should not be treated as a principled exclusion without substantive justification for the removed countries.

### Model comparison

```{python}
# | label: supervised-pca-compare
# | tbl-cap: "Model comparison across supervised PCA variants."
comp_tbl = registry.compare(sort_by="aic").drop(columns=["rhs"])[
    ["aic", "rmse", "r2", "adj_r2", "n_obs"]
]
comp_tbl
```

**Model evolution (concise):** full supervised-PCA (all PCs) -> reduced to significant PCs @tbl-supervised-pca-fit -> stepwise AIC among PCs -> add screened interactions -> Cook's-distance trim -> final model.

**AIC comparability:** interpret AIC only within the same sample size (trimmed vs untrimmed models are not directly comparable).

**Best-model criterion:** choose by mean cross-year RMSE/MAE for forecasting; use AIC for 2014 cross-sectional fit.

### Best model diagnostics

```{python}
# | label: supervised-pca-cooks-final
# | tbl-cap: "Final trimmed interaction model (best in-sample by AIC)."
best_model = registry.get(comp_tbl.index[0])
best_model_name = best_model.name
best_model = best_model.diag

display(best_model)
```

The AIC-ranked model is the best **in-sample** fit. Cross-year generalization favors the interaction model (mean RMSE 3.311; @tbl-supervised-pca-eval-all-models). Use AIC for 2014 cross-sectional fit and mean RMSE/MAE for forecasting across years.

```{python}
# | label: supervised-pca-diagnostics
# | fig-cap: "Supervised PCA regression diagnostics."
best_model.plot_residual_diags()
plt.show()
```

Diagnostics for the final model are largely favorable (see diagnostics plots below): JB p=0.560 and Shapiro p=0.774 do not reject normality; residual plots show only mild curvature. Heteroscedasticity is weak: BP p=0.048 is borderline while White p=0.529 passes, suggesting mild, form-dependent variance changes. Durbin-Watson ~ 1.99 indicates no autocorrelation. The condition number (~84) and max VIF (~1.7) are moderate, so multicollinearity is not a concern. For inference, non-robust SEs/CIs are slightly optimistic; prefer HC3/robust or bootstrap CIs. Prediction impacts are minor.

Because the model was selected via significance screening, stepwise AIC, interaction screening, and trimming, coefficient p-values/CIs are post-selection and should be interpreted as exploratory.

BP borderline failure with White passing implies mild heteroscedasticity: coefficients remain unbiased under exogeneity, but default SEs/CIs can be miscalibrated (often too narrow), and prediction intervals can be optimistic at the extremes.

## Holdout evaluation and bootstrap CIs

```{python}
# | label: supervised-pca-eval
# | tbl-cap: "Holdout-year performance (2011) for supervised PCA model."
eval_holdout = registry.evaluate_on(
    df=holdout_pca_df,
    name=best_model_name,
    label=f"year{HOLDOUT_YEAR}",
)
eval_holdout
```

Holdout performance (2011) is strong (RMSE 1.924, MAE 1.455, R^2 0.950; see table above). Bootstrap CIs quantify evaluation-sample uncertainty for a fixed model, not refit uncertainty.

```{python}
# | label: supervised-pca-eval-ci
# | tbl-cap: "Bootstrap CIs for holdout metrics (B=3000)."
eval_holdout.bootstrap_ci(n_bootstrap=N_BOOTSTAP, random_state=42).set_index("metric")
```

```{python}
# | label: supervised-pca-calibration
# | fig-cap: "Holdout calibration (2011) with bootstrap 95% CIs for binned means."
plt.figure(figsize=(7, 5))
eval_holdout.plot_calibration(bins=10, bootstrap=N_BOOTSTAP, random_state=42)
plt.show()
```

The calibration curve tracks the 45-degree line closely across most of the range, with slight over-prediction at the lowest life-expectancy levels (points below the diagonal) and good calibration in the mid-to-high range. This indicates that systematic bias is small and largely confined to the most disadvantaged countries.

```{python}
# | label: supervised-pca-eval-years
# | tbl-cap: "Supervised PCA evaluation across multiple years (same PCA mapping)."
# years_eval = [2010, 2011, 2012, 2013, 2014]
years_eval = range(2000, 2015)
rows = []
ci_rows = []
for year in years_eval:
    yr_ds = LifeExpectancyDataset.from_csv_updated(
        aggregate_by_country=year,
        resolve_nand_pred="carry_forward",
    )
    yr_full = yr_ds.tf_and_norm().drop(columns=[LECol.YEAR])
    if STATUS_DUMMY not in yr_full.columns and LECol.STATUS in yr_full.columns:
        yr_full = yr_full.assign(**{STATUS_DUMMY: yr_full[LECol.STATUS].astype(int)})
    yr_pca = pca_result.transform(yr_full)
    yr_df = yr_pca.assign(**yr_full[[LECol.TARGET]])
    metrics = registry.evaluate_on(df=yr_df, name=best_model_name, label=f"year{year}")
    rows.append(
        {
            "year": year,
            "rmse": metrics.rmse,
            "mae": metrics.mae,
            "r2": metrics.r2,
            "n_obs": metrics.n_obs,
        }
    )
    ci = metrics.bootstrap_ci(n_bootstrap=N_BOOTSTAP, random_state=42)
    ci_rows.append(ci.assign(year=year))

metrics_df = pd.DataFrame(rows).sort_values(by="year")
ci_df = pd.concat(ci_rows, ignore_index=True)

plot_df = ci_df.query("metric in ['mae', 'r2']").sort_values(["metric", "year"])
fig, axes = plt.subplots(1, 2, figsize=(11, 4), sharex=True)
for metric, ax in zip(["mae", "r2"], axes):
    sub = plot_df[plot_df["metric"] == metric]
    ax.plot(sub["year"], sub["estimate"], marker="o")
    ax.fill_between(sub["year"], sub["ci_low"], sub["ci_high"], alpha=0.2)
    ax.axvline(TRAIN_YEAR, color="grey", linestyle="--", linewidth=1, alpha=0.7)
    ax.set_title(f"{metric.upper()} with 95% CI")
    ax.set_xlabel("Year")
    ax.set_ylabel(metric.upper())
plt.tight_layout()
plt.show()

metrics_df

```

```{python}
# | label: supervised-pca-eval-all-models
# | fig-cap: "RMSE across years for supervised PCA variants."
all_years = (
    LifeExpectancyDataset.from_csv_updated(
        aggregate_by_country=False,
        resolve_nand_pred=False,
    )
    .df[LECol.YEAR]
    .dt.year.sort_values()
    .unique()
    .tolist()
)
years_eval_all = [year for year in all_years if year != TRAIN_YEAR]

model_labels = {
    "m_spca": "Full PCA",
    "m_spca_reduced": "Reduced PCA",
    spca_stepwise_name: "Stepwise AIC",
    spca_interactions_name: "Interactions",
    "m_inter_trim": "Cooks-trimmed",
    "m_final": "Final model",
}

rows = []
for year in years_eval_all:
    yr_ds = LifeExpectancyDataset.from_csv_updated(
        aggregate_by_country=year,
        resolve_nand_pred="carry_forward",
    )
    yr_full = yr_ds.tf_and_norm().drop(columns=[LECol.YEAR])
    if STATUS_DUMMY not in yr_full.columns and LECol.STATUS in yr_full.columns:
        yr_full = yr_full.assign(**{STATUS_DUMMY: yr_full[LECol.STATUS].astype(int)})
    yr_pca = pca_result.transform(yr_full)
    yr_df = yr_pca.assign(**yr_full[[LECol.TARGET]])
    for model in model_labels.keys():
        metrics = registry.evaluate_on(df=yr_df, name=model, label=f"year{year}")
        rows.append(
            {
                "year": year,
                "model": model_labels.get(model, model),
                "rmse": metrics.rmse,
                "mae": metrics.mae,
                "r2": metrics.r2,
                "n_obs": metrics.n_obs,
            }
        )

rmse_by_year = pd.DataFrame(rows)
plt.figure(figsize=(9, 5))
sns.lineplot(data=rmse_by_year, x="year", y="rmse", hue="model", marker="o")
plt.axvline(TRAIN_YEAR, color="grey", linestyle="--", linewidth=1, alpha=0.7)
plt.ylabel("RMSE")
plt.yscale("log")
plt.xlabel("Year")
plt.legend(title="Model")
plt.tight_layout()
plt.show()
```


```{python}
# | label: tbl-supervised-pca-eval-all-models
# | tbl-cap: "Supervised PCA model evaluation across years accumulated over years."

rmse_by_year.groupby("model")[["rmse", "mae", "r2"]].mean().sort_values("rmse")
```

## Conclusions on model choice

**Model selection conclusion (concise).**
Two candidates emerge: the **Interactions model** (untrimmed) and the **Final model** (trimmed + simplified). The Final model is best **in-sample for 2014** ($R^2=0.981$, RMSE=1.066, AIC=533.2) and has the **most stable diagnostics** (JB p=0.560, Shapiro p=0.774, White p=0.529; BP borderline p=0.048; low VIF~1.7; max Cook's=0.056). Its drawback is **poorer temporal generalization**: mean RMSE across years is 3.403, the worst among candidates. The Interactions model generalizes **best across years** (mean RMSE=3.311; mean MAE=2.791; mean R^2=0.865), with the Final model worse by +0.092 RMSE (+2.79%) and +0.003 MAE (+0.10%). It also shows **less stable inference** (BP p=0.036; max Cook's=0.417; 12 points exceed 4/n), indicating higher influence sensitivity and more heteroscedasticity.

**Conclusion:** If the goal is **explain 2014 cross-sectional structure with stable diagnostics**, report the **Final model** (with a robust-SE/boot CI caveat). If the goal is **prediction across years**, select the **Interactions model** as the best generalizer despite less stable inference.


## Loadings and biplots (retained PCs)

```{python}
# | label: supervised-pca-loadings
# | fig-cap: "Loadings heatmap for retained supervised PCs."
group_res = pca_result.group_results[0]
kept_pcs_prefixed = sig_pcs if sig_pcs else group_res.pc_scores.columns.tolist()
kept_pc_names = [
    pc.split(f"{group_res.group.name}_", 1)[1] for pc in kept_pcs_prefixed
]

plot_loadings_heatmap(
    group_res,
    pc_subset=kept_pc_names,
    top_n_features=15,
    figsize=(14, 8),
).show()
```

Interpretation of the most impactful PCs (largest |t| values) in terms of original features:

- **PC1 (t $\approx$ -69)**: strong positive loadings for under-five/infant mortality and adult mortality, with negative loadings for schooling and GDP. The negative coefficient implies that higher mortality burden reduces life expectancy, while higher education/wealth increases it.
- **PC3 (t $\approx$ -20)**: dominated by HIV/AIDS, alcohol, and adult mortality (positive loadings). The negative coefficient indicates this disease-burden axis is strongly associated with lower life expectancy.
- **PC10 (t $\approx$ -9.7)**: contrasts developed status and BMI (positive) against HIV/AIDS (negative). The negative coefficient reflects the arbitrary sign of PCs; interpreted substantively, this PC separates development/nutrition from HIV burden and contributes meaningfully to life expectancy differences.
- **PC4 (t $\approx$ -9.0)**: combines BMI and percentage health expenditure (positive) with negative loadings on development status and total expenditure. This axis likely reflects compositional shifts in spending and nutrition; its negative sign suggests higher values on this contrast are associated with lower life expectancy after accounting for other PCs.
- **PC8 (t $\approx$ +7.7)**: loads on BMI, thinness, and schooling (positive) versus measles and percentage expenditure (negative), capturing an anthropometric/education contrast against infectious disease proxies; its positive coefficient indicates higher scores align with higher life expectancy.
- **PC5 (t $\approx$ +7.3)**: high on percentage expenditure and GDP but negative on total expenditure and BMI, suggesting a relative spending intensity vs. absolute resource/anthropometry contrast; the positive coefficient indicates higher scores correspond to higher life expectancy in this reduced-PC basis.

```{python}
# | label: supervised-pca-biplot
# | fig-cap: "Biplot for the first two retained supervised PCs"
pc_axes = kept_pc_names[:2] if len(kept_pc_names) >= 2 else []
if len(pc_axes) == 2:
    biplot_fig = plot_biplot_plotly(
        group_res,
        dims=2,
        pc_axes=pc_axes,
        top_features=12,
        color=train_full.loc[group_res.pc_scores.index, LECol.TARGET],
    )
    biplot_fig.show()
```

```{python}
# | label: supervised-pca-biplot-2d
# | fig-cap: "2D biplot for supervised PCs 4, 5."
pc_axes_3d = [pc for pc in ["PC4", "PC5"] if pc in kept_pc_names]
if len(pc_axes_3d) == 2:
    biplot_fig_3d = plot_biplot_plotly(
        group_res,
        dims=2,
        pc_axes=pc_axes_3d,
        top_features=10,
        color=train_full.loc[group_res.pc_scores.index, LECol.TARGET],
    )
    biplot_fig_3d.show()
```

```{python}
# | label: supervised-pca-biplot-3d
# | fig-cap: "3D biplot for supervised PCs 5, 7, 8 (if available)."
pc_axes_3d = [pc for pc in ["PC5", "PC7", "PC8"] if pc in kept_pc_names]
if len(pc_axes_3d) == 3:
    biplot_fig_3d = plot_biplot_plotly(
        group_res,
        dims=3,
        pc_axes=pc_axes_3d,
        top_features=10,
        color=train_full.loc[group_res.pc_scores.index, LECol.TARGET],
    )
    biplot_fig_3d.show()
```

**Interpretation guidance:** Use the PCA loadings to interpret each significant PC. Do not interpret PC signs without loadings. Include a short paragraph linking top loadings to life expectancy dynamics.
