---
title: "Data Cleaning & Preprocessing"
format:
  html:
    toc: true
---

# Overview

This chapter documents the preprocessing workflow implemented in `ama-tlbx` and clarifies which dataset views are used in downstream chapters. The raw data are a country-year panel (2000–2015); to reduce within-country temporal dependence, most multivariate analyses in this report use the 2014 cross-section (see [Year Selection](2_preproc_year_selection.qmd)). In addition to describing the pipeline (type conversion $\rightarrow$ missingness handling $\rightarrow$ transforms $\rightarrow$ standardization), we perform a **data-integrity audit** of the ["original Kaggle dataset"](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who) and show why we prefer a [cleaned, consistency-checked dataset](https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated/data) for modeling (`LifeExpectancyDataset.from_csv_updated()`), rather than “cleaning harder” within the same file.

# Preprocessing workflow (ama-tlbx)

::: {#fig-preproc-flow}
![](../figures/ama_preprocessing_flow.svg){width=100%}

Preprocessing flow in `ama-tlbx`.
:::

Figure @fig-preproc-flow summarizes the pipeline executed by `LifeExpectancyDataset.from_csv()`. After normalizing headers to stable snake_case names and converting types (`year` parsed; development status recoded), missingness in the outcome can be handled separately from missingness in predictors. The next decision is whether to aggregate the panel to one record per country (e.g., select 2014) or keep the full longitudinal data. Finally, predictors can either be left on their original scale, standardized, or transformed-and-standardized (`tf_only()` / `tf_and_norm()`), where the per-variable transformation rules are taken from the column metadata.

The workflow is driven by a schema-as-code layer: all variables are accessed via the `LECol` enum (with `ColumnMetadata`), which defines original names, dtypes, display labels, and default transformations. This makes preprocessing choices explicit and reproducible across chapters.

<!-- ## Known issues in the raw Kaggle dataset
Claims in Kaggle discussions:

- Population of some countries like Israel, Srilanka are in thousands, while other in millions.
- if you look for the first country about the infant death (Number of Infant Deaths per 1000 population)
https://data.who.int/indicators/i/E3CAF2B/2322814?m49=004?m49=004?m49=004
You notice that amount in the data 2000-2015 and the link provided above their huge different…
```
Using the IQR method, these are the outlier counts:

Measles: 542, HIV/AIDS: 542, under-five deaths: 394, percentage expenditure: 389, GDP: 365, Diphtheria: 298, Population: 294, Polio: 279, Hepatitis B: 254, thinness 5-9 years: 96, thinness 1-19 years: 89, Adult Mortality: 82, Income composition of resources: 130, Schooling: 44, Total expenditure: 32, Life expectancy: 10

As the count is quite high for some columns and the values seem illogical in some cases. For example, the measles columns are occurances per 1000 inhabitants, yet there are hundreds of values above 1000 (up to 200000). Similarly the infant deaths and under-five deaths are also measured per 1000 and have values above 1000.
```
- The BMI data shows BMI up to 60 which is impossible. Does anyone know how to interpret these data points or are they just incorrect? I see there was a question about this 5 years ago but nothing since.
```
If you pay attention to some numbers such as GDP, you will see that those numbers have their decimal points placed wrong. Or, it could be that the author totally fabricated those numbers. In a word, don't use that for serious research; your research will be ruined.
```
```
DO NOT USE the data set since the data quality is really low.
For example

The maximum infant death number (per 1000 population) is 1800
The maximum Expenditure on health as a percentage of Gross Domestic Product per capita(%) is 19479.912
The maximum number of reported Measles cases per 1000 population is 212183
The maximum Average Body Mass Index of the entire population is 87.30
The maximum number of under-five deaths per 1000 population is 2500
The minimum population of the country is 34
```

```
I did a bit of research:
China 2015: the number 42 361 exactly matches cases of Measels in China rather than cases per 1000 in China.
source WHO China
Same goes for the 11 cases in Bhutan 2015.
source WHO Butan

So I guess it is like this for all countries, they forgot to divide by population.
```
- I noticed that France and Finland have been misclassified as developing countries.

```
There are values which are clearly impossible, like India having 1800 per 1000 population.
There are values which are unreasonable, like Switzerland and Sweden having less than 1 per 1000.
There are values which look like they are reasonable, but are wrong. For the years 2013-2015, Uzbekistan has values from 15 to 17. But going to the WHO web site, the actual values should be 23.0 to 26.3
```
---

The raw Life Expectancy panel is widely used but has **documented inconsistencies and missingness**. The most important issues for our analysis are:

1. **Substantial predictor missingness** (e.g., population, GDP, HepB coverage, schooling), which reduces the complete‑case sample size.
2. **Ambiguous or inconsistent units** in several columns. Example: `population` behaves like a mix of absolute counts and scaled values for some countries; `measles` and child deaths look like **counts**, not rates, in many rows.
3. **Variable definitions differ across sources** (e.g., our `hiv_aids` is a death-related measure, while some cleaned datasets use HIV *incidence* per 1,000). These are not interchangeable.
4. **Country naming inconsistencies** (e.g., “Bolivia (Plurinational State of)” vs “Bolivia”) which complicate merges with external datasets.

These issues are why we make missingness and unit checks explicit in this chapter and treat all downstream inference as descriptive.

## Reproducing a “cleaned & imputed” version (conceptual recipe)

The updated dataset you provided uses **external sources** (WDI, WHO, OWID), **country/region reclassification**, and **imputation**. If we want to reproduce a similar cleaning pipeline (without switching datasets), the steps would be:

1. **Source updates by variable**
   - Replace *life expectancy* with WHO series.
   - Replace *GDP per capita* and *population* with World Bank series (note: their units differ; WDI population is absolute counts, the updated dataset stores population in millions).
   - Replace *schooling* with OWID (mean years of schooling).
   - Replace *vaccination coverage* and *mortality* metrics with WHO indicator series.
   - Replace *HIV variable* consistently (decide on **deaths** vs **incidence** and stick to one).

2. **Country name and ISO3 normalization**
   - Create a deterministic ISO3 mapping (manual overrides for known mismatches).
   - Use ISO3 as the merge key across sources.

3. **Imputation policy**
   - For intermittent gaps: **nearest 3‑year average** (as described in the updated dataset).
   - For entirely missing series per country: **regional averages**.
   - Drop countries with **>4 missing columns** (or document the threshold and sensitivity).

4. **Document units and transformations**
   - Align units (e.g., population in absolute counts vs millions) before merging.
   - Explicitly record any re‑scaling in metadata (`LECol`).

We do **not** apply this full pipeline in the current report to avoid mixing sources mid‑analysis. Instead, we keep the Kaggle dataset as the primary source and report missingness, transformations, and sensitivity checks transparently.

---
Description from "https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated/data":

About Dataset
Data contains life expectancy, health, immunization, and economic and demographic information about 179 countries from 2000-2015 years. The adjusted dataset has 21 variables and 2.864 rows.

Data were initially collected from Kaggle Source.

The dataset had inaccurate data and a lot of values were missing.

The dataset is completely updated.

Data about Population, GDP, and Life Expectancy was updated according to World Bank Data. Information about vaccinations for Measles, Hepatitis B, Polio, and Diphtheria, alcohol consumption, BMI, HIV incidents, mortality rates, and thinness were collected from World Health Organization public datasets. Information about Schooling was collected from the Our World in Data which is a project of the University of Oxford.

The data had some missing values. A few strategies for filling in missing values were applied.

Filling data with the closest three-year average. If a specific country had a missing value in any year, the data was filled with the closest three-year average.
Filling data with the average of the Region. If a specific country was missing values for all years, the data was filled with the average of the Region (e.g. Asia, Africa, European Union, etc.)
Data is adjusted and the missing values are filled. Countries that were missing more than 4 data columns were omitted from the database. Examples of these countries are Sudan, South Sudan, and North Korea.

The database has one variable that categorizes countries into two groups: Developed vs Developing countries. According to World Trade Organization, each country defines itself as “Developed” or “Developing”. Therefore, it is challenging to categorize countries. UN has a list dated 2014 that for analytical purposes classifies countries as developed, in transition, and developing economies. Countries that have economies in transition have similar characteristics to the countries that are categorized as developed or developing countries. Countries have been grouped according to their Gross National Income per capita. As a result, nations were divided into four income groups: high-income, higher-middle-income, lower-middle-income, and low-income. The levels of Gross Domestic Income are set by the World Bank to ensure comparability.

Data Sources: Average life expectancy of both genders in different years from 2000 to 2015: https://www.who.int/data/gho/data/indicators/indicator-details/GHO/life-expectancy-at-birth-(years) Mortality-related attributes (infant deaths, under-five-deaths, adult mortality): https://www.who.int/data/gho/data/themes/mortality-and-global-health-estimates Alcohol consumption that is recorded in liters of pure alcohol per capita with 15+ years old: https://www.who.int/data/gho/data/indicators/indicator-details/GHO/alcohol-recorded-per-capita-(15-)-consumption-(in-litres-of-pure-alcohol) % of coverage of Hepatitis B (HepB3) immunization among 1-year-olds: https://www.who.int/data/gho/data/indicators/indicator-details/GHO/hepatitis-b-(hepb3)-immunization-coverage-among-1-year-olds-(-) % of coverage of Measles containing vaccine first dose (MCV1) immunization among 1-year-olds: https://www.who.int/data/gho/data/indicators/indicator-details/GHO/measles-containing-vaccine-first-dose-(mcv1)-immunization-coverage-among-1-year-olds-(-) % of coverage of Polio (Pol3) immunization among 1-year-olds: https://www.who.int/data/gho/data/indicators/indicator-details/GHO/polio-(pol3)-immunization-coverage-among-1-year-olds-(-) % of coverage of Diphtheria tetanus toxoid and pertussis (DTP3) immunization among 1-year-olds: https://www.who.int/data/gho/data/indicators/indicator-details/GHO/diphtheria-tetanus-toxoid-and-pertussis-(dtp3)-immunization-coverage-among-1-year-olds-(-) BMI: https://www.who.int/europe/news-room/fact-sheets/item/a-healthy-lifestyle---who-recommendations Incidents of HIV per 1000 population aged 15-49: https://data.worldbank.org/indicator/SH.HIV.INCD.ZS Prevalence of thinness among adolescents aged 10-19 years. BMI < -2 standard deviations below the median: https://www.who.int/data/gho/indicator-metadata-registry/imr-details/4805 GDP per capita in current USD: https://data.worldbank.org/indicator/NY.GDP.PCAP.CD?most_recent_year_desc=true Total population in millions: https://data.worldbank.org/indicator/SP.POP.TOTL?most_recent_year_desc=true Average years that people aged 25+ spent in formal education: https://ourworldindata.org/grapher/mean-years-of-schooling-long-run
-->

# Missing Values and analysis views

In the raw panel, 10 of 2,938 country-year rows have missing life expectancy; these correspond to 10 countries that appear only once in the dataset. When `drop_missing_target=True`, these rows (and countries) are removed before any further processing.

::: {.callout-note collapse="true"}
### Additional countries in 2013

```{python}
# | include: false
import pandas as pd
from ama_tlbx.data import LECol, LifeExpectancyDataset
```

```{python}
# | label: tbl-extra-2013-countries
# | tbl-cap: "Countries present in 2013 but absent from the 183-country baseline (n=10)."
panel = LifeExpectancyDataset.from_csv(
    aggregate_by_country=False,
    resolve_nand_pred=False,
    drop_missing_target=False,
).df

countries_by_year = (
    panel.assign(year=lambda d: d[LECol.YEAR].dt.year)
    .query("year != 2013")
    .groupby("year")[LECol.COUNTRY]
    .apply(lambda s: frozenset(s.dropna().unique()))
)
baseline_countries = countries_by_year.value_counts().idxmax()

countries_2013 = set(
    panel.assign(year=lambda d: d[LECol.YEAR].dt.year)
    .query("year == 2013")[LECol.COUNTRY]
    .dropna()
    .unique()
)
extra_countries_2013 = sorted(countries_2013 - set(baseline_countries))

panel.query("country in @extra_countries_2013 and year == 2013").set_index(
    LECol.COUNTRY
)
```

Given the singular appearance of these ten countries in 2013 only, their missing life expectancy values and addtional high degree of predictor missingness, we exclude them from all analyses by default.
:::

Predictor missingness is controlled by `resolve_nand_pred`. Some analyses require a fully observed predictor matrix (e.g., PCA, standardized regression), while others can operate on partially observed predictors (e.g., univariate plots of the target). We therefore distinguish the raw 2014 cross-section from complete-case and imputed variants, and we report the effective sample size in each chapter.
<!-- Population of some countries like Israel, Srilanka are in thousands, while other in millions. -->

# Raw panel consistency checks

Throughout the analysis, we observe that the raw panel has inconsistent units and significant missingness. These issues are consistend with various discussions on the Kaggle page of the dataset.
Below are minimal checks on the **raw panel** (`aggregate_by_country=False`, no imputation) that surface those issues directly.

```{python}
# | label: tbl-panel-missingness
# | tbl-cap: "Top missing predictors in the raw panel (share of missing values)."
from ama_tlbx.data import LECol, LifeExpectancyDataset

panel = LifeExpectancyDataset.from_csv(
    aggregate_by_country=False,
    resolve_nand_pred=False,
    drop_missing_target=False,
).df

(
    panel.isna()
    .mean()
    .mul(100)
    .rename("missing_pct")
    .to_frame()
    .assign(n_missing=lambda d: (d["missing_pct"] * len(panel) / 100).round().astype(int))
    .sort_values("missing_pct", ascending=False)
    .head(10)
)
```

The raw panel’s missingness is concentrated in a small subset of predictors (notably population, HepB coverage, and GDP per capita), which explains the sharp drop in complete‑case sample size. Especially the missingness of easily sourced variables like population and GDP suggest sloppy data assembly and fundametal data gaps.


We proceed with unit and range checks of key variables which were labeled as per‑1,000 rates in the original dataset.

```{python}
# | label: tbl-rate-vs-counts
# | tbl-cap: "Rates vs counts check: extreme magnitudes for variables labeled as per‑1000."
(
    panel.loc[
        :,
        [
            LECol.COUNTRY,
            LECol.YEAR,
            LECol.INFANT_DEATHS,
            LECol.UNDER_FIVE_DEATHS,
            LECol.MEASLES,
        ],
    ]
    .assign(year=lambda d: d[LECol.YEAR].dt.year)
    .melt(id_vars=[LECol.COUNTRY, "year"], var_name="feature", value_name="value")
    .groupby("feature")[
        "value"
    ]
    .describe()
)
```

The magnitudes above show that some samples of `measles`, `infant_deaths`, and `under_five_deaths` behave like **counts**, not per‑1,000 rates. While the lower quantiles suggest that many rows are indeed rates, the extreme maxima are certainly inconsistent with the variable definitions and suggest incoherence within columns.

```{python}
# | label: tbl-iqr-outliers
# | tbl-cap: "IQR outlier counts (1.5×IQR rule) for key variables cited in Kaggle comments."
iqr_cols = [
    LECol.MEASLES,
    LECol.HIV_AIDS,
    LECol.UNDER_FIVE_DEATHS,
    LECol.PERCENTAGE_EXPENDITURE,
    LECol.GDP,
    LECol.DIPHTHERIA,
    LECol.POPULATION,
    LECol.POLIO,
    LECol.HEPATITIS_B,
    LECol.THINNESS_5_9_YEARS,
    LECol.THINNESS_1_19_YEARS,
    LECol.ADULT_MORTALITY,
    LECol.HDI,
    LECol.SCHOOLING,
    LECol.TOTAL_EXPENDITURE,
    LECol.LIFE_EXPECTANCY,
]

(
    LifeExpectancyDataset(df=panel)
    .make_iqr_outlier_detector(columns=iqr_cols, standardized=False, threshold=1.5)
    .fit()
    .result()
    .n_outliers_per_column.rename("iqr_outliers")
    .to_frame()
    .assign(
        feature=lambda d: d.index.map(LifeExpectancyDataset(df=panel).get_pretty_name)
    )
    .loc[:, ["feature", "iqr_outliers"]]
    .sort_values("iqr_outliers", ascending=False)
)
```

The IQR counts in Table @tbl-iqr-outliers quantify the heavy tails flagged by Kaggle users: measles, HIV/AIDS, under‑five deaths, and percentage expenditure dominate the outlier ranks, consistent with the extreme maxima and skewed distributions above.

Another check focuses on `percentage_expenditure`, which is labeled as a percentage of the GDP that is spent on healthcare.

```{python}
# | label: tbl-percentage-expenditure-check
# | tbl-cap: "`percentage_expenditure` exceeds 100 for a large share of rows (not a literal percent)."
(
    panel[[LECol.PERCENTAGE_EXPENDITURE]]
    .dropna()
    .assign(gt_100=lambda d: d[LECol.PERCENTAGE_EXPENDITURE] > 100)
    .agg(
        share_gt_100=("gt_100", "mean"),
        p99=(LECol.PERCENTAGE_EXPENDITURE, lambda s: s.quantile(0.99)),
        max_val=(LECol.PERCENTAGE_EXPENDITURE, "max"),
    )
)
```

```{python}
# | label: tbl-percentage-expenditure-check-2
# | tbl-cap: "`percentage_expenditure` exceeds 100 for a large share of rows (not a literal percent)."
panel[[LECol.PERCENTAGE_EXPENDITURE]].dropna().describe().T
```

Nearly half of the rows exceed 100 in `percentage_expenditure`, indicating that it should be interpreted as a scaled spending measure rather than a literal percentage; The mean "percentage" expenditure being 738% furthe supports extreme inconsistency with the column.

```{python}
# | label: tbl-population-scale-check
# | tbl-cap: "Population values span tiny and very large scales (possible unit inconsistency)."
(
    panel[[LECol.POPULATION]]
    .dropna()
    .agg(
        min_pop=(LECol.POPULATION, "min"),
        p1=(LECol.POPULATION, lambda s: s.quantile(0.01)),
        p99=(LECol.POPULATION, lambda s: s.quantile(0.99)),
        max_pop=(LECol.POPULATION, "max"),
        share_lt_1e5=(LECol.POPULATION, lambda s: (s < 1e5).mean() * 100),
    )
)
```

```{python}
# | label: tbl-population-min-countries
# | tbl-cap: "Countries with the smallest reported population values (raw panel; minimum over years)."
pop_min = (
    panel[[LECol.COUNTRY, LECol.YEAR, LECol.POPULATION]]
    .dropna(subset=[LECol.POPULATION])
    .assign(year=lambda d: d[LECol.YEAR].dt.year.astype(int))
)
idx = pop_min.groupby(LECol.COUNTRY)[LECol.POPULATION].idxmin()
(
    pop_min.loc[idx, [LECol.COUNTRY, "year", LECol.POPULATION]]
    .rename(columns={LECol.POPULATION: "min_population"})
    .sort_values("min_population", ascending=True)
    .head(10)
)
```

Population values range from double‑digit counts to 1.3B. The non‑trivial share below 100k suggests mixed scaling (e.g., absolute counts vs millions or thousands) across countries. This is also visible in Table @tbl-population-min-countries, which shows countries with implausibly small populations (e.g., 34 for the Maldives and 657 for Israel), while some countries have populations in the billions.

## Outlier detectors as integrity checks (2014 cross-section) {#sec-outlier-integrity}

Outliers are not automatically “bad”: a country can be extreme because it is genuinely different. In this dataset, however, extreme values can also reflect **unit mismatches**, **corrupted entries**, or **merge artifacts** (as suggested by the checks above). We therefore use outlier detection primarily as an *integrity diagnostic* and interpret it jointly with unit/range checks, rather than applying a global trimming rule.

We compare three detectors implemented in `ama-tlbx`:

- **IQR rule (feature-wise, robust):** flags values outside Tukey fences
  $$
  [Q_1 - 1.5\cdot IQR,\; Q_3 + 1.5\cdot IQR], \qquad IQR = Q_3 - Q_1,
  $$
  which is distribution-agnostic and suited for heavy-tailed variables.
- **Z-score threshold (feature-wise):** flags values with $|z| > 3$ after standardization (conservative under skewness).
- **Isolation Forest (row-wise):** flags globally unusual country profiles in multivariate space.

All detectors below run on the **2014 complete-case subset** (after carry-forward + dropping remaining missing predictors), because standardization and Isolation Forest require a fully observed predictor matrix. This also means that the effective $n$ differs from the raw 2014 cross-section.

```{python}
# | label: tbl-outlier-detector-summary-2014
# | tbl-cap: "Outlier detector comparison on the 2014 complete-case subset (transformed predictors)."
import pandas as pd

ds_2014_cc = LifeExpectancyDataset.from_csv(
    aggregate_by_country=2014,
    resolve_nand_pred="carry_forward",
)
ds_2014_tf = LifeExpectancyDataset(df=ds_2014_cc.tf_only())
cols = ds_2014_cc.feature_columns(include_target=False)

iqr_res = (
    ds_2014_tf.make_iqr_outlier_detector(columns=cols, standardized=False, threshold=1.5)
    .fit()
    .result()
)
z_res = (
    ds_2014_tf.make_zscore_outlier_detector(columns=cols, standardized=True, threshold=3.0)
    .fit()
    .result()
)
if_res = (
    ds_2014_tf.make_isolation_forest_outlier_detector(
        columns=cols,
        standardized=True,
        contamination="auto",
        random_state=42,
    )
    .fit()
    .result()
)

(
    pd.DataFrame(
        [
            {
                "method": "IQR (k=1.5)",
                "n_rows": int(iqr_res.outlier_mask.shape[0]),
                "rows_with_≥1_flag": int(iqr_res.outlier_mask.any(axis=1).sum()),
                "share_rows_%": float(iqr_res.outlier_mask.any(axis=1).mean() * 100),
                "median_flags_per_row": float(iqr_res.n_outliers_per_row.median()),
            },
            {
                "method": "Z-score (|z|>3)",
                "n_rows": int(z_res.outlier_mask.shape[0]),
                "rows_with_≥1_flag": int(z_res.outlier_mask.any(axis=1).sum()),
                "share_rows_%": float(z_res.outlier_mask.any(axis=1).mean() * 100),
                "median_flags_per_row": float(z_res.n_outliers_per_row.median()),
            },
            {
                "method": "Isolation Forest (row-wise)",
                "n_rows": int(if_res.outlier_mask.shape[0]),
                "rows_with_≥1_flag": int(if_res.outlier_mask.any(axis=1).sum()),
                "share_rows_%": float(if_res.outlier_mask.any(axis=1).mean() * 100),
                "median_flags_per_row": float(if_res.n_outliers_per_row.median()),
            },
        ]
    )
    .assign(share_rows_pct=lambda d: d["share_rows_%"].round(1))
    .drop(columns=["share_rows_%"])
    .rename(columns={"share_rows_pct": "share_rows_%"} )
)
```

```{python}
# | label: tbl-outlier-top-features-2014
# | tbl-cap: "Top features by IQR-flagged values (2014 complete-case subset; transformed predictors)."
(
    iqr_res.n_outliers_per_column.sort_values(ascending=False)
    .head(10)
    .rename("iqr_outliers")
    .to_frame()
    .assign(feature=lambda d: d.index.map(iqr_res.pretty_names.get))
    .reset_index(drop=True)
    .loc[:, ["feature", "iqr_outliers"]]
)
```

Tables @tbl-outlier-detector-summary-2014 and @tbl-outlier-top-features-2014 show that univariate IQR flags substantially more marginal extremes than a Gaussian-tail Z-score rule (which should be about 0.135% for |z|>3), while Isolation Forest identifies a smaller set of globally unusual country profiles (row-wise anomalies).

We continue by reproducing further claims from Kaggle comments regarding specific variables and countries.

```{python}
# | label: tbl-bmi-extremes
# | tbl-cap: "BMI values above 60 (raw panel)."
(
    panel.assign(year=lambda d: d[LECol.YEAR].dt.year)
    .loc[:, [LECol.COUNTRY, "year", LECol.BMI]]
    .dropna()
    .query("bmi >= 60")
    .sort_values(LECol.BMI, ascending=False)
    .head(12)
)
```

Another set of inconsistencies involves BMI values above 60, which are biologically implausible. Table @tbl-bmi-extremes lists all such entries in the raw panel.

```{python}
# | label: tbl-status-france-finland
# | tbl-cap: "Status for France and Finland (raw panel; 0=Developing, 1=Developed)."
(
    panel.assign(year=lambda d: d[LECol.YEAR].dt.year)
    .query("country in ['France', 'Finland']")
    .groupby(LECol.COUNTRY)
    .agg(
        status_values=(LECol.STATUS, lambda s: sorted(s.dropna().unique().tolist())),
        share_developed=(LECol.STATUS, "mean"),
    )
    .reset_index()
)
```

Classifying Finland and France as developing countries doesn't align with standard economic classifications.


**Implication for preprocessing.** The evidence above indicates that several variables are on inconsistent or ambiguous scales (counts vs per‑1,000 rates; percent vs scaled spending), and that missingness is concentrated in a few high‑impact predictors. Simple cleaning and imputation techniques are certainly not sufficient to resolve these issues. The cleaned Version of the [Life Expectancy dataset](https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated/data) addresses many of these problems by sourcing consistent series from WHO, WDI, and OWID, applying systematic imputation, and normalizing country names.

## Why consider the updated Kaggle dataset (and what cleaning would be required)

The diagnostics above show that the raw Kaggle panel requires **non‑trivial cleaning** before it can serve as a high‑quality regression dataset: unit ambiguities (counts vs rates), scale inconsistencies (population and GDP), and concentrated missingness in core predictors. The cleaned dataset at https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated/data is appealing because it replaces several columns with values from external sources (WHO, WDI, OWID) and applies systematic imputation.

To reproduce that quality *without switching datasets*, we would need to: (i) replace population and GDP with World Bank series (rescaling units where needed), (ii) replace immunization and mortality metrics with WHO indicators, (iii) align schooling with OWID, (iv) normalize ISO3 country codes across sources, and (v) implement the reported imputation policy (nearest three‑year average + regional averages; drop countries with too many missing columns). Given limited time, using the cleaned dataset for regression is a pragmatic choice, while retaining the raw Kaggle panel for transparency and sensitivity analyses in preprocessing and diagnostics.

To make this decision concrete, Table @tbl-raw-vs-updated-coverage compares basic coverage, Table @tbl-raw-vs-updated-missingness compares missingness for shared columns, and Table @tbl-raw-vs-updated-ranges shows that several variables move back into plausible ranges in the updated dataset. Importantly, these improvements come with **semantic changes** for some variables (e.g., `measles` becomes an immunization-coverage percentage in the updated file), so we treat the updated dataset as a *different measurement table* rather than a drop-in “imputation-only” patch.

```{python}
# | label: tbl-raw-vs-updated-coverage
# | tbl-cap: "Coverage comparison: original Kaggle table vs updated dataset (panel, 2000–2015)."
import pandas as pd
import warnings

from ama_tlbx.data import LifeExpectancyDataset

with warnings.catch_warnings():
    warnings.filterwarnings("ignore", message=r"from_csv_updated\(\): mapping incidents_hiv.*")
    raw_panel = LifeExpectancyDataset.from_csv(
        aggregate_by_country=False,
        resolve_nand_pred=False,
        drop_missing_target=True,
    ).df
    upd_panel = LifeExpectancyDataset.from_csv_updated(
        aggregate_by_country=False,
        resolve_nand_pred=False,
        drop_missing_target=True,
        merge_original_expenditure=False,
    ).df

pd.DataFrame(
    [
        {
            "dataset": "Original Kaggle",
            "n_rows": int(len(raw_panel)),
            "n_countries": int(raw_panel[LECol.COUNTRY].nunique()),
            "year_min": int(raw_panel[LECol.YEAR].dt.year.min()),
            "year_max": int(raw_panel[LECol.YEAR].dt.year.max()),
        },
        {
            "dataset": "Updated Kaggle",
            "n_rows": int(len(upd_panel)),
            "n_countries": int(upd_panel[LECol.COUNTRY].nunique()),
            "year_min": int(upd_panel[LECol.YEAR].dt.year.min()),
            "year_max": int(upd_panel[LECol.YEAR].dt.year.max()),
        },
    ],
)
```

```{python}
# | label: tbl-raw-vs-updated-missingness
# | tbl-cap: "Missingness comparison for shared columns (panel, 2000–2015; % missing)."
shared_cols = sorted(set(raw_panel.columns) & set(upd_panel.columns))
shared_numeric = (
    pd.Index(shared_cols)
    .difference([LECol.COUNTRY, LECol.YEAR])
    .intersection(raw_panel.select_dtypes(include=["number"]).columns)
    .intersection(upd_panel.select_dtypes(include=["number"]).columns)
)

missing_cmp = (
    pd.DataFrame(
        {
            "raw_missing_pct": raw_panel[shared_numeric].isna().mean() * 100,
            "updated_missing_pct": upd_panel[shared_numeric].isna().mean() * 100,
        },
    )
    .assign(delta_pct=lambda d: d["updated_missing_pct"] - d["raw_missing_pct"])
    .sort_values("raw_missing_pct", ascending=False)
    .assign(feature=lambda d: d.index.map(LifeExpectancyDataset(df=raw_panel).get_pretty_name))
    .loc[:, ["feature", "raw_missing_pct", "updated_missing_pct", "delta_pct"]]
    .head(12)
    .rename(
        columns={
            "feature": "Feature",
            "raw_missing_pct": "Original (%)",
            "updated_missing_pct": "Updated (%)",
            "delta_pct": "Δ (Updated - Original)",
        },
    )
)

missing_cmp.style.format(
    {
        "Original (%)": "{:.1f}",
        "Updated (%)": "{:.1f}",
        "Δ (Updated - Original)": "{:+.1f}",
    },
)
```

```{python}
# | label: tbl-raw-vs-updated-ranges
# | tbl-cap: "Selected range checks: original vs updated dataset (panel; min/median/max)."
cols_compare = [
    LECol.POPULATION,
    LECol.GDP,
    LECol.INFANT_DEATHS,
    LECol.UNDER_FIVE_DEATHS,
    LECol.MEASLES,
    LECol.BMI,
]
cols_compare = [c for c in cols_compare if c in raw_panel.columns and c in upd_panel.columns]

raw_sel = raw_panel.loc[:, cols_compare]
upd_sel = upd_panel.loc[:, cols_compare]

range_cmp = (
    pd.concat(
        [
            pd.DataFrame(
                {
                    "min": raw_sel.min(),
                    "median": raw_sel.median(),
                    "p99": raw_sel.quantile(0.99),
                    "max": raw_sel.max(),
                },
            ).assign(dataset="Original"),
            pd.DataFrame(
                {
                    "min": upd_sel.min(),
                    "median": upd_sel.median(),
                    "p99": upd_sel.quantile(0.99),
                    "max": upd_sel.max(),
                },
            ).assign(dataset="Updated"),
        ],
    )
    .reset_index(names="col")
    .assign(feature=lambda d: d["col"].map(LifeExpectancyDataset(df=raw_panel).get_pretty_name))
    .loc[:, ["feature", "dataset", "min", "median", "p99", "max"]]
    .sort_values(["feature", "dataset"])
)

range_cmp.style.format({"min": "{:,.2f}", "median": "{:,.2f}", "p99": "{:,.2f}", "max": "{:,.2f}"})
```

Taken together, Tables @tbl-raw-vs-updated-missingness and @tbl-raw-vs-updated-ranges show that the updated dataset (i) reduces missingness in high-impact predictors like population and GDP, and (ii) moves several “per-1000 / percent”-type variables back into plausible ranges (e.g., measles values bounded by 100; child deaths no longer in the thousands). At the same time, it changes the semantics of some columns (notably `measles` and `hiv_aids`), so our modeling chapters treat the updated dataset as the preferred measurement table and interpret coefficients descriptively.


:::{.callout-note collapse="false"}

### Dataset snapshots

**The following section is outdated and proceeds the claims from the previous section. It is kept here for completeness.**


```{python}
# | label: tbl-dataset-snapshots
# | tbl-cap: "Dataset snapshots under alternative missing-value strategies."
import pandas as pd
from matplotlib import pyplot as plt

from ama_tlbx.data import LifeExpectancyDataset
from ama_tlbx.utils.plotting_config import DEFAULT_PLOT_CFG

DEFAULT_PLOT_CFG.apply_global()

snapshots = {
    "panel_raw": LifeExpectancyDataset.from_csv(
        aggregate_by_country=False,
        resolve_nand_pred=False,
    ).df,
    "xs_2014_raw": LifeExpectancyDataset.from_csv(
        aggregate_by_country=2014,
        resolve_nand_pred=False,
    ).df,
    "xs_2014_imputed": LifeExpectancyDataset.from_csv(
        aggregate_by_country=2014,
        resolve_nand_pred="carry_forward",
    ).df,
}


summary_rows = []
for name, df in snapshots.items():
    y_min, y_max = LifeExpectancyDataset.year_bounds(df)
    summary_rows.append(
        {
            "view": name,
            "rows": int(len(df)),
            "countries": LifeExpectancyDataset.country_count(df),
            "year_min": y_min,
            "year_max": y_max,
            "pct_missing_numeric": LifeExpectancyDataset.pct_missing_numeric(df),
        }
    )

(
    pd.DataFrame(summary_rows)
    .sort_values("view")
    .assign(pct_missing_numeric=lambda d: d.pct_missing_numeric.round(2))
    .style
    .format({"pct_missing_numeric": "{:.2f}%"})
)
```

<!-- QUESTION(PROF): How should we deal with the missing values in predictors when analyzing the dataset? -->
@tbl-dataset-snapshots highlights the trade-off between sample size and missing-value handling. In the raw panel, numeric predictors contain 4.36% missing entries; dropping missing targets removes 10 country-year rows and reduces country coverage from 193 to 183 because those countries occur only once in 2013. For the 2014 cross-section, life expectancy is observed for all 183 countries, and predictor missingness is modest on the numeric scale (2.81% in the raw cross-section). Carry-forward reduces this only slightly (2.71%), because the remaining missing values are structurally absent in the historical panel (see below). When a fully observed predictor matrix is required, complete-case analyses reduce the 2014 sample to $n=133$.

## Structural missingness after carry-forward


Carry-forward imputation only fills values that exist in earlier years for the same country. To quantify the remaining gaps, we apply carry-forward to the full panel and then inspect the 2014 slice.

```{python}
# | label: tbl-missing-2014
# | tbl-cap: "Predictors still missing in the 2014 cross-section after carry-forward (counts across 183 countries)."
import numpy as np
import pandas as pd

from ama_tlbx.data import LECol, LifeExpectancyDataset

panel = LifeExpectancyDataset.from_csv(
    aggregate_by_country=False,
    resolve_nand_pred=False,
).df

filled = LifeExpectancyDataset._resolve_missing_predictors(
    panel,
    strategy="carry_forward",
    drop_remaining=False,
)

df_2014 = filled.assign(year=lambda d: d[LECol.YEAR].dt.year).query("year == 2014")
pred_cols = df_2014.columns.difference([LECol.COUNTRY, LECol.YEAR])

le_2014 = LifeExpectancyDataset(df=df_2014)
missing_counts = df_2014[pred_cols].isna().sum().sort_values(ascending=False)

missing_tbl = (
    missing_counts[missing_counts > 0]
    .to_frame(name="n_missing_2014")
    .assign(share_missing_pct=lambda d: d["n_missing_2014"] / len(df_2014) * 100)
    .assign(feature=lambda d: d.index.map(le_2014.get_pretty_name))
    .loc[:, ["feature", "n_missing_2014", "share_missing_pct"]]
    .rename(
        columns={
            "feature": "Feature",
            "n_missing_2014": "Missing (n)",
            "share_missing_pct": "Missing (%)",
        }
    )
)

missing_tbl.style.format({"Missing (%)": "{:.1f}"})
```

```{python}
# | label: tbl-missing-2014-structural
# | tbl-cap: "Missing predictors in 2014 are structurally absent (never observed pre-2014) for the affected country series."
le_hist = (
    panel.assign(year=lambda d: d[LECol.YEAR].dt.year)
    .query("year <= 2014")
    .loc[:, [LECol.COUNTRY, *pred_cols]]
)

missing_2014 = (
    df_2014.loc[:, [LECol.COUNTRY, *pred_cols]]
    .melt(id_vars=[LECol.COUNTRY], var_name="predictor", value_name="value")
    .query("value.isna()", engine="python")
    .drop(columns=["value"])
)

ever_obs = (
    le_hist.melt(id_vars=[LECol.COUNTRY], var_name="predictor", value_name="value")
    .dropna(subset=["value"])
    .drop_duplicates([LECol.COUNTRY, "predictor"])
    .assign(ever_observed_pre2014=True)
    .loc[:, [LECol.COUNTRY, "predictor", "ever_observed_pre2014"]]
)

md = missing_2014.merge(ever_obs, on=[LECol.COUNTRY, "predictor"], how="left").assign(
    ever_observed_pre2014=lambda d: d["ever_observed_pre2014"].fillna(False)
)
summary = pd.DataFrame(
    {
        "metric": [
            "Countries with ≥1 missing predictor (2014)",
            "Missing predictor entries (country × feature)",
            "Entries with no prior observation (pre-2014)",
            "Share of missing entries with no prior observation (%)",
        ],
        "value": [
            int(md[LECol.COUNTRY].nunique()),
            int(len(md)),
            int((~md["ever_observed_pre2014"]).sum()),
            float((~md["ever_observed_pre2014"]).mean() * 100),
        ],
    }
)

summary["value"] = summary["value"].map(
    lambda v: f"{v:.1f}" if isinstance(v, (float, np.floating)) else f"{int(v)}"
)
summary.style
```

Table @tbl-missing-2014 shows that missingness in 2014 is concentrated in a small subset of features (notably HepB coverage, HDI, schooling, and GDP). Table @tbl-missing-2014-structural confirms that **all** missing entries in 2014 correspond to predictors that are never observed for those country series in any earlier year; carry-forward therefore cannot fill them. This structural missingness is why the carry-forward strategy only marginally reduces missingness in the 2014 cross-section. For analyses that require complete predictors (e.g., PCA, standardized regression), we explicitly restrict to complete cases and report the resulting sample size.

## Population backfill from UNDP HDR

Population is available in the UNDP HDR time series (`pop_total`). Because we now map all Life Expectancy country names to ISO3, we can **join on ISO3 + year** and use the UNDP population values to fill remaining missing values in the Kaggle `population` column. This does not overwrite existing Kaggle values; it only backfills missing entries.

```{python}
# | label: tbl-pop-backfill
# | tbl-cap: "Population missingness before vs after UNDP backfill (panel and 2014 cross-section)."
import pandas as pd

from ama_tlbx.data import LECol, LifeExpectancyDataset, UNDPCol, UNDPHDRDataset

le_panel = LifeExpectancyDataset.from_csv(
    aggregate_by_country=False,
    resolve_nand_pred=False,
)
undp = UNDPHDRDataset.from_csv(years=range(2000, 2016))

merged = undp.merge_life_expectancy(le_panel, how="left")

pop_kaggle = merged[LECol.POPULATION]
pop_undp = merged[str(UNDPCol.POP_TOTAL)]

panel_before = int(pop_kaggle.isna().sum())
panel_after = int(pop_kaggle.fillna(pop_undp).isna().sum())

merged_2014 = merged.loc[merged[LECol.YEAR] == 2014]
pop_kaggle_2014 = merged_2014[LECol.POPULATION]
pop_undp_2014 = merged_2014[str(UNDPCol.POP_TOTAL)]

xs_before = int(pop_kaggle_2014.isna().sum())
xs_after = int(pop_kaggle_2014.fillna(pop_undp_2014).isna().sum())

pd.DataFrame(
    [
        {"view": "Panel (2000–2015)", "missing_before": panel_before, "missing_after": panel_after},
        {"view": "2014 cross-section", "missing_before": xs_before, "missing_after": xs_after},
    ]
).rename(
    columns={
        "view": "View",
        "missing_before": "Missing population (Kaggle)",
        "missing_after": "Missing after UNDP backfill",
    },
)
```

If this backfill improves coverage (without introducing inconsistencies), it can be adopted as a preprocessing step for analyses that require population. For transparency, we would still retain the original Kaggle values and only fill missing entries with UNDP HDR.

:::

# Transformations and standardization (`tf_only`, `tf_and_norm`)

```{python}
# | label: tbl-transforms
# | tbl-cap: "Per-column transforms applied before standardization"
import pandas as pd

from ama_tlbx.data.life_expectancy_columns import LifeExpectancyColumn as Col

rows = []
for col in Col:
    rows.append(
        {
            "original": col.original_name,
            "column": col.value,
            "dtype": col.dtype_name,
            "pretty": col.pretty_name,
            "transform": Col.transform_label(col),
        }
    )

(
    pd.DataFrame(rows)
    .assign(
        column=lambda d: d["pretty"],
    )
    .loc[:, ["column", "dtype", "transform"]]
    .sort_values("column")
    .reset_index(drop=True)
    .style.hide(axis="index")
)
```

Table @tbl-transforms documents the deterministic, per-variable transformations applied prior to standardization. Right-skewed, non-negative intensity variables (e.g., GDP, deaths, measles) are transformed using $\log(1+x)$ to reduce leverage from extreme values and to make linear relationships closer to homoscedastic in downstream models. Immunization variables are recorded as coverage percentages; we re-parameterize them as a shortfall $(100-x)$ and apply $\log(1+(100-x))$, so that larger values consistently represent worse coverage while preserving rank order (monotone transform). After transformation, numeric predictors are z-scored to place them on a comparable scale for PCA, clustering, and standardized regression; Pearson correlation is invariant to affine rescaling, so any changes in correlations after preprocessing arise from the nonlinear transforms rather than the z-scoring step. The target (life expectancy, years) remains in its original units. If residual missing values remain, `tf_and_norm()` median-fills numeric predictors only to avoid failures in standardization (this is separate from the explicit imputation choices in Table @tbl-dataset-snapshots).

To make the effect of standardization explicit, Table @tbl-standardization-summary reports per-feature means and standard deviations before and after z-scoring on the 2014 cross-section (predictors mean-imputed within the year; target excluded).

```{python}
# | label: tbl-standardization-summary
# | tbl-cap: "Predictor summary statistics before vs after z-scoring (2014 cross-section; mean-imputed predictors; target excluded)."
import pandas as pd

from ama_tlbx.data import LECol, LifeExpectancyDataset

le_xs_2014 = LifeExpectancyDataset.from_csv(
    aggregate_by_country=2014,
    resolve_nand_pred="carry_forward",
)
ds_predictors = LifeExpectancyDataset(
    df=le_xs_2014.df.drop(columns=[LECol.TARGET]),
)

raw = ds_predictors.df.loc[:, ds_predictors.numeric_cols]
z = ds_predictors.df_standardized.loc[:, ds_predictors.numeric_cols]

summary = (
    pd.DataFrame(
        {
            "mean_raw": raw.mean(),
            "sd_raw": raw.std(ddof=0),
            "mean_z": z.mean(),
            "sd_z": z.std(ddof=0),
        }
    )
    .assign(feature=lambda d: d.index.map(ds_predictors.get_pretty_name))
    .loc[:, ["feature", "mean_raw", "sd_raw", "mean_z", "sd_z"]]
    .rename(
        columns={
            "feature": "Feature",
            "mean_raw": "Mean (raw)",
            "sd_raw": "SD (raw)",
            "mean_z": "Mean (z)",
            "sd_z": "SD (z)",
        }
    )
    .sort_values("Feature")
    .reset_index(drop=True)
)

(
    summary.style.hide(axis="index").format(
        {
            "Mean (raw)": "{:,.2f}",
            "SD (raw)": "{:,.2f}",
            "Mean (z)": "{:.3f}",
            "SD (z)": "{:.3f}",
        }
    )
)
```

Figure @fig-standardization-comparison illustrates why standardization is required for scale-sensitive multivariate methods: on the raw scale, variables measured in large units dominate dispersion, whereas z-scoring places predictors on a comparable scale. For clarity, we exclude the target from this plot.

```{python}
# | label: fig-standardization-comparison
# | fig-cap: "Predictor scale before vs after z-scoring (2014 cross-section; mean-imputed predictors; target excluded)."

ds_predictors.plot_standardization_comparison(figsize=(20, 10)).show()
```

