---
title: "Regression"
format:
  html:
    toc: true
---

```{python}
# | label: init
# | fig-cap: "Dataset loading function hidden"
# | include: false
from pathlib import Path

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.stats as stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm

from ama_tlbx.data import LifeExpectancyDataset, LECol

dataset = LifeExpectancyDataset.from_csv(
    csv_path="../../_data/life_expectancy_data.csv"    
    ,aggregate_by_country="mean")

assert(not dataset.df.empty)

df = dataset.df
```

# Regression Analysis

Performing regression on data is a common analysis method often used in attempt to derive and predict the life expectency using similar yet unknown data. Since the project primarely focuses on finding information as it pertains to life expectency, our regression analysis will fucus on predicting life expectency.

## Liniar Regression Analyis

Liniar regession is a common analysis performed on liniar data. Since life expectency is "mostly" liniar data, it is very fitting to use it to predict life expectency. The regression analysis has 5 parts each described in the figure below. 

```{python}
# | label: liniar-reg
# | fig-cap: "Process for regression"

# 1: Drop rows where target is missing
target_col = LECol.LIFE_EXPECTANCY
reg_df = df.dropna(subset=[target_col])

# 2: Select features (numeric only, excluding target)
X = reg_df.select_dtypes(include=['number']).drop(columns=[target_col])
y = reg_df[target_col]


# 3: Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4: Train model
model = LinearRegression()
model.fit(X_train, y_train)

# 5: Predict
y_pred = model.predict(X_test)
```

The six step process for the regeression analysis is as follows:
1. Defined the target variable we wish to estimate, here it is the Life expectency stored as an enum. We also drop all NA Columns since they cannot be used in a regression algorithm.
2. We extract all numeric features from the dataset excluding the target column to ensure contamination of data.
3. Split the dataset into train and test.
4. Train the model
5. Make predictions for analysis.

Afterwards we can perform a summary analysis using the OLS libary to guage our model capabilities.

```{python}
# | label: regression-summary
# | fig-cap: "Detailed Regression Summary"

# Fit OLS model with statsmodels for detailed summary
X_train_sm = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_train_sm).fit()

print(ols_model.summary())
```

Based on our OLS Regression result we get an `R-Squared: 0.937` Which means that the model explains 93% of the variance of the dataset. `Adj. R-Squared: 0.927` Shows that if you adjust for the number of features the amount still remains high. Showing that the majority of features play a signficant part of the analysis.

```{python}
# | label: liniar-reg-fig
# | echo: false
# | fig-cap: "Plot of how accurate our coefifients are on test data"


plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Life Expectancy')
plt.ylabel('Predicted Life Expectancy')
plt.title('Multivariate Regression: Actual vs Predicted')
plt.tight_layout()
plt.show()

```

The graph above tells a similar story showing a comparison between a countries predicted life expectency and it's actual expecetency on the test set, showing a strong fit given the data. The same strong fit that was shown previously.

### Residual Analysis

```{python}
# | label: residual analysis
# | fig-cap: "Residual Analysis"

residuals = y_test - y_pred

# Plot-Setup
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# --- Plot 1: Residuals vs. Test --
sns.scatterplot(x=y_pred, y=residuals, ax=axes[0], alpha=0.6)
axes[0].axhline(0, color='red', linestyle='--')
axes[0].set_xlabel('Predicted Life expectency')
axes[0].set_ylabel('Residuals')
axes[0].set_title('Residuals for the Fitted Values\n')

# --- Plot 2: Q-Q Plot ---
stats.probplot(residuals, dist="norm", plot=axes[1])
axes[1].set_title('Q-Q Plot\n')
axes[1].get_lines()[0].set_markerfacecolor('C0')
axes[1].get_lines()[0].set_markeredgewidth(0)

plt.tight_layout()
plt.show()
```

### Predictive capabilities

```{python}
# | label: pred-plot-function
# | echo: false

def pred_plot(model, feature_col, df):
    """Plot actual vs predicted values for a single feature regression"""
    feature_name = str(feature_col)
    
    # Get predictions
    y_pred = model.predict(df)
    
    plt.figure(figsize=(10, 6))
    plt.scatter(df[feature_name], df['life_expectancy'], alpha=0.6, label='Actual')
    plt.scatter(df[feature_name], y_pred, alpha=0.6, color='red', label='Predicted')
    
    # Sort for line plot
    sort_idx = df[feature_name].argsort()
    plt.plot(df[feature_name].iloc[sort_idx], y_pred.iloc[sort_idx], 
             color='red', linewidth=2, label='Regression Line')
    
    plt.xlabel(feature_name.replace('_', ' ').title())
    plt.ylabel('Life Expectancy')
    plt.title(f'Life Expectancy vs {feature_name.replace("_", " ").title()}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
```

```{python}
# | label: Prediction Income Composition of Resources
# | fig-cap: "icorpred"

pred = f"{LECol.INCOME_COMPOSITION}"
model = sm.formula.ols(f"life_expectancy ~ {pred}", data=df).fit()

print(model.summary())
pred_plot(model, LECol.INCOME_COMPOSITION, df)

```