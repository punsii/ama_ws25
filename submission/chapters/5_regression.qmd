---
title: "Regression: Explaining and Predicting Life Expectancy"
format:
  html:
    toc: true
    toc-depth: 3
execute:
  warning: false
  message: false
---

# Regression: Explaining and Predicting Life Expectancy

## Aim and research questions

This chapter uses ordinary least squares (OLS) regression to quantify how country-level predictors relate to life expectancy and how well those predictors generalize to another year. The analysis is structured to answer four questions:

1. Which predictors are important for life expectancy, and what is their direction and magnitude of association?
2. Which interaction effects are supported by the data, and how should they be interpreted?
3. Do the OLS assumptions look plausible for the final model (multicollinearity, residual structure, influence)?
4. How well does the model predict out-of-sample (year-based holdout, with optional cross-validation)?

Because the data are observational, all effects are interpreted as conditional associations rather than causal effects.

## Data, transformations, and PCA-based reduction

We work on the 2014 country cross-section to reduce temporal dependence and align with the earlier chapters. For external validation, we evaluate on 2011 after applying the same transformations and PCA mappings learned from 2014 (Table @tbl-split-summary). Numeric predictors are transformed according to the dataset metadata and standardized within each cross-section using `tf_and_norm` (median imputation + z‑scoring). Development status is dummy-encoded as `status_developed` (1 = developed) and the target remains in years. To reduce redundancy and multicollinearity, four correlated feature blocks are compressed using PCA, retaining the minimum number of components per block required to explain at least 80% of within-block variance (Table @tbl-pca-summary). (We intentionally exclude HDI from predictive models to avoid target leakage; see HDI chapter.)

```{python}
# | label: setup
# | include: false
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from ama_tlbx.analysis import FeatureGroup
from ama_tlbx.analysis.model_registry import ModelRegistry
from ama_tlbx.analysis.ols_helper import fit_ols_formula
from ama_tlbx.plotting.regression_plots import plot_observed_vs_fitted
from ama_tlbx.data import LECol, LifeExpectancyDataset
from ama_tlbx.utils.plotting_config import DEFAULT_PLOT_CFG

DEFAULT_PLOT_CFG.apply_global()
np.random.seed(42)

TRAIN_YEAR = 2014
HOLDOUT_YEAR = 2011
STATUS_DUMMY = "status_developed"
```

```{python}
# | label: feature-groups
# | include: true
feature_groups = [
    FeatureGroup(
        name="child_mortality",
        features=[LECol.INFANT_DEATHS, LECol.UNDER_FIVE_DEATHS],
    ),
    FeatureGroup(
        name="child_nutrition",
        features=[LECol.THINNESS_5_9_YEARS, LECol.THINNESS_1_19_YEARS],
    ),
    FeatureGroup(
        name="economic_development",
        features=[LECol.GDP, LECol.PERCENTAGE_EXPENDITURE],
    ),
    FeatureGroup(
        name="immunization",
        features=[LECol.DIPHTHERIA, LECol.HEPATITIS_B, LECol.MEASLES],
    ),
]

reduced_columns = [feature for g in feature_groups for feature in g.features]
```

## Baseline OLS (simple aggregate model)

<!-- TODO: refer to chapter 2_investigate_hdi by direct link, also check it out - do two baseline hdi models, one with the transformation to LE as per the equations by the UNDP for ihealth, the other by just using the default transform for hdi! -->
To anchor the analysis, we start with a compact, transparent baseline on the country‑level mean cross‑section. This mirrors the original regression workflow while using the project’s OLS utilities and registry. We reduce redundancy in the most correlated blocks via PCA (rather than manual aggregation) and keep HDI **excluded** to avoid target leakage (see the HDI investigation). We report an HDI‑only regression only as a leakage baseline for relative information‑criterion context.
```{python}
# | label: baseline-setup
# | code-fold: true
# | output: false
baseline_ds = LifeExpectancyDataset.from_csv(
    aggregate_by_country=TRAIN_YEAR,
    resolve_nand_pred="carry_forward",
    aggregate_by_country=2011,
)

dataset_latest = LifeExpectancyDataset.from_csv(
    resolve_nand_pred="carry_forward",
    aggregate_by_country=2014,
)

assert not dataset.df.empty
assert not dataset_latest.df.empty



baseline_X = baseline_df.select_dtypes(include=["number"]).drop(
    columns=[LECol.TARGET, LECol.HDI],
    errors="ignore",
)
baseline_rhs = " + ".join(baseline_X.columns.astype(str))

# Concatenate columns for 2014 dataset
column_concatinator = ColumnConcatenator(dataset)
dataset2 = column_concatinator.concatenate(
    columns=[
        LECol.HEPATITIS_B,
        LECol.POLIO,
        LECol.DIPHTHERIA,
    ],
    new_column_name="Immunisation",
)
```

```{python}
# | label: tbl-baseline-metrics
# | tbl-cap: "Baseline OLS metrics (country-level mean cross-section; HDI-only is a leakage reference)."
baseline_compare = baseline_registry.compare(sort_by="aic").rename(
    index={
        "baseline_simple": "Baseline (no HDI)",
        "baseline_hdi": "HDI-only (leakage baseline)",
    }
)
# TODO; why unoack metrics? they should be directly "printable" - refer to ols_helper.py
baseline_compare.loc[:, ["r2", "adj_r2", "rmse", "aic", "bic"]].round(3)
```

```{python}
# | label: fig-baseline-fit
# | fig-cap: "Baseline observed vs fitted (OLS)."
# TODO: use plotting methods from results classes
plot_observed_vs_fitted(baseline_diag)
```

<!-- TODO: entire next cell should be visible -->
```{python}
# | label: tbl-baseline-assumptions
# | tbl-cap: "Baseline OLS assumption checks (simple aggregate model)."
baseline_registry.assumptions_table(names=["baseline_simple"]).rename(
    index={"baseline_simple": "Baseline (no HDI)"}
)
concat_dataset = dataset4

# Concatenate columns for latest dataset
column_concatinator_latest = ColumnConcatenator(dataset_latest)
dataset2_latest = column_concatinator_latest.concatenate(
    columns=[
        LECol.HEPATITIS_B,
        LECol.POLIO,
        LECol.DIPHTHERIA,
    ],
    new_column_name="Immunisation",
)

column_concatinator2_latest = ColumnConcatenator(dataset2_latest)
dataset3_latest = column_concatinator2_latest.concatenate(
    columns=[
        LECol.THINNESS_1_19_YEARS,
        LECol.THINNESS_5_9_YEARS,
    ],
    new_column_name="Child Thinness",
)

column_concatinator3_latest = ColumnConcatenator(dataset3_latest)
dataset4_latest = column_concatinator3_latest.concatenate(
    columns=[
        LECol.INFANT_DEATHS,
        LECol.UNDER_FIVE_DEATHS,
    ],
    new_column_name="Baby Deaths",
)
concat_dataset_latest = dataset4_latest

assert not concat_dataset.df.empty
assert not concat_dataset_latest.df.empty

# Keep untransformed versions for analyses that need original columns
df_raw = concat_dataset.df
df_latest_raw = concat_dataset_latest.df

# Create normalized versions for regression
df = concat_dataset.tf_and_norm()
df_latest = concat_dataset_latest.tf_and_norm()
```

<!--  TODO: Remove this code cell, don't render python to markdown -->
```{python}
# | label: baseline-interpretation
# | results: asis
from IPython.display import Markdown, display

baseline_tbl = baseline_compare.loc[:, ["adj_r2", "rmse", "aic", "bic"]].copy()
if {
    "Baseline (no HDI)",
    "HDI-only (leakage baseline)",
}.issubset(baseline_tbl.index):
    base = baseline_tbl.loc["Baseline (no HDI)"]
    hdi = baseline_tbl.loc["HDI-only (leakage baseline)"]
    delta_rmse = float(hdi["rmse"] - base["rmse"])
    delta_aic = float(hdi["aic"] - base["aic"])
    delta_bic = float(hdi["bic"] - base["bic"])
    text = (
        f"The no‑HDI baseline achieves adj. $R^2$ {base['adj_r2']:.3f} with RMSE "
        f"{base['rmse']:.2f} years, while the HDI‑only leakage reference is weaker "
        f"(ΔRMSE = {delta_rmse:.2f}, ΔAIC = {delta_aic:.1f}, ΔBIC = {delta_bic:.1f}). "
        "We therefore keep the HDI model only as a context point rather than a candidate."
    )
else:
    text = (
        "The no‑HDI baseline outperforms the HDI‑only leakage reference across "
        "fit and information criteria, so HDI is shown only for context."
    )

# TODO: neither of these conversions and upacking should be necessary. directly print the __repr__ of the assumptionschecksresult or the table!
assumptions_base = baseline_registry.assumptions_table(names=["baseline_simple"])
if not assumptions_base.empty:
    row = assumptions_base.iloc[0]
    dw = float(row["durbin_watson"])
    jb_p = float(row["jarque_bera_pvalue"])
    sh_p = float(row["shapiro_pvalue"])
    bp_p = float(row["breusch_pagan_pvalue"])
    white_p = float(row["white_pvalue"])
    normality = "reject" if min(jb_p, sh_p) < 0.05 else "do not reject"
    hetero = "evidence" if min(bp_p, white_p) < 0.05 else "limited evidence"
    text += (
        f" Baseline diagnostics show DW = {dw:.2f} (little autocorrelation). "
        f"Normality tests {normality} normal residuals (JB p={jb_p:.3f}, "
        f"Shapiro p={sh_p:.3f}), and heteroscedasticity tests show {hetero} "
        f"(BP p={bp_p:.3f}, White p={white_p:.3f})."
    )

display(Markdown(text))
```

```{python}
# | label: data-prep
train_ds_raw = LifeExpectancyDataset.from_csv(
    aggregate_by_country=TRAIN_YEAR,
    resolve_nand_pred="carry_forward",
)

holdout_ds_raw = LifeExpectancyDataset.from_csv(
    aggregate_by_country=HOLDOUT_YEAR,
    resolve_nand_pred="carry_forward",
)

# TODO: direct interaction with the ama-tlbx should always be displayed in the chapter
df_train_raw = train_ds_raw.tf_and_norm().drop(columns=[LECol.YEAR])
df_holdout_raw = holdout_ds_raw.tf_and_norm().drop(columns=[LECol.YEAR])

# TODO: direct interaction with the ama-tlbx should always be displayed in the chapter
pca_groups = (
    LifeExpectancyDataset(df=df_train_raw)
    .make_pca_dim_reduction_analyzer(
        feature_groups,
        columns=reduced_columns,
        standardized=False,
        min_var_explained=0.8,
    )
    .fit()
    .result()
)

# TODO: direct interaction with the ama-tlbx should always be displayed in the chapter
pc_train = pca_groups.reduced_df.copy()
pc_holdout = pca_groups.transform(df_holdout_raw).copy()

# Replace original correlated columns with retained PCs
pc_cols = pc_train.columns.tolist()

df_train = pc_train.assign(**df_train_raw.drop(columns=reduced_columns))
df_holdout = pc_holdout.assign(**df_holdout_raw.drop(columns=reduced_columns))

model_cols = [
    LECol.TARGET,
    STATUS_DUMMY,
    LECol.ADULT_MORTALITY,
    LECol.HIV_AIDS,
    LECol.TOTAL_EXPENDITURE,
    *pc_cols,
]

# TODO: instantiation of the dataset and direct interaction with the ama-tlbx should always be displayed in the chapter
df_train_model = (
    LifeExpectancyDataset(df=df_train)
    .df.loc[:, model_cols]
    .reset_index(drop=False)
    .set_index(LECol.COUNTRY)
)

df_holdout_model = (
    LifeExpectancyDataset(df=df_holdout)
    .df.loc[:, model_cols]
    .reset_index(drop=False)
    .set_index(LECol.COUNTRY)
)
```

```{python}
# | label: tbl-split-summary
# | tbl-cap: "Training and holdout cross-sections used in regression."
split_summary = pd.DataFrame(
    {
        "split": ["train", "holdout"],
        "year": [TRAIN_YEAR, HOLDOUT_YEAR],
        "n_countries": [int(df_train_model.shape[0]), int(df_holdout_model.shape[0])],
        "n_features": [
            int(df_train_model.shape[1] - 1),
            int(df_holdout_model.shape[1] - 1),
        ],
    }
)

split_summary
```

```{python}
# | label: split-summary-interpretation
# | results: asis
from IPython.display import Markdown, display

if not split_summary.empty:
    # TODO: always use query!
    train_row = split_summary.loc[split_summary["split"] == "train"].iloc[0]
    hold_row = split_summary.loc[split_summary["split"] == "holdout"].iloc[0]
    text = (
        f"The training cross‑section contains {int(train_row['n_countries'])} countries "
        f"with {int(train_row['n_features'])} predictors; the holdout year has "
        f"{int(hold_row['n_countries'])} countries with the same {int(hold_row['n_features'])} predictors, "
        "so comparisons reflect a clean year‑based split rather than a change in feature space."
    )
    display(Markdown(text))
```

```{python}
# | label: tbl-pca-summary
# | tbl-cap: "PCA reduction by feature group (trained on 2014)."
pca_summary = pd.DataFrame(
    {
        "group": [gr.group.name for gr in pca_groups.group_results],
        "n_components": [gr.n_components for gr in pca_groups.group_results],
        "variance_retained": [gr.cumulative_variance_explained for gr in pca_groups.group_results],
        "pc1_explained_ratio": [gr.explained_variance_pc1 for gr in pca_groups.group_results],
    }
)

pca_summary
```

<!-- REMOE This cell, all information should be in pca_summary table above -->
```{python}
# | label: pca-summary-interpretation
# | results: asis
from IPython.display import Markdown, display

if not pca_summary.empty:
    lines = []
    for _, row in pca_summary.iterrows():
        lines.append(
            f"- **{row['group']}**: {int(row['n_components'])} component(s), "
            f"variance retained {row['variance_retained']:.3f} (PC1 share {row['pc1_explained_ratio']:.3f})."
        )
    display(
        Markdown(
            "The grouped PCA compresses each correlated block while retaining most within‑block variance:\n\n"
            + "\n".join(lines)
        )
    )
```

## Univariate screening

Before building multivariate models, we fit each standardized predictor on its own and record its correlation with life expectancy. This provides a quick sense of which variables carry the strongest marginal signal after transformation and normalization.

<!-- TODO: This looks like boilerplate that should not be necessary when using the ama-tlbx idiomatically -->
```{python}
# | label: tbl-univariate-screen
# | tbl-cap: "Univariate screening: correlation and single‑predictor fits (2014, standardized)."
pc_pretty = {}
for gr in pca_groups.group_results:
    for col in gr.pc_scores.columns:
        if "_PC" in col:
            base, pc = col.rsplit("_PC", 1)
            pc_pretty[col] = f"{base.replace('_', ' ').title()} PC{pc}"

rows = []
# TODO: this code is really intransparent. data transformations should always be done with method chaining if possible, avoid branches!
for col in df_train_model.columns:
    if col == LECol.TARGET:
        continue
    tmp = df_train_model[[LECol.TARGET, col]].dropna()
    if tmp.empty:
        continue
    diag = fit_ols_formula(tmp, rhs=str(col), target_col=LECol.TARGET)
    corr = float(tmp[col].corr(tmp[LECol.TARGET]))
    pretty = pc_pretty.get(
        col,
        train_ds_raw.get_pretty_name(col)
        if col in train_ds_raw.df.columns
        else str(col),
    )
    # TODO: why unpack metrics? should not be necessary. reread ols_helper.py and model_registry.py and plan how the ama-tlbx can be used more idiomatically!
    rows.append(
        {
            "feature": pretty,
            "corr": corr,
            "r2": diag.metrics.r2,
            "adj_r2": diag.metrics.adj_r2,
            "rmse": diag.metrics.rmse,
            "aic": diag.metrics.aic,
            "bic": diag.metrics.bic,
            "n": diag.metrics.n_obs,
        }
    )

univariate = pd.DataFrame(rows)
univariate.assign(abs_corr=univariate["corr"].abs()).sort_values(
    "abs_corr", ascending=False
).drop(columns=["abs_corr"]).round(3)
```

<!-- TODO: don't define __repr__ use the functionalities from the ama-tlbx add missing features if you really cannot find the right one! -->
```{python}
# | label: univariate-interpretation
# | results: asis
from IPython.display import Markdown, display

if not univariate.empty:
    ranked = (
        univariate.assign(abs_corr=univariate["corr"].abs())
        .sort_values("abs_corr", ascending=False)
        .reset_index(drop=True)
    )
    top3 = ranked.head(3)
    strongest_pos = ranked.loc[ranked["corr"] > 0].head(1)
    strongest_neg = ranked.loc[ranked["corr"] < 0].head(1)
    bullets = []
    for _, row in top3.iterrows():
        bullets.append(
            f"- {row['feature']}: corr = {row['corr']:.3f}, adj $R^2$ = {row['adj_r2']:.3f}, "
            f"RMSE = {row['rmse']:.2f}."
        )
    if not strongest_pos.empty:
        row = strongest_pos.iloc[0]
        bullets.append(
            f"- Strongest positive association: {row['feature']} (corr = {row['corr']:.3f})."
        )
    if not strongest_neg.empty:
        row = strongest_neg.iloc[0]
        bullets.append(
            f"- Strongest negative association: {row['feature']} (corr = {row['corr']:.3f})."
        )
    display(
        Markdown(
            "The univariate screen highlights which predictors carry the strongest marginal signal:\n\n"
            + "\n".join(bullets)
        )
    )
```

## Modeling setup and notation

We estimate linear models of the form

$$
Y_i = \beta_0 + \sum_{j=1}^p X_{ij}\beta_j + \varepsilon_i,
$$

and consider interaction effects in the form

$$
Y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \beta_3(x_i z_i) + \varepsilon_i.
$$

Because all continuous predictors are standardized, coefficients are interpreted as the expected change in life expectancy (in years) per one standard deviation increase in the predictor, holding other variables constant. The `status_developed` coefficient represents the mean difference between developed and developing countries, conditional on the other covariates.

## Stepwise model building

We fit a stepwise sequence that starts with a single socioeconomic predictor and then adds motivated blocks. This follows the course guidance: add complexity only when it improves interpretability, fit, or generalization, and keep interactions only when they are supported by comparison metrics and uncertainty.
<!-- TODO: this chapter doesn't do stepweise model building! This is done in @6_model_selection.qmd - so remove this -->
```{python}
# | label: helper-functions
# | include: false
from typing import Iterable


def _as_terms(items: Iterable[object]) -> list[str]:
    return [str(item) for item in items]


def _pretty_term(term: str, pretty_map: dict[str, str]) -> str:
    if term in ("Intercept", "const"):
        return "Intercept"
    if ":" in term:
        parts = term.split(":")
        return " x ".join(_pretty_term(p, pretty_map) for p in parts)
    return pretty_map.get(term, term.replace("_", " ").title())


def coef_table(result, pretty_map: dict[str, str], alpha: float = 0.05) -> pd.DataFrame:
    params = result.params
    conf = result.conf_int(alpha=alpha)
    conf_arr = np.asarray(conf)
    if hasattr(params, "index"):
        terms = list(params.index)
        param_values = params.values
    else:
        terms = list(getattr(result.model, "exog_names", []))
        param_values = np.asarray(params)
    ci_low = conf_arr[:, 0] if conf_arr.ndim == 2 else conf_arr
    ci_high = conf_arr[:, 1] if conf_arr.ndim == 2 else conf_arr
    out = pd.DataFrame(
        {
            "term": terms,
            "estimate": param_values,
            "std_err": np.asarray(result.bse),
            "p_value": np.asarray(result.pvalues),
            "ci_low": ci_low,
            "ci_high": ci_high,
        }
    )
    out["term"] = out["term"].astype(str)
    out["term_pretty"] = out["term"].map(lambda t: _pretty_term(t, pretty_map))
    out = out.loc[:, ["term_pretty", "estimate", "std_err", "ci_low", "ci_high", "p_value"]]
    numeric_cols = ["estimate", "std_err", "ci_low", "ci_high", "p_value"]
    out[numeric_cols] = out[numeric_cols].astype(float).round(3)
    return out


def _term_stats(result, term: str) -> tuple[float, float]:
    params = result.params
    pvalues = result.pvalues
    if hasattr(params, "get"):
        est = float(params.get(term, np.nan))
    else:
        est = float(params[result.model.exog_names.index(term)]) if term in result.model.exog_names else np.nan
    if hasattr(pvalues, "get"):
        pval = float(pvalues.get(term, np.nan))
    else:
        pval = float(pvalues[result.model.exog_names.index(term)]) if term in result.model.exog_names else np.nan
    return est, pval


# Build pretty-name mapping (metadata for original features + readable names for PCs)
pretty_ds = train_ds_raw
pretty_map = {col: pretty_ds.get_pretty_name(col) for col in df_train_model.columns}

pc_pretty = {}
for gr in pca_groups.group_results:
    for col in gr.pc_scores.columns:
        if "_PC" in col:
            base, pc = col.rsplit("_PC", 1)
            pc_pretty[col] = f"{base.replace('_', ' ').title()} PC{pc}"

pretty_map.update(pc_pretty)
pretty_map[STATUS_DUMMY] = "Development status (1 = developed)"
```

<!-- TODO: remove this: clunky, intransparent, not the right chapter, not our task -->
```{python}
# | label: model-specs
# | include: false
# Identify PCA columns by group
pcs_by_group = {
    gr.group.name: gr.pc_scores.columns.tolist() for gr in pca_groups.group_results
}

child_mortality_pcs = pcs_by_group.get("child_mortality", [])
child_nutrition_pcs = pcs_by_group.get("child_nutrition", [])
econ_pcs = pcs_by_group.get("economic_development", [])
immun_pcs = pcs_by_group.get("immunization", [])

econ_pc1 = econ_pcs[0] if econ_pcs else None
econ_rest = econ_pcs[1:] if len(econ_pcs) > 1 else []

if econ_pc1 is None:
    raise ValueError("Expected at least one PCA component for economic_development.")

# Predictor blocks (HDI excluded to avoid target leakage)
block_baseline = [econ_pc1]
block_mortality = [LECol.ADULT_MORTALITY, LECol.HIV_AIDS]
block_development = [STATUS_DUMMY, LECol.TOTAL_EXPENDITURE, *econ_rest]
block_child = [*child_mortality_pcs, *child_nutrition_pcs]
block_immun = [*immun_pcs]

# Build stepwise RHS formulas
rhs_m0 = " + ".join(_as_terms(block_baseline))
rhs_m1 = " + ".join(_as_terms([*block_baseline, *block_mortality]))
rhs_m2 = " + ".join(_as_terms([*block_baseline, *block_mortality, *block_development]))
rhs_m3 = " + ".join(
    _as_terms([*block_baseline, *block_mortality, *block_development, *block_child])
)
rhs_m4 = " + ".join(
    _as_terms(
        [
            *block_baseline,
            *block_mortality,
            *block_development,
            *block_child,
            *block_immun,
        ]
    )
)

candidate_interactions = [
    f"{LECol.ADULT_MORTALITY}:{LECol.HIV_AIDS}",
    f"{STATUS_DUMMY}:{LECol.HIV_AIDS}",
    f"{STATUS_DUMMY}:{LECol.TOTAL_EXPENDITURE}",
]
if econ_pc1 is not None:
    candidate_interactions.append(f"{STATUS_DUMMY}:{econ_pc1}")

interaction_tests = []
for term in candidate_interactions:
    diag = fit_ols_formula(
        df_train_model,
        rhs=f"{rhs_m4} + {term}",
        target_col=str(LECol.TARGET),
    )
    interaction_tests.append(
        {
            "interaction": term,
            "p_value": float(diag.model.pvalues.get(term, float("nan"))),
        }
    )

interaction_tests = pd.DataFrame(interaction_tests)
significant_interactions = (
    interaction_tests.loc[interaction_tests["p_value"] < 0.05, "interaction"]
    .astype(str)
    .tolist()
)
rhs_int1 = " + ".join([rhs_m4, *significant_interactions])
```

<!-- TODO: this should be included / visible! For transparency reasons define the rhs direcly here for *every* model, don't pass default kwargs like cv_folds, shuffle_cv and random_state change default if necessary -->
```{python}
# | label: model-fit
# | include: false
registry = ModelRegistry(eval_year=HOLDOUT_YEAR)

registry.fit(
    df_train_model,
    name="m0_baseline_econ",
    rhs=rhs_m0,
    cv_folds=5,
    shuffle_cv=True,
    random_state=42,
)
registry.fit(
    df_train_model,
    name="m1_add_mortality",
    rhs=rhs_m1,
    cv_folds=5,
    shuffle_cv=True,
    random_state=42,
)
registry.fit(
    df_train_model,
    name="m2_add_development",
    rhs=rhs_m2,
    cv_folds=5,
    shuffle_cv=True,
    random_state=42,
)
registry.fit(
    df_train_model,
    name="m3_add_child_health",
    rhs=rhs_m3,
    cv_folds=5,
    shuffle_cv=True,
    random_state=42,
)
registry.fit(
    df_train_model,
    name="m4_add_immunization",
    rhs=rhs_m4,
    cv_folds=5,
    shuffle_cv=True,
    random_state=42,
)
registry.fit(
    df_train_model,
    name="m_int1_mortality_hiv",
    rhs=rhs_int1,
    cv_folds=5,
    shuffle_cv=True,
    random_state=42,
)
```

### Step 0: baseline socioeconomic association (M0)

The baseline model includes only the first PCA component of the **economic development** block (GDP and health‑expenditure shares). This establishes a simple reference fit and a first estimate of the macroeconomic gradient without using HDI (to avoid target leakage).

```{python}
# | label: fig-baseline
# | fig-cap: "Baseline association between economic development PC1 and life expectancy (2014)."
plt.figure()
sns.regplot(
    x=econ_pc1,
    y=LECol.TARGET,
    data=df_train_model,
    scatter_kws={"alpha": 0.3},
    line_kws={"linewidth": 2},
)
plt.xlabel(pretty_map.get(econ_pc1, econ_pc1))
plt.ylabel(pretty_map.get(str(LECol.TARGET), "Life expectancy"))
plt.show()
```

We see that after performing the analysis that the majority of the residuals remain within the `|y| < 2` range showing a good fit. This is especially good since a lot of them are clumped towards `leverage = 0`, indicating that each residual isn't being an extreme outlier.

The most notable outliers are **Austria**, **Malta**, and **India**. Austria and Malta show high studentized residuals (around 2-3) with moderate leverage, while India appears as a high-leverage point at the far right of the plot. However, most countries including **Sierra Leone**, **Lesotho**, and **Angola** (which show as large circles indicating high Cook's distance) remain within acceptable bounds (`|y| < 2`), suggesting the model is generally robust despite these influential points.

### Predictive capabilities

To test some predictive capabilities we decided to perform some single variable analyses, here we chose `Schooling` because it showed the strongest univariate relationship with life expectancy (R² = 0.601) among features that don't suffer from data leakage.

**Note on HDI Exclusion**: While Human Development Index showed the highest R² (0.799) in initial analysis, it was excluded from predictive modeling because HDI is calculated using life expectancy as one of its three components. Using HDI to predict life expectancy would constitute data leakage—using the target variable to predict itself—which artificially inflates performance and produces unreliable models.

```{python}
# | label: pred-plot-function
# | echo: false

def pred_plot(model, feature_col, df):
    """Plot actual vs predicted values for a single feature regression"""
    feature_name = str(feature_col)

    # Get predictions with confidence intervals
    predictions = model.get_prediction(df)
    pred_summary = predictions.summary_frame(alpha=0.05)  # 95% CI

    # Sort by feature for clean line plots
    sort_idx = df[feature_name].argsort()
    x_sorted = df[feature_name].iloc[sort_idx]

    plt.figure(figsize=(10, 6))

    # Plot actual data
    plt.scatter(df[feature_name], df['life_expectancy'], alpha=0.6, label='Actual', zorder=3)

    # Plot regression line
    plt.plot(x_sorted, pred_summary['mean'].iloc[sort_idx],
             color='red', linewidth=2, label='Regression Line', zorder=2)

    # Plot 95% confidence interval
    plt.fill_between(x_sorted,
                     pred_summary['mean_ci_lower'].iloc[sort_idx],
                     pred_summary['mean_ci_upper'].iloc[sort_idx],
                     alpha=0.2, color='red', label='95% CI', zorder=1)

    plt.xlabel(feature_name.replace('_', ' ').title())
    plt.ylabel('Life Expectancy')
    plt.title(f'Life Expectancy vs {feature_name.replace("_", " ").title()}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
```
```{python}
# | label: prediction-schooling
# | fig-cap: prediction_schooling
# | echo: false

pred = f"{LECol.SCHOOLING}"
model = sm.formula.ols(f"life_expectancy ~ {pred}", data=df).fit()

print(model.summary())
pred_plot(model, LECol.SCHOOLING, df)

```

In the plot for schooling we see a reasonably strong positive relationship with life expectancy. While the 95% confidence interval is relatively narrow, there is still considerable scatter around the regression line, indicating that schooling alone explains only about 60% of the variance (R² = 0.601). This demonstrates that while education is an important predictor, other factors are necessary for a comprehensive model of life expectancy. The clear positive trend confirms our expectation that countries with higher education levels tend to have longer life expectancies.

## Comparison: 2011 vs 2014 Data

To understand how well regression models generalize across time, we compare three scenarios using data from 2011 ("Old") and 2014 ("Latest"). This cross-temporal analysis reveals whether the relationships between predictors and life expectancy remain stable over time.

The 2011 model achieves `R² = 0.892` on its own test data, demonstrating strong predictive power for that year. The 2014 model, trained on data three years later, shows `R² = 0.786` on its test set—still a good result, explaining nearly 79% of the variance, though noticeably lower than the 2011 model. This difference could reflect changes in data quality, sample size, or the relationships between variables over time.

```{python}
# | label: comparison-models
# | fig-cap: "Comparing 2014 vs Latest Year Models"
# | echo: false

# Prepare old data
reg_df_old = df.dropna(subset=[LECol.LIFE_EXPECTANCY])
X_old = reg_df_old.select_dtypes(include=['number']).drop(columns=[LECol.LIFE_EXPECTANCY])
y_old = reg_df_old[LECol.LIFE_EXPECTANCY]

# Prepare latest year data
reg_df_latest = df_latest.dropna(subset=[LECol.LIFE_EXPECTANCY])
X_latest = reg_df_latest.select_dtypes(include=['number']).drop(columns=[LECol.LIFE_EXPECTANCY])
y_latest = reg_df_latest[LECol.LIFE_EXPECTANCY]

# Train/test split for both datasets
X_train_old, X_test_old, y_train_old, y_test_old = train_test_split(
    X_old, y_old, test_size=0.2, random_state=42
)
X_train_latest, X_test_latest, y_train_latest, y_test_latest = train_test_split(
    X_latest, y_latest, test_size=0.2, random_state=42
)

# Fit old model
model_old = LinearRegression()
model_old.fit(X_train_old, y_train_old)
y_pred_old_on_old = model_old.predict(X_test_old)

# Fit latest year model
model_latest = LinearRegression()
model_latest.fit(X_train_latest, y_train_latest)
y_pred_latest_on_latest = model_latest.predict(X_test_latest)

# Apply old model to latest year data (cross-temporal prediction)
y_pred_old_on_latest = model_old.predict(X_test_latest)

# Calculate metrics for old model on its own data
r2_old_on_old = r2_score(y_test_old, y_pred_old_on_old)
rmse_old_on_old = np.sqrt(mean_squared_error(y_test_old, y_pred_old_on_old))

# Calculate metrics for latest model on its own data
r2_latest_on_latest = r2_score(y_test_latest, y_pred_latest_on_latest)
rmse_latest_on_latest = np.sqrt(mean_squared_error(y_test_latest, y_pred_latest_on_latest))

# Calculate metrics for old model applied to latest data
r2_old_on_latest = r2_score(y_test_latest, y_pred_old_on_latest)
rmse_old_on_latest = np.sqrt(mean_squared_error(y_test_latest, y_pred_old_on_latest))

# Create comparison dataframe
comparison = pd.DataFrame({
    'Model': ['Old → Old test', 'Latest → Latest test', 'Old → Latest test'],
    'R² Score': [r2_old_on_old, r2_latest_on_latest, r2_old_on_latest],
    'RMSE': [rmse_old_on_old, rmse_latest_on_latest, rmse_old_on_latest],
    'Training Size': [len(y_train_old), len(y_train_latest), len(y_train_old)],
    'Test Size': [len(y_test_old), len(y_test_latest), len(y_test_latest)]
})

print("\nModel Performance Comparison:")
print(comparison.to_string(index=False))
```

```{python}
# | label: comparison-plots
# | fig-cap: "Comparison of Model Predictions: 2014 Model on Both Datasets"
# | echo: false

fig = plt.figure(figsize=(8, 8))
gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)
ax1 = fig.add_subplot(gs[0, 0])
ax2 = fig.add_subplot(gs[0, 1])
ax3 = fig.add_subplot(gs[1, :])

# Old model on old data
ax1.scatter(y_test_old, y_pred_old_on_old, alpha=0.5)
ax1.plot([y_test_old.min(), y_test_old.max()],
         [y_test_old.min(), y_test_old.max()], 'r--', lw=2)
ax1.set_xlabel('Actual Life Expectancy')
ax1.set_ylabel('Predicted Life Expectancy')
ax1.set_title(f'Old Model on Old Data\n(R² = {r2_old_on_old:.3f})')
ax1.grid(True, alpha=0.3)

# Latest model on latest data
ax2.scatter(y_test_latest, y_pred_latest_on_latest, alpha=0.5, color='green')
ax2.plot([y_test_latest.min(), y_test_latest.max()],
         [y_test_latest.min(), y_test_latest.max()], 'r--', lw=2)
ax2.set_xlabel('Actual Life Expectancy')
ax2.set_ylabel('Predicted Life Expectancy')
ax2.set_title(f'Latest Model on Latest Data\n(R² = {r2_latest_on_latest:.3f})')
ax2.grid(True, alpha=0.3)

# Old model on latest data (cross-temporal)
ax3.scatter(y_test_latest, y_pred_old_on_latest, alpha=0.5, color='orange')
ax3.plot([y_test_latest.min(), y_test_latest.max()],
         [y_test_latest.min(), y_test_latest.max()], 'r--', lw=2)
ax3.set_xlabel('Actual Life Expectancy (2014)')
ax3.set_ylabel('Predicted Life Expectancy (2011 Model)')
ax3.set_title(f'Old Model on Latest Data\n(R² = {r2_old_on_latest:.3f})')
ax3.grid(True, alpha=0.3)

plt.show()
```

The comparison reveals the temporal stability of our predictive models:

- **Old → Old test** (R² = 0.892): Strong baseline performance when the 2011 model predicts 2011 data
- **Latest → Latest test** (R² = 0.786): Good performance when the 2014 model predicts 2014 data, though lower than the 2011 baseline
- **Old → Latest test** (R² = 0.862): Surprisingly strong cross-temporal prediction—the 2011 model predicts 2014 data better than the 2014 model predicts its own data

The third scenario is particularly noteworthy: the 2011 model achieves R² = 0.862 when predicting 2014 data, which exceeds the 2014 model's performance on its own test set (0.786). This suggests that the relationships between health, economic, and social factors learned from 2011 data generalized remarkably well to 2014, indicating relatively stable predictive patterns over this three-year period. The superior performance could also reflect better data quality or a larger sample size in the 2011 dataset, allowing the model to learn more robust relationships.

## Univariate Regression Application (2011)

To evaluate how well individual features predict life expectancy, we apply the 2011 model to perform single-variable regression analysis for each key predictor. This shows the relationship between each feature and life expectancy in isolation.

```{python}
# | label: monovariate-analysis
# | fig-cap: "Single Variable Regression Analysis using 2011 Data"
# | echo: false

data_to_analyse = [
    LECol.ALCOHOL,
    LECol.SCHOOLING,
    LECol.BMI,
    LECol.TOTAL_EXPENDITURE,
    LECol.PERCENTAGE_EXPENDITURE,
    LECol.GDP,
    LECol.POPULATION,
    LECol.HIV_AIDS,
    LECol.MEASLES,
    "Baby Deaths",
    "Child Thinness",
    "Immunisation",
]

# Calculate number of rows needed (2 columns per row)
n_features = len(data_to_analyse)
n_rows = (n_features + 1) // 2  # Ceiling division

fig, axes = plt.subplots(n_rows, 2, figsize=(8, n_rows * 3))
axes = axes.flatten()  # Flatten to make indexing easier

for idx, feature in enumerate(data_to_analyse):
    ax = axes[idx]

    # Prepare data for this feature
    feature_name = str(feature)
    df_feature = df[[feature_name, LECol.LIFE_EXPECTANCY]].dropna()

    # Fit single-variable model
    X_single = df_feature[[feature_name]]
    y_single = df_feature[LECol.LIFE_EXPECTANCY]

    model_single = LinearRegression()
    model_single.fit(X_single, y_single)
    y_pred_single = model_single.predict(X_single)

    # Calculate R² for this feature
    r2_single = r2_score(y_single, y_pred_single)

    # Sort for clean line plot
    sort_idx = X_single[feature_name].argsort()
    X_sorted = X_single.iloc[sort_idx]
    y_pred_sorted = y_pred_single[sort_idx]

    # Plot
    ax.scatter(X_single, y_single, alpha=0.5, s=30)
    ax.plot(X_sorted, y_pred_sorted, 'r-', linewidth=2, label=f'Regression Line')
    ax.set_xlabel(feature_name.replace('_', ' ').title())
    ax.set_ylabel('Life Expectancy')
    ax.set_title(f'{feature_name.replace("_", " ").title()}\n(R² = {r2_single:.3f})')
    ax.grid(True, alpha=0.3)
    ax.legend()

# Hide any unused subplots
for idx in range(n_features, len(axes)):
    axes[idx].set_visible(False)

plt.tight_layout()
plt.show()
```

The monovariate regression analysis reveals distinct patterns in how individual features predict life expectancy:

**Strong Predictors (R² > 0.5):**
- **Schooling (R² = 0.601)**: The strongest valid single predictor, showing a clear positive relationship. Education consistently correlates with better health outcomes, as countries with higher average years of schooling tend to have longer life expectancies.
- **HIV/AIDS (R² = 0.568)**: Strong negative correlation—higher HIV/AIDS prevalence significantly reduces life expectancy.

**Note**: Human Development Index was initially observed to have the highest R² (0.799), but was excluded from analysis because it incorporates life expectancy as one of its components, creating data leakage that would artificially inflate model performance.

**Moderate Predictors (0.1 < R² < 0.5):**
- **BMI (R² = 0.381)**: Shows a positive relationship with life expectancy. Higher average BMI typically indicates better nutrition and food security.
- **GDP (R² = 0.339)**: Economic output shows moderate predictive power, reflecting better healthcare access in wealthier nations.
- **Percentage Expenditure (R² = 0.319)**: Healthcare spending as a percentage of GDP shows moderate predictive power.
- **Child Thinness (R² = 0.203)**: Negative correlation as expected—higher malnutrition rates are associated with lower life expectancy, though the relationship is weaker than anticipated.
- **Alcohol (R² = 0.144)**: Shows a positive relationship, possibly because wealthier nations with better healthcare also have higher alcohol consumption, creating a confounding effect.

**Weak Predictors (R² < 0.1):**
- **Immunisation (R² = 0.091)**: Despite being health-critical, immunization alone shows weak predictive power in isolation. Most countries have reached high vaccination rates, reducing variance.
- **Baby Deaths (R² = 0.044)**: Very weak relationship, likely because this raw count is influenced by population size rather than mortality rates.
- **Measles (R² = 0.039)**: Very weak negative correlation, suggesting measles cases have minimal direct linear impact on life expectancy.
- **Total Expenditure (R² = 0.002)**: Essentially no linear relationship, suggesting health spending alone doesn't predict outcomes—how funds are allocated matters more than the amount spent.
- **Population (R² = 0.002)**: No meaningful linear relationship with life expectancy.

These results highlight why multivariate models outperform single-variable approaches. Features like Total Expenditure and Baby Deaths contribute meaningful information when combined with other predictors, even though they show minimal individual predictive power. The interaction between economic, social, and health factors provides a more complete picture than any single metric.

### Alcohol Consumption Analysis

To better understand the positive correlation between alcohol consumption and life expectancy (R² = 0.144 after normalization), we examine how this relationship differs between developed and developing countries.

```{python}
# | label: alcohol-by-status
# | fig-cap: "Alcohol Consumption vs Life Expectancy by Development Status"
# | echo: false

# Split data by status for alcohol analysis - use raw data since STATUS is transformed to dummies in normalized version
df_alcohol = df_raw[[LECol.ALCOHOL, LECol.LIFE_EXPECTANCY, LECol.STATUS]].dropna()

# STATUS: 0 = Developing, 1 = Developed
df_developed = df_alcohol[df_alcohol[LECol.STATUS] == 1]
df_developing = df_alcohol[df_alcohol[LECol.STATUS] == 0]

fig, ax = plt.subplots(figsize=(8, 5))

has_data = False

# Fit and plot developed countries
if len(df_developed) > 1:
    has_data = True
    X_dev = df_developed[[LECol.ALCOHOL]]
    y_dev = df_developed[LECol.LIFE_EXPECTANCY]

    model_dev = LinearRegression()
    model_dev.fit(X_dev, y_dev)
    y_pred_dev = model_dev.predict(X_dev)
    r2_dev = r2_score(y_dev, y_pred_dev)

    # Sort for line plot
    sort_idx = X_dev[LECol.ALCOHOL].argsort()
    X_sorted = X_dev.iloc[sort_idx]
    y_pred_sorted = y_pred_dev[sort_idx]

    # Plot
    ax.scatter(X_dev, y_dev, alpha=0.6, s=50, color='blue', label='Developed', edgecolors='darkblue', linewidth=0.5)
    ax.plot(X_sorted, y_pred_sorted, 'b-', linewidth=2.5, label=f'Developed (R²={r2_dev:.3f})')

# Fit and plot developing countries
if len(df_developing) > 1:
    has_data = True
    X_devg = df_developing[[LECol.ALCOHOL]]
    y_devg = df_developing[LECol.LIFE_EXPECTANCY]

    model_devg = LinearRegression()
    model_devg.fit(X_devg, y_devg)
    y_pred_devg = model_devg.predict(X_devg)
    r2_devg = r2_score(y_devg, y_pred_devg)

    # Sort for line plot
    sort_idx = X_devg[LECol.ALCOHOL].argsort()
    X_sorted = X_devg.iloc[sort_idx]
    y_pred_sorted = y_pred_devg[sort_idx]

    # Plot
    ax.scatter(X_devg, y_devg, alpha=0.6, s=50, color='orange', label='Developing', edgecolors='darkorange', linewidth=0.5)
    ax.plot(X_sorted, y_pred_sorted, color='darkorange', linewidth=2.5, label=f'Developing (R²={r2_devg:.3f})')

ax.set_xlabel('Alcohol Consumption (liters per capita)', fontsize=12)
ax.set_ylabel('Life Expectancy (years)', fontsize=12)
ax.set_title('Alcohol Consumption vs Life Expectancy by Development Status', fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3)

if has_data:
    ax.legend(fontsize=10)

plt.tight_layout()
plt.show()
```

The stratified analysis provides crucial insights into the alcohol-life expectancy relationship:

**Key Findings:**
- **Developed Countries (Blue, R² = 0.019)**: The regression line is nearly flat with a slight negative slope, and the extremely low R² indicates virtually no predictive relationship. Among developed nations, alcohol consumption does not meaningfully predict life expectancy—these countries cluster at high life expectancy (mostly 75-87 years) regardless of alcohol consumption levels.

- **Developing Countries (Orange, R² = 0.037)**: Shows a weak positive correlation, though still with very low R². The much wider spread in life expectancy (50-85 years) suggests that factors other than alcohol consumption are far more influential in determining outcomes.

If we look at the overall positive correlation (R² = 0.175) observed in the combined data above. When separated by development status, alcohol shows almost no predictive power within each group. The apparent correlation exists primarily because:
1. Developed countries tend to have both higher alcohol consumption and higher baseline life expectancy due to better healthcare, infrastructure, and living conditions
2. Within developed countries, increasing alcohol consumption actually shows a slight negative trend (decline in the regression line)
3. The relationship is driven by confounding socioeconomic factors rather than any beneficial effect of alcohol

This demonstrates why univariate analysis can be deceptive—the positive correlation disappears when controlling for development status, revealing alcohol as a poor predictor of life expectancy within similar economic contexts.

## Multivariate Regression Analysis

After examining individual predictors, we now combine the selected features to build a multivariate regression model. This allows us to see if the combined predictive power exceeds what any single variable can achieve.

```{python}
# | label: multivariate-analysis
# | fig-cap: "Multi Variable Regression Analysis using 2011 Data"
# | echo: false

### Step 1: add mortality burden (M1)

# Prepare data with selected features
df_multi = df[data_to_analyse + [LECol.LIFE_EXPECTANCY]].dropna()
X_multi = df_multi[data_to_analyse]
y_multi = df_multi[LECol.LIFE_EXPECTANCY]

# Train/test split
X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(
    X_multi, y_multi, test_size=0.2, random_state=42
)

# Fit multivariate model
model_multi = LinearRegression()
model_multi.fit(X_train_multi, y_train_multi)
y_pred_multi = model_multi.predict(X_test_multi)

# Calculate metrics
r2_multi = r2_score(y_test_multi, y_pred_multi)
rmse_multi = np.sqrt(mean_squared_error(y_test_multi, y_pred_multi))

print(f"Multivariate Model Performance:")
print(f"R² Score: {r2_multi:.4f}")
print(f"RMSE: {rmse_multi:.4f}")
print(f"Number of features: {len(data_to_analyse)}")
print(f"Training samples: {len(y_train_multi)}")
print(f"Test samples: {len(y_test_multi)}")

# Create visualization with wider figure to accommodate labels
fig, axes = plt.subplots(1, 2, figsize=(8, 4))

# Plot 1: Actual vs Predicted
axes[0].scatter(y_test_multi, y_pred_multi, alpha=0.6, s=50)
axes[0].plot([y_test_multi.min(), y_test_multi.max()],
             [y_test_multi.min(), y_test_multi.max()], 'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual Life Expectancy', fontsize=11)
axes[0].set_ylabel('Predicted Life Expectancy', fontsize=11)
axes[0].set_title(f'Multivariate Model: Actual vs Predicted\n(R² = {r2_multi:.4f})', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3)
axes[0].legend()

# Plot 2: Feature Coefficients
coefficients = pd.DataFrame({
    'Feature': [str(f).replace('_', ' ').title() for f in data_to_analyse],
    'Coefficient': model_multi.coef_
}).sort_values('Coefficient', key=abs, ascending=False)

# Create bar chart with adjusted margins
colors = ['green' if c > 0 else 'red' for c in coefficients['Coefficient']]
bars = axes[1].barh(coefficients['Feature'], coefficients['Coefficient'], color=colors, alpha=0.7)

# Add value labels on the bars with smart positioning
for i, (idx, row) in enumerate(coefficients.iterrows()):
    value = row['Coefficient']

    # Position labels to the right of bars to avoid overlap
    if value < 0:
        x_pos = max(-0.5,value - 0.15)  # Left of negative bars
        ha = 'right'
    else:
        x_pos = min(0.5,value + 0.15)  # Right of positive bars
        ha = 'left'

    axes[1].text(x_pos, i, f'{value:.2f}',
                va='center', ha=ha,
                fontsize=9, fontweight='bold', color='black')

axes[1].set_xlabel('Coefficient Value', fontsize=11)
axes[1].set_title('Feature Importance (Coefficients)', fontsize=12, fontweight='bold')
axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)
axes[1].grid(True, alpha=0.3, axis='x')
# Adjust subplot to prevent label cutoff
plt.subplots_adjust(left=0.15, right=0.95)

plt.tight_layout()
plt.show()
```

**Analysis:**

The multivariate model using normalized features achieves **R² = 0.7791**, which represents a substantial improvement over the best individual predictor (Schooling with R² = 0.601). This demonstrates the power of combining multiple features:

**Key Insights:**
1. **Synergistic Effects**: Features that showed weak individual predictive power (like Total Expenditure, R² = 0.002) contribute meaningfully when combined with other variables, capturing interaction effects that aren't visible in isolation.

2. **Feature Coefficients**: The bar chart reveals the strongest predictors after normalization:
   - **HIV/AIDS (-4.76)**: Strongest negative impact—diseases dramatically reduce life expectancy
   - **Schooling (+3.39)**: Strongest positive impact—education remains the most beneficial factor
   - **Percentage Expenditure (+1.85)**: Healthcare spending shows substantial positive effect
   - **Alcohol (+0.91)** and **GDP (-0.95)**: Moderate effects, though GDP's negative coefficient may reflect confounding
   - **Measles (-0.52)**, **Baby Deaths (-0.23)**: Smaller but still meaningful negative impacts
   - Other features show coefficients below ±0.3

3. **Normalization Impact**: The normalized model (R² = 0.7791) differs from the raw data model, as standardization changes how features interact. The coefficient magnitudes now reflect standardized units, making them directly comparable.

The difference reveals how much additional predictive power comes from features beyond our selected 7, suggesting that including more comprehensive health, economic, and demographic indicators improves the model further.

## Multivariate Regression with Outlier Removal

To assess whether extreme observations are disproportionately affecting our model, we now repeat the multivariate analysis after removing outliers based on studentized residuals. Countries with `|studentized residual| > 2` are excluded, which removes observations that deviate significantly from the model's predictions. This approach helps us understand whether the model's performance is driven by a few unusual cases or reflects genuine patterns across the majority of countries.

```{python}
# | label: multivariate-no-outliers
# | fig-cap: "Multivariate Regression with Outliers Removed (|y| > 2)"
# | echo: false

# Fit initial model to identify outliers
df_multi_clean = df[data_to_analyse + [LECol.LIFE_EXPECTANCY]].dropna()
X_multi_clean = df_multi_clean[data_to_analyse]
y_multi_clean = df_multi_clean[LECol.LIFE_EXPECTANCY]

# Fit OLS model to get studentized residuals
X_multi_clean_sm = sm.add_constant(X_multi_clean)
ols_model_clean = sm.OLS(y_multi_clean, X_multi_clean_sm).fit()

# Get studentized residuals
influence = ols_model_clean.get_influence()
studentized_resids = influence.resid_studentized_internal

# Remove outliers with |studentized residual| > 2
outlier_mask = np.abs(studentized_resids) <= 2
n_outliers_removed = (~outlier_mask).sum()

print(f"Outliers removed: {n_outliers_removed} countries (|studentized residual| > 2)")
print(f"Remaining samples: {outlier_mask.sum()} countries\n")

# Filter data
X_multi_clean = X_multi_clean[outlier_mask]
y_multi_clean = y_multi_clean[outlier_mask]

# Train/test split on cleaned data
X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(
    X_multi_clean, y_multi_clean, test_size=0.2, random_state=42
)

# Fit model on cleaned data
model_clean = LinearRegression()
model_clean.fit(X_train_clean, y_train_clean)
y_pred_clean = model_clean.predict(X_test_clean)

# Calculate metrics
r2_clean = r2_score(y_test_clean, y_pred_clean)
rmse_clean = np.sqrt(mean_squared_error(y_test_clean, y_pred_clean))

print(f"Model Performance (without outliers):")
print(f"R² Score: {r2_clean:.4f}")
print(f"RMSE: {rmse_clean:.4f}")
print(f"Training samples: {len(y_train_clean)}")
print(f"Test samples: {len(y_test_clean)}")
print(f"\nComparison to original model:")
print(f"Original R²: {r2_multi:.4f}")
print(f"Improvement: {(r2_clean - r2_multi):.4f} ({((r2_clean - r2_multi) / r2_multi * 100):.1f}%)")

# Create visualization
fig, axes = plt.subplots(1, 2, figsize=(8, 4))

# Plot 1: Actual vs Predicted (cleaned data)
axes[0].scatter(y_test_clean, y_pred_clean, alpha=0.6, s=50, color='steelblue')
axes[0].plot([y_test_clean.min(), y_test_clean.max()],
             [y_test_clean.min(), y_test_clean.max()], 'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual Life Expectancy', fontsize=11)
axes[0].set_ylabel('Predicted Life Expectancy', fontsize=11)
axes[0].set_title(f'Without Outliers: Actual vs Predicted\n(R² = {r2_clean:.4f})', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3)
axes[0].legend()

# Plot 2: Feature Coefficients (cleaned model)
coefficients_clean = pd.DataFrame({
    'Feature': [str(f).replace('_', ' ').title() for f in data_to_analyse],
    'Coefficient': model_clean.coef_
}).sort_values('Coefficient', key=abs, ascending=False)

colors_clean = ['green' if c > 0 else 'red' for c in coefficients_clean['Coefficient']]
bars = axes[1].barh(coefficients_clean['Feature'], coefficients_clean['Coefficient'], color=colors_clean, alpha=0.7)

# Add value labels
for i, (idx, row) in enumerate(coefficients_clean.iterrows()):
    value = row['Coefficient']

    if value < 0:
        x_pos = max(-0.5, value - 0.15)
        ha = 'right'
    else:
        x_pos = min(0.5, value + 0.15)
        ha = 'left'

    axes[1].text(x_pos, i, f'{value:.2f}',
                va='center', ha=ha,
                fontsize=9, fontweight='bold', color='black')

axes[1].set_xlabel('Coefficient Value', fontsize=11)
axes[1].set_title('Feature Importance (Without Outliers)', fontsize=12, fontweight='bold')
axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)
axes[1].grid(True, alpha=0.3, axis='x')
plt.subplots_adjust(left=0.15, right=0.95)

plt.tight_layout()
plt.show()
```

**Analysis:**

Removing outliers with studentized residuals `|y| > 2` reveals several important insights about model robustness:

**Model Performance:**
- The cleaned model achieves **R² = 0.8128** compared to R² = 0.7791 with outliers included
- This represents a **+0.0337 improvement (+4.3%)** in explained variance
- The tighter scatter around the perfect prediction line shows reduced prediction errors
- Removing extreme cases allows the model to better capture the core relationships for typical countries

**Coefficient Changes:**

Comparing the feature importance before and after outlier removal:

| Feature | Original | Without Outliers | Change |
|---------|----------|------------------|--------|
| **HIV/AIDS** | -4.76 | -4.70 | +0.06 (stable) |
| **Schooling** | +3.39 | +3.30 | -0.09 (stable) |
| **Percentage Expenditure** | +1.85 | +1.88 | +0.03 (stable) |
| **GDP** | -0.95 | -1.02 | -0.07 (stable) |
| **Alcohol** | +0.91 | +0.17 | -0.74 (reduced) |
| **BMI** | +0.31 | +0.62 | +0.31 (increased) |
| **Total Expenditure** | +0.03 | +0.50 | +0.47 (increased) |
| **Baby Deaths** | -0.23 | -0.20 | +0.03 (stable) |

**Key Findings:**

1. **Improved Predictive Power**: The R² improvement from 0.7791 to 0.8128 indicates that outliers were introducing noise rather than capturing genuine patterns. The model now explains 81% of variance in typical countries, up from 78%.

2. **Robust Core Predictors**: HIV/AIDS (-4.70), Schooling (+3.30), and Percentage Expenditure (+1.88) maintain their dominance and relative magnitudes. This confirms these relationships are consistent across all country types, not driven by extreme cases.

3. **Outlier-Sensitive Features**:
   - **Alcohol** coefficient dropped dramatically from +0.91 to +0.17, suggesting the positive alcohol-longevity relationship was largely driven by outlier countries
   - **BMI** and **Total Expenditure** became more important (+0.31 and +0.47 increases), indicating outliers were masking their true contributions

4. **Practical Implications**: For policy applications, the cleaned model is more reliable for predicting outcomes in typical development scenarios. The stability of HIV/AIDS, Schooling, and Healthcare Expenditure as top predictors reinforces their importance regardless of country context, while the alcohol relationship appears less robust and may reflect confounding rather than causal effects.
