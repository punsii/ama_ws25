---
title: "Year Selection"
format:
  html:
    toc: true
---

# Selecting a representative cross-sectional year

The dataset is a country-year panel; repeated observations per country can induce serial dependence in residuals and violate the regression assumption of no autocorrelation. We therefore conduct the main multivariate analyses on a single cross-section and select a representative year that balances recency, completeness, and within-year typicality (Isolation Forest).


```{python}
# | label: setup-year-selection
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from ama_tlbx.data import LECol, LifeExpectancyDataset
from ama_tlbx.utils.plotting_config import DEFAULT_PLOT_CFG
np.random.seed(42)

DEFAULT_PLOT_CFG.apply_global()

```

## Load the longitudinal panel

```{python}
# | label: load-longitudinal
le_ds = LifeExpectancyDataset.from_csv(
    aggregate_by_country=False,
    drop_missing_target=False,
    resolve_nand_pred=False,
)

df = le_ds.df

print(f"Total observations: {len(df):,}")
print(f"Countries: {df[LECol.COUNTRY].nunique()}")
print(f"Years: {df[LECol.YEAR].nunique()}")
print(
    f"Year range: {df[LECol.YEAR].dt.year.min():.0f} - {df[LECol.YEAR].dt.year.max():.0f}"
)
```

Year selection is therefore driven mainly by completeness near the end of the panel.

## Data completeness by year

For each year $y$, we summarize country coverage, target availability, and average predictor missingness. To fuse these aspects into a single indicator, we define a completeness score

$$
CS_y = \frac{n^{(t)}_y}{n_y}\,(100 - m_y),
$$

where $n_y$ is the number of available country records in year $y$, $n^{(t)}_y$ is the number of countries with observed life expectancy, and $m_y$ is the mean percentage of missing values across *numeric predictors* in that year (excluding the target). Higher values indicate broader coverage and fewer missing predictors.

```{python}
# | label: tbl-year-completeness
# | tbl-cap: "Data completeness metrics by year (panel: 193 countries, 2000-2015)."
numeric_cols = le_ds.feature_columns(include_target=False)

year_stats = (
    df.groupby(LECol.YEAR)
    .agg(
        n_countries=(LECol.COUNTRY, "count"),
        n_countries_with_target=(LECol.TARGET, lambda x: x.notna().sum()),
        pct_missing_target=(LECol.TARGET, lambda x: x.isna().mean() * 100),
        pct_missing_all=(
            numeric_cols[0],
            lambda x: df.loc[x.index, numeric_cols].isna().mean().mean() * 100,
        ),
    )
    .reset_index()
    .assign(
        completeness_score=lambda d: d["n_countries_with_target"]
        * (100 - d["pct_missing_all"])
        / d["n_countries"]
    )
    .sort_values(by="completeness_score", ascending=False)
)

year_stats.set_index(LECol.YEAR)
```

```{python}
# | label: fig-completeness-over-time
# | fig-cap: "Completeness score and missingness over time."
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

year_stats_time = year_stats.sort_values(LECol.YEAR).assign(
    year=lambda d: d[LECol.YEAR].dt.year,
    completeness_rank=lambda d: d["completeness_score"]
    .rank(ascending=False)
    .astype(int),
)

sns.barplot(
    data=year_stats_time,
    x="year",
    y="completeness_score",
    hue="completeness_rank",
    alpha=0.7,
    ax=ax1,
    palette="coolwarm",
)
ax1.legend()
ax1.set_xlabel("Year")
ax1.set_ylabel("Completeness Score (log)")
ax1.set_title("Data Quality Score by Year")
ax1.tick_params(axis="x", rotation=45)
ax1.set_yscale("log")

sns.lineplot(
    data=year_stats_time.melt(
        id_vars="year",
        value_vars=["pct_missing_all", "pct_missing_target"],
        var_name="Category",
        value_name="Missing %",
    ).replace(
        {"pct_missing_all": "All Features", "pct_missing_target": "Life Expectancy"}
    ),
    x="year",
    y="Missing %",
    hue="Category",
    markers=True,
    dashes=False,
    linewidth=2,
    ax=ax2,
)
ax2.set_xlabel("Year")
ax2.set_ylabel("Missing Data (%)")
ax2.set_title("Missing Data Over Time")
ax2.legend(title=None)

plt.tight_layout()
plt.show()
```

Completeness improves from 2000 through the mid-2000s and then plateaus. Two outliers matter: 2013 has target missingness (5.18%) despite full country coverage, and 2015 shows a sharp predictor-missingness spike (13.32%) with the lowest completeness score (86.68). We therefore restrict candidates to **2007-2014**, exclude **2013** due to target missingness, and exclude **2015** due to the missingness spike.

## Multivariate anomaly burden in candidate years

Completeness alone does not guarantee typicality, so we estimate within-year anomaly rates using Isolation Forest on complete cases; the share of flagged countries summarizes within-year coherence (lower is more typical). Anomaly rates are stable across candidates, so completeness remains the primary discriminator.

```{python}
# | label: fig-outliers-by-year
# | fig-cap: "Share of countries flagged as Isolation Forest anomalies (per year)."
candidate_years = list(range(2007, 2015))
candidate_years.remove(2013)

feature_cols = le_ds.feature_columns(include_target=False)
outlier_rows: list[dict[str, float]] = []
for year in candidate_years:
    df_year = df[df[LECol.YEAR].dt.year == year].copy()
    ds_year = LifeExpectancyDataset(df=df_year)
    iso_result = (
        ds_year.make_isolation_forest_outlier_detector(
            columns=feature_cols,
            standardized=True,
            random_state=42,
            n_estimators=100,
        )
        .fit()
        .result()
    )
    outlier_rate = iso_result.n_outliers_per_row.gt(0).mean()
    outlier_rows.append(
        {
            "year": year,
            "outlier_rate": outlier_rate,
            "mean_outliers": iso_result.n_outliers_per_row.mean(),
            "n_rows": float(len(iso_result.n_outliers_per_row)),
        }
    )

df_mean_outliers = (
    pd.DataFrame(outlier_rows)
    .assign(outlier_rate_pct=lambda d: d["outlier_rate"] * 100)
    .sort_values("outlier_rate")
)
outlier_max = df_mean_outliers["outlier_rate"].max()
outlier_range = outlier_max - df_mean_outliers["outlier_rate"].min()

sns.barplot(
    df_mean_outliers,
    x="year",
    y="outlier_rate_pct",
    hue="outlier_rate_pct",
    palette="coolwarm",
)
plt.ylabel("Countries flagged (%)")
plt.show()
```

## Combined quality score and final recommendation

To balance completeness with anomaly burden, we min-max normalize completeness and invert-normalize the anomaly rate to 0--100 within the candidate window. The combined score

$$
Q_y = 0.6\,CS^{\star}_y + 0.4\,O^{\star}_y
$$

prioritizes completeness while still penalizing years with unusually many outliers.

```{python}
# | label: fig-combined-year-score
# | fig-cap: "Normalized completeness, outlier quality, and the combined score."
comparison_df = (
    year_stats[year_stats[LECol.YEAR].dt.year.isin(candidate_years)]
    .assign(year=lambda d: d[LECol.YEAR].dt.year)[["year", "completeness_score"]]
    .merge(
        df_mean_outliers.loc[:, ["year", "outlier_rate"]],
        on="year",
    )
    .assign(
        completeness_norm=lambda d: (
            d["completeness_score"] - d["completeness_score"].min()
        )
        / (d["completeness_score"].max() - d["completeness_score"].min())
        * 100,
        outlier_quality_norm=lambda d: np.where(
            outlier_range == 0,
            100.0,
            (outlier_max - d["outlier_rate"]) / outlier_range * 100,
        ),
        combined_score=lambda d: d["completeness_norm"] * 0.6
        + d["outlier_quality_norm"] * 0.4,
    )
)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

sns.barplot(
    data=comparison_df.melt(
        id_vars="year",
        value_vars=["completeness_norm", "outlier_quality_norm"],
        var_name="Metric",
        value_name="Score",
    ).replace(
        {
            "completeness_norm": "Data completeness",
            "outlier_quality_norm": "Outlier quality",
        }
    ),
    x="year",
    y="Score",
    hue="Metric",
    ax=ax1,
)
ax1.set(
    xlabel="Year",
    ylabel="Normalized score (0--100)",
    title="Completeness vs outlier quality",
)
ax1.legend(title=None)
ax1.grid(True, alpha=0.3, axis="y")

sns.barplot(
    data=comparison_df,
    x="year",
    y="combined_score",
    color="darkgreen",
    alpha=0.7,
    ax=ax2,
)

best_idx = comparison_df["combined_score"].idxmax()
score_tol = 1.0
score_top = comparison_df.loc[best_idx, "combined_score"]
best_year = (
    comparison_df[comparison_df["combined_score"] >= score_top - score_tol]
    .sort_values("year")
    .iloc[-1]["year"]
)
best_row = comparison_df.loc[comparison_df["year"] == best_year].iloc[0]

ax2.set(
    xlabel="Year",
    ylabel="Combined score",
    title="Combined quality score\n(60% completeness + 40% low outliers)",
)
ax2.grid(True, alpha=0.3, axis="y")
sns.despine(fig=fig)
plt.tight_layout()
plt.show()

print(f"\n{'=' * 70}\nFINAL YEAR RECOMMENDATION\n{'=' * 70}")
print(f"  Best year: {int(best_year)}")
print(f"   Combined score: {best_row['combined_score']:.1f}/100")
print(f"   Completeness: {best_row['completeness_score']:.1f}")
print(f"   Outlier rate: {best_row['outlier_rate']*100:.2f}%")
print("=" * 70)
```

The combined score yields a narrow band of top years because completeness has plateaued and anomaly rates are tightly clustered. We therefore select the **most recent** year within one point of the maximum score, which yields **2014**. With a fixed Isolation Forest seed ($random\_state = 42$), 2014 combines the highest completeness with a typical anomaly rate, and it is used as the default cross-section in subsequent chapters (see `2_preprocessing.qmd` and `2_outlier_detection.qmd`).
