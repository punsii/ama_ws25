---
title: "Principal Component Analysis"
format:
  html:
    toc: true
---

## Dimensionality reduction
Performing analysis on large invaried data often leads to significant noise, unwanted or unneeded data in further analysis.
To combat this a dimensionality reduction can be performed on the dataset using a PCA.
The goal here is to make sure that unneeded columns are merged together if they share variance, thus reducing our column size.

### Methodology

Finding candidates for column concatinations originally involved looking at the descriptions of the columns.
One candidates that stood out were the columns 'Hepatitis B' , 'Polio' and 'Diphtheria' all having a minimum value of around 1-3 and a maximum value of 99.
The column description also states that they represent the vaccination percentage for these specific deceases. The reason these columns intuatively make sense for merging is that we don't neccesarily care about the correlation between speficic deceases and life expecency, especially if they are all highly correlated.

So the process we went with for data concatination was as follows:
- Extract columns for concatination and put them into their own dataframe (DF-A) and make a dataframe containing the removed columns (DF-B)
- Perform a PCA with dimensionality of 1 on the columns (DF-A).
- Extract the loadings from the PCA.
- Take the loadings and lift them to the power of 2 effectively normalising them.
- Row by Row (DF-A)
  - create new row based on weighted sum of the analysed columns row.
- Add new row to original dataframe  (DF-A) <- (DF-B)

