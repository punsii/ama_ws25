---
title: "Principal Component Analysis"
format:
  html:
    toc: true
---

```{python}
# | label: init
# | fig-cap: "A line plot on a polar axis"
# | include: false
from pathlib import Path

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np


# LOAD ALL PROCESSING CLASSES
from ama_tlbx.data import LifeExpectancyDataset, LECol

from ama_tlbx.analysis import (
    IQROutlierDetector,
    IsolationForestOutlierDetector,
    ZScoreOutlierDetector,
)
from ama_tlbx.plotting import (
    plot_correlation_heatmap,
    plot_explained_variance,
    plot_loadings_heatmap,
    plot_target_correlations,
    plot_top_correlated_pairs,
    plot_biplot_plotly,
)



dataset = LifeExpectancyDataset.from_csv(
    csv_path="../../_data/life_expectancy_data.csv"    
,aggregate_by_country="mean")

assert(not dataset.df.empty)


from ama_tlbx.analysis import (
    ColumnConcatenator,
)

column_concatinator = ColumnConcatenator(dataset)
dataset2 = column_concatinator.concatenate(
    columns=[
        LECol.HEPATITIS_B,
        LECol.POLIO,
        LECol.DIPHTHERIA,
    ],
    new_column_name="Immunisation",
)

column_concatinator2 = ColumnConcatenator(dataset2)
dataset3 = column_concatinator2.concatenate(
    columns=[
        LECol.THINNESS_1_19_YEARS,
        LECol.THINNESS_5_9_YEARS,
    ],
    new_column_name="Child Thinness",
)

column_concatinator3 = ColumnConcatenator(dataset3)
dataset4 = column_concatinator3.concatenate(
    columns=[
        LECol.INFANT_DEATHS,
        LECol.UNDER_FIVE_DEATHS,
    ],
    new_column_name="Baby Deaths",
)
concat_dataset = dataset4

assert(not concat_dataset.df.empty)
```


## Dimensionality reduction

Performing analysis on large data sets often leads to significant noise, unwanted or unneeded data in further analysis.
To combat this dimensionality reduction can be performed on the data set using principal component analysis (PCA).
The goal is to reduce the number of features by merging columns that have high covariance, thus reducing our column size.

### Methodology

Our process for merging columns was as follows:

1. Extract the columns that should be merged to a separate dataframe (DF-A).
2. Add all other columns to a second dataframe (DF-B).
3. Perform a PCA with dimensionality of 1 on the columns (DF-A).
4. Extract the loadings from the PCA.
5. Take the loadings and raise them to the power of 2 in order to normalize them.
6. Add a new column in (DF-B) based on the weighted sum of the analyzed columns.

### Implementation

The implementation for the PCA concatenation follows the methodology outlined in the section above.
It takes a dataframe and a list of column names as inputs and returns a reduced dataframe.
```python
def concatenate(
        self,
        columns: list[str],
        new_column_name: str,
    ) -> BaseDataset:
```

The function starts with getting the columns and performing a PCA to get the loadings.
```python
        df = self.dataset.df.copy()

        only_columns = df[columns]
        only_not_columns = df.drop(columns=columns)

        pca = PCA(n_components=1)
        pc = pca.fit(only_columns)

        loadings = (
            pd.DataFrame(pc.components_.T, index=only_columns.columns, columns=["loading"])
            .reset_index()
            .rename(columns={"index": "feature"})
        )
```

In PCA, the loadings are normalized such that the sum of their squared values equals one.
We can use this to get each columns contribution to the principle component.
For this analysis we opted to keep only one principle component in order to be able to interpret the result.

$$ w_i = l_i^2 $$

In the end we add the resulting column into the dataframe DF-B.
```python
        loadings["loading"] **= 2
        self.loadings = loadings

        weights = loadings.set_index("feature")["loading"]

        self.explained_variance = pc.explained_variance_ratio_[0] * 100

        available = [f for f in weights.index if f in only_columns.columns]
        if len(available) == 0:
            raise ValueError("None of the PCA features are present in the dataframe columns")

        selected = only_columns[available].fillna(0)

        weighted_series = selected.mul(weights.loc[available], axis=1).sum(axis=1)
        only_not_columns[new_column_name] = weighted_series

        return self.dataset.with_df(only_not_columns)
```

### Findings

Finding candidates for column concatenations originally involved looking at the descriptions of the columns.
One group of candidates that stood out were the columns 'Hepatitis B', 'Polio' and 'Diphtheria',
which alle have a pairwise correlation value between 0.48 and 0.67.
The meta-data gathered from concatenating these columns returns the values of:


```{python}
#| label: pca-immune
#| fig-cap: "Immunisation Coefficients"

column_concatinator.print_results()
```

The explained variance of 89% shows a strong correlation between these columns, however 
not high enough to safely merge them with no practical data loss.

Since "Immunization rate" is an easy to interpret variable and we are not interested in analyzing the different deceases,
we decided to merge the columns, leaving the room for further analysis of the contribution of different vaccines on life expectancy.

For other groups of columns the question of whether or not to merge them was more clear cut:
```{python}
#| label: pca-babydeath
#| fig-cap: "Coefficients for thinness in adolescents"

column_concatinator2.print_results()
```
and
```{python}
#| label: pca-thinness
#| fig-cap: "Coefficients for deaths at young age"

column_concatinator3.print_results()
```

In both cases the columns contain high overlap in their data:
"under-five deaths" contains "infant dealths" and "thinnness 1-19 years" contains "thinness 5-9 years".
The explained variance of PC1 is 99.8%, which means that they might aswell mean the same thing.
A second easy candidate for concatination is "thinnness 1-19 years" and "thinness 5-9 years" since one is a subset of the other concatinating them makes sense from a data reduction perspective, the explained variance of the PC1 is 97%.

## General Principle Component Analysis

Performing a general principle component analysis on the reduced data signficantly reduces the amount of noise in our principle components aswell as our initial principle components, this eases the readability of PC analysis.
Our underlying goal of performing a PCA is to figure out if there is some correlation between the variables

Looking at the PCA we can see we have a very high explained variance in the first 2 columns. This indicates that there is a lot of highly correlated data. Whilst not a bad thing it could mean redundant data.
```{python}
#| label: pca-explained-variance-no-dim
#| fig-cap: "Graph over explained variance from the dataset without Dimensionality reduction"

pca_result = (
    dataset.make_pca_analyzer(standardized=True, exclude_target=True)
    .fit(n_components=None)
    .result()
)
fig = plot_explained_variance(pca_result)
fig.show()

```

When we run the dimensionality reduction we see that our data has been smoothed out and there is a lot less explained variance in a single column, indicating more uncorrelated data points which makes sense given that we've removed a couple of columns.
```{python}
#| label: pca-explained-variance
#| fig-cap: "Graph over explained variance from the dataset"

pca_result = (
    concat_dataset.make_pca_analyzer(standardized=True, exclude_target=True)
    .fit(n_components=None)
    .result()
)
fig = plot_explained_variance(pca_result)
fig.show()

```

### Biplots

To get better insight into our data we can also make a Biplot with the reduced data. Here we can see 3 consolidated directions appearing on our Biplot. With ``Baby Deaths`` and ``measles`` acting very similar. This is expected given that they both represent similar underlying concepts.

In a 90 degree offset we see a ``adult mortality`` and ``hiv_aids`` which also represents similar negative. Except this time its for deceases that primary target adults. 

The rest represent all the general positive things associated with high live expectancy. We see the ``Income disposition of resources`` which represents healthcare spending as a proportion of GDP and ``Schooling`` being close second, which is also commonly associated with highly developed countries.

```{python}
#| label: pca-biplot1
#| fig-cap: "Graph over Biplot for the dimensionality reduced data"

fig = plot_biplot_plotly(
    pca_result,
    dims=2,
    top_features=10,
    color=concat_dataset.df[LECol.TARGET],
)
fig.show()
```

The same data can also be visualized in 3D to further illustrate how the data points congregate.
When visualized this way, some dimensions become less congregated and we get a U-shaped plot wherein one end contains categories that correlate negatively with life expectancy, while the other other end correlates positively.
```{python}
fig = plot_biplot_plotly(
    pca_result,
    dims=3,
    top_features=10,
    color=concat_dataset.df[LECol.TARGET],
)
fig
```
