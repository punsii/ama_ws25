---
title: "Principal Component Analysis"
format:
  html:
    toc: true
---

```{python}
# | label: init
# | fig-cap: "A line plot on a polar axis"
# | include: false
from pathlib import Path

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np


# LOAD ALL PROCESSING CLASSES
from ama_tlbx.data import LifeExpectancyDataset, LECol

from ama_tlbx.analysis import (
    IQROutlierDetector,
    IsolationForestOutlierDetector,
    ZScoreOutlierDetector,
)
from ama_tlbx.plotting import (
    plot_correlation_heatmap,
    plot_explained_variance,
    plot_loadings_heatmap,
    plot_target_correlations,
    plot_top_correlated_pairs,
    plot_biplot_plotly,
)



dataset = LifeExpectancyDataset.from_csv(
    csv_path="../../_data/life_expectancy_data.csv"    
,aggregate_by_country="mean")

assert(not dataset.df.empty)


from ama_tlbx.analysis import (
    ColumnConcatenator,
)

column_concatinator = ColumnConcatenator(dataset)
dataset2 = column_concatinator.concatenate(
    columns=[
        LECol.HEPATITIS_B,
        LECol.POLIO,
        LECol.DIPHTHERIA,
    ],
    new_column_name="Immunisation",
)

column_concatinator2 = ColumnConcatenator(dataset2)
dataset3 = column_concatinator2.concatenate(
    columns=[
        LECol.THINNESS_1_19_YEARS,
        LECol.THINNESS_5_9_YEARS,
    ],
    new_column_name="Child Thinness",
)

column_concatinator3 = ColumnConcatenator(dataset3)
dataset4 = column_concatinator3.concatenate(
    columns=[
        LECol.INFANT_DEATHS,
        LECol.UNDER_FIVE_DEATHS,
    ],
    new_column_name="Baby Deaths",
)
concat_dataset = dataset4

assert(not concat_dataset.df.empty)
```


## Dimensionality reduction

Performing analysis on large invaried data often leads to significant noise, unwanted or unneeded data in further analysis.
To combat this a dimensionality reduction can be performed on the dataset using a PCA.
The goal here is to make sure that unneeded columns are merged together if they share variance, thus reducing our column size.

### Methodology

So the process we went with for data concatination was as follows:
- Extract columns you want to remove from original dataframe (DF) and put them into their own dataframe (DF-A) and make a dataframe containing the removed columns (DF-B)
- Perform a PCA with dimensionality of 1 on the columns (DF-A).
- Extract the loadings from the PCA.
- Take the loadings and lift them to the power of 2 effectively normalising them.
- Row by Row create new row based in (DF-B) based on weighted sum of the analysed columns row.

### Implementation

The implementation for the PCA concatination follows the methodology outlined in the Section above. It operates by taking a dataframe, a list of columns and returns a modified Dataset with the old columns removed
```python
def concatenate(
        self,
        columns: list[str],
        new_column_name: str,
    ) -> BaseDataset:
```

The function starts with getting the columns and performing a PCA to get the loadings.
```python
        df = self.dataset.df.copy()

        only_columns = df[columns]
        only_not_columns = df.drop(columns=columns)

        pca = PCA(n_components=1)
        pc = pca.fit(only_columns)

        loadings = (
            pd.DataFrame(pc.components_.T, index=only_columns.columns, columns=["loading"])
            .reset_index()
            .rename(columns={"index": "feature"})
        )
```

The way PCA loadings work is that the sum of all loadings squared add up to 1. We can use this advantagously to get each columns contributon to the principle component. For this analysis we opted to keep only one Principle Component, mostly due to explainability.

$$ w_i = l_i^2 $$

Then at the end you can merge the new cell that has the loadings into the datafram that has the old cells removed.
```python
        loadings["loading"] **= 2
        self.loadings = loadings

        weights = loadings.set_index("feature")["loading"]

        self.explained_variance = pc.explained_variance_ratio_[0] * 100

        available = [f for f in weights.index if f in only_columns.columns]
        if len(available) == 0:
            raise ValueError("None of the PCA features are present in the DataFrame columns")

        selected = only_columns[available].fillna(0)

        weighted_series = selected.mul(weights.loc[available], axis=1).sum(axis=1)
        only_not_columns[new_column_name] = weighted_series

        return self.dataset.with_df(only_not_columns)

```

### Findings

Finding candidates for column concatinations originally involved looking at the descriptions of the columns.
One candidates that stood out were the columns 'Hepatitis B' , 'Polio' and 'Diphtheria' all having a minimum value of around 1-3 and a maximum value of 99.
This means that there is a high likelyhood of this data is correlated.
The meta-data gathered from concatinating these columns returns the values of:


```{python}
#| label: pca-immune
#| fig-cap: "Immunisation Coefficients"

column_concatinator.print_results()
```

Based on these values we get a weight values for the first PC column of around 0.25 to 0.4, which roughly makes sense. Most of them ringe around the value of ```0.33``` which should make sense for correlated variables.
Secondly we also get an explained variance of 89% for the first column. Which is high showing a strong correlation between each column.
However not high enough to where you could safely concat them with no pratical dataloss.

In our case since we aren't as much intrested in the which deceases are the biggest contributors to life expecency. We decided to concat the columns, perhaps with the ability to do further analysis on which vaccine is best for life expectancy.
The reason for ommiting this analysis point is that the variance between them is to similar in this dataset so we dont belive the time is at the given moment best spent there.

Other columns concatination candiates are way more clear cut when it came to whether or not to concatinate them:

```{python}
#| label: pca-babydeath
#| fig-cap: "Coefficients for Baby Deaths"

column_concatinator2.print_results()
```
and
```{python}
#| label: pca-thinness
#| fig-cap: "Coefficients for Thinness"

column_concatinator3.print_results()
```

The first easy candidate to concatinate were the "under-five deaths" and "infant dealths" since they explain the same underlying thing, only difference is what you define as a infant. Regardless of the definition of an infant the correlation and explained variance of PC1 is so high at 99.8% that they might aswell mean the same thing.
A second easy candidate for concatination is "thinnness 1-19 years" and "thinness 5-9 years" since one is a subset of the other concatinating them makes sense from a data reduction perspective, the explained variance of 97%.

## General Principle Component Analysis

Performing a general principle component analysis on the reduced data signficantly reduces the amount of noise in our principle components aswell as our initial principle components, this eases the readability of PC analysis.
Our underlying goal of performing a PCA is to figure out if there is some underlying correlation between the variables

Looking at the PCA we can see we have a very high explained variance in the first 2 columns. This indicates that there is a lot of highly correlated data. Whilst not a bad thing it could mean redundant data.
```{python}
#| label: pca-explained-variance-no-dim
#| fig-cap: "Graph over explained variance from the dataset without Dimensionality reduction"

pca_result = (
    dataset.make_pca_analyzer(standardized=True, exclude_target=True)
    .fit(n_components=None)
    .result()
)
fig = plot_explained_variance(pca_result)
fig.show()

```

When we run the dimensionality reduction we see that our data has been smoothed out and there is a lot less explained variance in a single column, indicating more uncorrelated datapoints which makes sense given that we've removed a couple of columns
```{python}
#| label: pca-explained-variance
#| fig-cap: "Graph over explained variance from the dataset"

pca_result = (
    concat_dataset.make_pca_analyzer(standardized=True, exclude_target=True)
    .fit(n_components=None)
    .result()
)
fig = plot_explained_variance(pca_result)
fig.show()

```

### Biplots

To get a larger insight into our data we can also make a Biplot, over the reduced data. Here we can see 3 consolidated directions appearing on our Biplot. With ``Baby Deaths`` and ```measles`` acting very similar. this is expected given that they both represent similar underlying concepts.

In a 90 degree offset we see a ``adult mortality`` and ``hiv_aids`` which also represents similar negative. Except this time its for deceases that primarly target adults. 

The rest represent all the general positive things assoiated with high live expectency. Herein we see the ``Income disposition of resources`` which represents healthcare spending as a proportion of GDP. With ``Schooling`` and ``schooling`` being close seconds, which are also things assoiated with highly developed contries.

```{python}
#| label: pca-biplot1
#| fig-cap: "Graph over Biplot for the dimensionality reduced data"

fig = plot_biplot_plotly(
    pca_result,
    dims=2,
    top_features=10,
    color=concat_dataset.df[LECol.TARGET],
)
fig.show()
```

The same data can also be visualised in 3D to further illustrate how the data points congregate, although when visualised in 3D some dimensions become less congregated. and we Get a U shaped dataplot wherein one end represents bad things and the other represents good things.
```{python}
fig = plot_biplot_plotly(
    pca_result,
    dims=3,
    top_features=10,
    color=concat_dataset.df[LECol.TARGET],
)
fig
```