[
  {
    "objectID": "chapters/4_clustering.html",
    "href": "chapters/4_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "1 Clustering\n\n## | label: clustering-imports\n#| fig-cap: \"A line plot on a polar axis\"\n#| include: false\nfrom IPython.display import display, Math, Latex\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport ama_tlbx.plotting as plotting\nimport ama_tlbx.data as data\nimport ama_tlbx.analysis as analysis\n\n\ndataset = data.LifeExpectancyDataset.from_csv(csv_path=\"../../_data/life_expectancy_data.csv\")\ndf = dataset.df_standardized\nprint(df)\n\nkmeans_4 = KMeans(n_clusters=4, n_init=25)\nprint(type(df))\nkmeans_4.fit(df)\n\npca = PCA(n_components=2) # 2-D visualization\npca_fit = pca.fit_transform(df)\n# Create a DataFrame for PCA results (i.e. score-vector of datapoints w.r.t principal components)\npca_df = pd.DataFrame(pca_fit, columns=['PC1', 'PC2'], index=df.index)\n\npca_df['Cluster_4means'] = kmeans_4.labels_\n\n# Biplot Visualization\nplt.figure(figsize=(15, 10))\n\n# Plot datapoints with different colors for different clusters\nclusters = np.unique(pca_df['Cluster_4means'])\ncolors = plt.get_cmap('viridis', len(clusters))\n\nfor cluster in clusters:\n    cluster_data = pca_df[pca_df['Cluster_4means'] == cluster]\n    plt.scatter(cluster_data['PC1'], cluster_data['PC2'], color=colors(cluster), label=f'Cluster {cluster}')\n\n# Plot cluster centroids\ncentroids = pd.DataFrame(kmeans_4.cluster_centers_, columns=df.columns)\ncentroids_pca=pca.transform(centroids)\nplt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=100, c='black', marker='X', label='Centroids')\n\n# Plot arrows for principal components\nfor i, var in enumerate(dataset.feature_columns()):\n    plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], color='r', alpha=0.5, head_width=0.05, head_length=0.1)\n    plt.text(pca.components_[0, i] * 1.15, pca.components_[1, i] * 1.15, var, color='r', fontsize=8)\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Biplot of the two biggest principal components')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n             adult_mortality   alcohol       bmi  diphtheria       gdp  \\\ncountry                                                                  \nAfghanistan         1.059807 -1.198332 -1.010669   -1.141593 -0.458457   \nAlbania            -1.207668  0.159330  0.790938    0.646137 -0.459231   \nAlgeria            -0.256280 -1.095775 -1.474644    0.492903 -0.046028   \nAngola              1.694065  0.347351 -0.832976   -0.630813 -0.168223   \nArgentina          -0.256280  0.789324  1.101901    0.288591  0.330899   \n...                      ...       ...       ...         ...       ...   \nUruguay            -0.295921  0.425490  1.175939    0.441825 -0.492019   \nUzbekistan          0.179773 -0.678220  0.149270    0.543981 -0.491582   \nVanuatu            -0.034290 -0.956590  0.539207   -0.937281 -0.224380   \nZambia              2.582028 -0.695313 -0.813232   -3.950883 -0.487983   \nZimbabwe           -0.747831 -0.255783 -0.369000   -0.630813 -0.455314   \n\n             hepatitis_b  hiv_aids  income_composition_of_resources  \\\ncountry                                                               \nAfghanistan    -0.717680 -0.359632                        -1.246312   \nAlbania         0.703469 -0.359632                         0.437064   \nAlgeria        -2.910311 -0.359632                         0.361078   \nAngola         -0.311638  0.046426                        -1.018355   \nArgentina       0.175613 -0.359632                         0.933894   \n...                  ...       ...                              ...   \nUruguay         0.541052 -0.359632                         0.770232   \nUzbekistan      0.744073 -0.327147                         0.016220   \nVanuatu        -3.032124 -0.359632                        -0.287723   \nZambia         -2.950915  1.833079                        -0.796243   \nZimbabwe       -0.352242  3.473552                        -1.252157   \n\n             infant_deaths  life_expectancy   measles  percentage_expenditure  \\\ncountry                                                                         \nAfghanistan       0.303532        -1.209762 -0.052904               -0.394466   \nAlbania          -0.264058         0.782299 -0.157708               -0.378609   \nAlgeria          -0.130919         0.554944 -0.159769               -0.164506   \nAngola            0.338569        -2.216620 -0.064799               -0.265213   \nArgentina        -0.200992         0.663209 -0.159769                0.419166   \n...                    ...              ...       ...                     ...   \nUruguay          -0.264058         0.728167 -0.159769               -0.391818   \nUzbekistan       -0.116905        -0.094641 -0.078941               -0.395670   \nVanuatu          -0.271065         0.468333 -0.159769               -0.148031   \nZambia           -0.046832        -1.740257 -0.109661               -0.394510   \nZimbabwe         -0.067854        -2.389843 -0.137103               -0.380416   \n\n                polio  population  schooling  thinness_1_19_years  \\\ncountry                                                             \nAfghanistan -1.134227    0.049053  -1.226165             2.942695   \nAlbania      0.721078   -0.192173  -0.086099            -0.758942   \nAlgeria      0.514933   -0.192131   0.163291             0.177104   \nAngola      -0.515792   -0.165245  -1.475555             0.942960   \nArgentina    0.360324   -0.156176   1.588374            -0.865311   \n...               ...         ...        ...                  ...   \nUruguay      0.463397   -0.162141   1.232103            -0.758942   \nUzbekistan   0.669542   -0.192199  -0.014845            -0.439835   \nVanuatu     -0.928082   -0.190447  -0.406742            -0.758942   \nZambia      -0.412719   -0.076979  -0.264234             0.368568   \nZimbabwe    -0.618864   -0.180349  -0.798640             0.645127   \n\n             thinness_5_9_years  total_expenditure  under_five_deaths  \ncountry                                                                \nAfghanistan            2.886683           0.496767           0.301612  \nAlbania               -0.722103           0.206919          -0.267803  \nAlgeria                0.148983          -0.842054          -0.155954  \nAngola                 0.916369          -1.044487           0.428713  \nArgentina             -0.888024           0.386349          -0.211878  \n...                         ...                ...                ...  \nUruguay               -0.763583           1.186880          -0.267803  \nUzbekistan            -0.431741           0.073497          -0.140701  \nVanuatu               -0.763583          -0.911065          -0.272887  \nZambia                 0.335645          -0.589012          -0.013600  \nZimbabwe               0.626007          -0.543004          -0.039020  \n\n[120 rows x 19 columns]\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "chapters/2_preprocessing.html",
    "href": "chapters/2_preprocessing.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "1 Plot Data\n\n\n2 Remove outliers"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AMA Submission",
    "section": "",
    "text": "1 Life Expectancy\nAnalysis of life expectancy"
  },
  {
    "objectID": "chapters/1_introduction.html",
    "href": "chapters/1_introduction.html",
    "title": "Intro",
    "section": "",
    "text": "1 Lifetime expectancy analysis"
  },
  {
    "objectID": "chapters/3_pca.html",
    "href": "chapters/3_pca.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Performing analysis on large invaried data often leads to significant noise, unwanted or unneeded data in further analysis. To combat this a dimensionality reduction can be performed on the dataset using a PCA. The goal here is to make sure that unneeded columns are merged together if they share variance, thus reducing our column size.\n\n\nSo the process we went with for data concatination was as follows: - Extract columns you want to remove from original dataframe (DF) and put them into their own dataframe (DF-A) and make a dataframe containing the removed columns (DF-B) - Perform a PCA with dimensionality of 1 on the columns (DF-A). - Extract the loadings from the PCA. - Take the loadings and lift them to the power of 2 effectively normalising them. - Row by Row create new row based in (DF-B) based on weighted sum of the analysed columns row.\n\n\n\nThe implementation for the PCA concatination follows the methodology outlined in the Section above. It operates by taking a dataframe, a list of columns and returns a modified Dataset with the old columns removed\ndef concatenate(\n        self,\n        columns: list[str],\n        new_column_name: str,\n    ) -&gt; BaseDataset:\nThe function starts with getting the columns and performing a PCA to get the loadings.\n        df = self.dataset.df.copy()\n\n        only_columns = df[columns]\n        only_not_columns = df.drop(columns=columns)\n\n        pca = PCA(n_components=1)\n        pc = pca.fit(only_columns)\n\n        loadings = (\n            pd.DataFrame(pc.components_.T, index=only_columns.columns, columns=[\"loading\"])\n            .reset_index()\n            .rename(columns={\"index\": \"feature\"})\n        )\nThe way PCA loadings work is that the sum of all loadings squared add up to 1. We can use this advantagously to get each columns contributon to the principle component. For this analysis we opted to keep only one Principle Component, mostly due to explainability.\n\\[ w_i = l_i^2 \\]\nThen at the end you can merge the new cell that has the loadings into the datafram that has the old cells removed.\n        loadings[\"loading\"] **= 2\n        self.loadings = loadings\n\n        weights = loadings.set_index(\"feature\")[\"loading\"]\n\n        self.explained_variance = pc.explained_variance_ratio_[0] * 100\n\n        available = [f for f in weights.index if f in only_columns.columns]\n        if len(available) == 0:\n            raise ValueError(\"None of the PCA features are present in the DataFrame columns\")\n\n        selected = only_columns[available].fillna(0)\n\n        weighted_series = selected.mul(weights.loc[available], axis=1).sum(axis=1)\n        only_not_columns[new_column_name] = weighted_series\n\n        return self.dataset.with_df(only_not_columns)\n\n\n\nFinding candidates for column concatinations originally involved looking at the descriptions of the columns. One candidates that stood out were the columns ‘Hepatitis B’ , ‘Polio’ and ‘Diphtheria’ all having a minimum value of around 1-3 and a maximum value of 99. This means that there is a high likelyhood of this data is correlated. The meta-data gathered from concatinating these columns returns the values of:\n\ncolumn_concatinator.print_results()\n\nThe first principal component  explains 88.90% of the variance within these columns.\nPCA loadings, weighted (squared for component contribution):\n       feature   loading\n0  hepatitis_b  0.326564\n1        polio  0.325088\n2   diphtheria  0.348348\n\n\nBased on these values we get a weight values for the first PC column of around 0.25 to 0.4, which roughly makes sense. Most of them ringe around the value of 0.33 which should make sense for correlated variables. Secondly we also get an explained variance of 89% for the first column. Which is high showing a strong correlation between each column. However not high enough to where you could safely concat them with no pratical dataloss.\nIn our case since we aren’t as much intrested in the which deceases are the biggest contributors to life expecency. We decided to concat the columns, perhaps with the ability to do further analysis on which vaccine is best for life expectancy. The reason for ommiting this analysis point is that the variance between them is to similar in this dataset so we dont belive the time is at the given moment best spent there.\nOther columns concatination candiates are way more clear cut when it came to whether or not to concatinate them:\n\ncolumn_concatinator2.print_results()\n\nThe first principal component  explains 99.26% of the variance within these columns.\nPCA loadings, weighted (squared for component contribution):\n               feature   loading\n0  thinness_1_19_years  0.489387\n1   thinness_5_9_years  0.510613\n\n\nand\n\ncolumn_concatinator3.print_results()\n\nThe first principal component  explains 99.86% of the variance within these columns.\nPCA loadings, weighted (squared for component contribution):\n             feature   loading\n0      infant_deaths  0.351458\n1  under_five_deaths  0.648542\n\n\nThe first easy candidate to concatinate were the “under-five deaths” and “infant dealths” since they explain the same underlying thing, only difference is what you define as a infant. Regardless of the definition of an infant the correlation and explained variance of PC1 is so high at 99.8% that they might aswell mean the same thing. A second easy candidate for concatination is “thinnness 1-19 years” and “thinness 5-9 years” since one is a subset of the other concatinating them makes sense from a data reduction perspective, the explained variance of 97%."
  },
  {
    "objectID": "chapters/3_pca.html#dimensionality-reduction",
    "href": "chapters/3_pca.html#dimensionality-reduction",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Performing analysis on large invaried data often leads to significant noise, unwanted or unneeded data in further analysis. To combat this a dimensionality reduction can be performed on the dataset using a PCA. The goal here is to make sure that unneeded columns are merged together if they share variance, thus reducing our column size.\n\n\nSo the process we went with for data concatination was as follows: - Extract columns you want to remove from original dataframe (DF) and put them into their own dataframe (DF-A) and make a dataframe containing the removed columns (DF-B) - Perform a PCA with dimensionality of 1 on the columns (DF-A). - Extract the loadings from the PCA. - Take the loadings and lift them to the power of 2 effectively normalising them. - Row by Row create new row based in (DF-B) based on weighted sum of the analysed columns row.\n\n\n\nThe implementation for the PCA concatination follows the methodology outlined in the Section above. It operates by taking a dataframe, a list of columns and returns a modified Dataset with the old columns removed\ndef concatenate(\n        self,\n        columns: list[str],\n        new_column_name: str,\n    ) -&gt; BaseDataset:\nThe function starts with getting the columns and performing a PCA to get the loadings.\n        df = self.dataset.df.copy()\n\n        only_columns = df[columns]\n        only_not_columns = df.drop(columns=columns)\n\n        pca = PCA(n_components=1)\n        pc = pca.fit(only_columns)\n\n        loadings = (\n            pd.DataFrame(pc.components_.T, index=only_columns.columns, columns=[\"loading\"])\n            .reset_index()\n            .rename(columns={\"index\": \"feature\"})\n        )\nThe way PCA loadings work is that the sum of all loadings squared add up to 1. We can use this advantagously to get each columns contributon to the principle component. For this analysis we opted to keep only one Principle Component, mostly due to explainability.\n\\[ w_i = l_i^2 \\]\nThen at the end you can merge the new cell that has the loadings into the datafram that has the old cells removed.\n        loadings[\"loading\"] **= 2\n        self.loadings = loadings\n\n        weights = loadings.set_index(\"feature\")[\"loading\"]\n\n        self.explained_variance = pc.explained_variance_ratio_[0] * 100\n\n        available = [f for f in weights.index if f in only_columns.columns]\n        if len(available) == 0:\n            raise ValueError(\"None of the PCA features are present in the DataFrame columns\")\n\n        selected = only_columns[available].fillna(0)\n\n        weighted_series = selected.mul(weights.loc[available], axis=1).sum(axis=1)\n        only_not_columns[new_column_name] = weighted_series\n\n        return self.dataset.with_df(only_not_columns)\n\n\n\nFinding candidates for column concatinations originally involved looking at the descriptions of the columns. One candidates that stood out were the columns ‘Hepatitis B’ , ‘Polio’ and ‘Diphtheria’ all having a minimum value of around 1-3 and a maximum value of 99. This means that there is a high likelyhood of this data is correlated. The meta-data gathered from concatinating these columns returns the values of:\n\ncolumn_concatinator.print_results()\n\nThe first principal component  explains 88.90% of the variance within these columns.\nPCA loadings, weighted (squared for component contribution):\n       feature   loading\n0  hepatitis_b  0.326564\n1        polio  0.325088\n2   diphtheria  0.348348\n\n\nBased on these values we get a weight values for the first PC column of around 0.25 to 0.4, which roughly makes sense. Most of them ringe around the value of 0.33 which should make sense for correlated variables. Secondly we also get an explained variance of 89% for the first column. Which is high showing a strong correlation between each column. However not high enough to where you could safely concat them with no pratical dataloss.\nIn our case since we aren’t as much intrested in the which deceases are the biggest contributors to life expecency. We decided to concat the columns, perhaps with the ability to do further analysis on which vaccine is best for life expectancy. The reason for ommiting this analysis point is that the variance between them is to similar in this dataset so we dont belive the time is at the given moment best spent there.\nOther columns concatination candiates are way more clear cut when it came to whether or not to concatinate them:\n\ncolumn_concatinator2.print_results()\n\nThe first principal component  explains 99.26% of the variance within these columns.\nPCA loadings, weighted (squared for component contribution):\n               feature   loading\n0  thinness_1_19_years  0.489387\n1   thinness_5_9_years  0.510613\n\n\nand\n\ncolumn_concatinator3.print_results()\n\nThe first principal component  explains 99.86% of the variance within these columns.\nPCA loadings, weighted (squared for component contribution):\n             feature   loading\n0      infant_deaths  0.351458\n1  under_five_deaths  0.648542\n\n\nThe first easy candidate to concatinate were the “under-five deaths” and “infant dealths” since they explain the same underlying thing, only difference is what you define as a infant. Regardless of the definition of an infant the correlation and explained variance of PC1 is so high at 99.8% that they might aswell mean the same thing. A second easy candidate for concatination is “thinnness 1-19 years” and “thinness 5-9 years” since one is a subset of the other concatinating them makes sense from a data reduction perspective, the explained variance of 97%."
  },
  {
    "objectID": "chapters/3_pca.html#general-principle-component-analysis",
    "href": "chapters/3_pca.html#general-principle-component-analysis",
    "title": "Principal Component Analysis",
    "section": "2 General Principle Component Analysis",
    "text": "2 General Principle Component Analysis\nPerforming a general principle component analysis on the reduced data signficantly reduces the amount of noise in our principle components aswell as our initial principle components, this eases the readability of PC analysis. Our underlying goal of performing a PCA is to figure out if there is some underlying correlation between the variables\nThe full PCA for the dataset\n    feature PC1 PC2 PC3 PC4 PC5\n0   life_expectancy 0.361855    0.130614    -0.228990   0.023706    -0.007008\n1   adult_mortality -0.309021   -0.179180   0.388777    0.001862    -0.167414\n2   alcohol 0.257201    0.039492    0.360851    0.243854    -0.042597   0.247928\n3   percentage_expenditure  0.281612    0.112948    0.322523    -0.452369   0.325435\n4   measles -0.132957   0.442035    0.106486    0.104698    0.023987    0.656015\n5   bmi 0.329641    -0.034862   -0.096278   0.285799    -0.211092   -0.013664\n6   total_expenditure   0.172507    -0.096567   0.291616    0.562497    0.591793\n7   hiv/aids    -0.179048   -0.167945   0.599997    0.013996    -0.341367\n8   gdp 0.298375    0.120481    0.259397    -0.474694   0.198554    0.015851\n9   population  -0.090882   0.546238    0.048160    0.160008    -0.085112\n10  income_composition_of_resources 0.343011    0.134960    0.058061    -0.056920   -0.400266\n11  schooling   0.343628    0.094620    0.113897    0.051756    -0.376017\n12  Immunisation    -0.157956   0.557837    0.100588    0.145554    0.031030\n13  Child Thinness  -0.286315   0.226130    -0.021637   -0.219200   0.055522\n\nExplained variance\n  0.43486117 0.17509572 0.10366902 0.07807773 0.04615732\nNOTE: Only the top 5 PCs are included in this report for brevity.\nIf we look at each of the significant PC we can see they explain certain concepts often found in developed countries.\nFor example PC1 weighs life_expectancy highly schooling, income_composition_of_resources and bmi highly and negative loading such as adult_mortality and Child Thinness. Indicating that this PC represents overall development and health infrastructure with an explained variance of 43% which would make sense. If we were to give this principle component a name we could call it Development and that it is relates significantly with life expectancy. It is the principle component with the highest life expectency weight (+0.36).\nPC2 has a significantly smaller explained variance of 17% and weighs Immunisation, measels cases and population highly and adult mortality and AIDS deaths negatively. The columns does weigh things inversely to what you would expect. Immunisation is weighted highly and Measels cases are also weighed highly. This suggest the PC represent something like Population and Decease surveylance with tihs PC having a small positive weight (+0.13).\nPC3 has similar variance as PC2 with a variance of 10% and weighs HIV/AIDS deaths, adult mortality, alcohol highly and weighs population and income composition neutrally, this PC doesn’t have strong negative weights except for our life expectency with a significant negative weight of (-0.25)"
  }
]